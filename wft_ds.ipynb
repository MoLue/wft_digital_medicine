{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSFIa2jm56iWo8QYniZTtW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoLue/wft_digital_medicine/blob/main/wft_ds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "In this notebook, we will work with a cardiovascular disease (CVD) dataset published on [Kaggle](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data). The primary goal is to perform an initial data exploration to understand the dataset's structure and key insights, followed by the application of various machine learning methods to predict the likelihood of heart disease."
      ],
      "metadata": {
        "id": "KjXQo0wyvjth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "The imports are categorized for clarity into core libraries, visualization, machine learning, and configurations, making them easier to locate and manage."
      ],
      "metadata": {
        "id": "9kI8lAsvvmB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qbZZTg3b8Thv"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 1. Core Libraries for Data and Math\n",
        "# ===============================\n",
        "import numpy as np  # Linear algebra\n",
        "import pandas as pd  # Data processing, CSV file I/O (e.g., pd.read_csv)\n",
        "\n",
        "# ===============================\n",
        "# 2. Visualization Libraries\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt  # Plotting\n",
        "import matplotlib  # Additional Matplotlib configuration\n",
        "import seaborn as sns  # Statistical data visualization\n",
        "\n",
        "# ===============================\n",
        "# 3. Machine Learning Libraries\n",
        "# ===============================\n",
        "from sklearn import preprocessing  # Preprocessing utilities\n",
        "from sklearn.preprocessing import LabelEncoder  # Label encoding\n",
        "\n",
        "# ===============================\n",
        "# 4. Miscellaneous Configurations\n",
        "# ===============================\n",
        "import warnings  # Suppress warnings\n",
        "\n",
        "# ===============================\n",
        "# 5. Configurations\n",
        "# ===============================\n",
        "warnings.filterwarnings(\"ignore\")  # Ignore warnings\n",
        "pd.set_option(\"display.max_rows\", None)  # Show all rows in DataFrames\n",
        "matplotlib.style.use('ggplot')  # Use ggplot style for Matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The data\n",
        "Cardiovascular diseases (CVDs) are the leading cause of death globally, claiming 17.9 million lives annually, with many deaths linked to heart attacks, strokes, or premature cases under 70. This dataset includes 11 features to help predict heart disease, aiding early detection and management through machine learning models.\n",
        "\n",
        "Let’s import the data to begin our analysis."
      ],
      "metadata": {
        "id": "Si2759YZv0ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV and store it in our data frame variable\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/MoLue/wft_digital_medicine/main/data/heart.csv\")\n",
        "\n",
        "# show the first 5 entries\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IKzRo_W-_xJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Attributess include:**\n",
        "* Age: age of the patient [years]\n",
        "* Sex: sex of the patient [M: Male, F: Female]\n",
        "* ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
        "* RestingBP: resting blood pressure [mm Hg]\n",
        "* Cholesterol: serum cholesterol [mm/dl]\n",
        "* FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
        "* RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
        "* MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n",
        "* ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n",
        "* Oldpeak: oldpeak = ST [Numeric value measured in depression]\n",
        "* ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
        "* HeartDisease: output class [1: heart disease, 0: Normal]\n",
        "\n",
        "More information about the dataset you can find here: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction"
      ],
      "metadata": {
        "id": "LDw66F-5_1oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas as central data exploration and data handling tool\n",
        "\n",
        "The Pandas Cheat Sheet is a concise and powerful reference tool that summarizes key Pandas operations, making it easier to work with data in Python. It’s particularly helpful for beginners learning Pandas or for experienced users needing a quick refresher.\n",
        "\n",
        "If you’re new to Pandas, the cheat sheet introduces essential concepts like creating DataFrames, indexing, and basic operations like filtering, sorting, and aggregation.\n",
        "\n",
        "Quick Reference for Common Tasks:\n",
        "When working on data manipulation, the cheat sheet provides quick access to frequently used operations, such as:\n",
        "Handling missing data.\n",
        "Applying functions (apply, map).\n",
        "Joining and merging datasets.\n",
        "Grouping and aggregating data.\n",
        "\n",
        "Explore Advanced Features:\n",
        "For advanced users, the cheat sheet highlights more complex operations like pivot tables, reshaping data, and handling time-series data, which can save time when dealing with complex workflows.\n",
        "\n",
        "Bookmark for Fast Access:\n",
        "Keep the cheat sheet accessible while working on projects to quickly recall the syntax for specific operations, ensuring smoother and more efficient coding.\n",
        "Where to Find It\n",
        "\n",
        "You can access the Pandas Cheat Sheet via this [link](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)."
      ],
      "metadata": {
        "id": "mPiBQP-GPCvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration\n",
        "In the first step you should start to feel comfortable with the data. You can get an overview about this notebook, the tools and the data as well.\n",
        "\n",
        "Some useful methods to get an overview:\n",
        "- `df.describe()` and `df.describe().T`\n",
        "- `df.head()`\n",
        "- `df.tail()`\n",
        "- `df.info()`\n",
        "- `df.dtypes`\n",
        "\n"
      ],
      "metadata": {
        "id": "BtLA6p_X_5h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this block to get information about your data\n",
        "# ...\n",
        "\n"
      ],
      "metadata": {
        "id": "VWf4ntkO_7sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prepares the dataset for more structured and efficient data exploration by clearly separating categorical (string) columns and numerical columns. Here's how it helps:\n",
        "\n",
        "**Consistent Data Types**:\n",
        "Converts all object columns to string explicitly, ensuring uniform data types for categorical columns. This avoids errors or inconsistencies during exploration or analysis.\n",
        "\n",
        "**Column Segmentation:**\n",
        "By separating categorical (string_col) and numerical columns (num_col), you can apply appropriate analysis techniques to each type of data.\n",
        "Example: Use summary statistics or visualizations (like histograms) for numeric columns.\n",
        "\n",
        "\n",
        "**Focus on the Target Variable**:\n",
        "Excludes the target variable HeartDisease from num_col to avoid accidentally treating it as an independent feature during exploratory analysis or model training.\n",
        "By organizing the data this way, the process of exploring relationships, distributions, and patterns becomes much easier and reduces the risk of errors. This segmentation is particularly useful for tasks like:\n",
        "\n",
        "- Feature engineering\n",
        "- Applying statistical tests\n",
        "- Creating tailored visualizations for numeric and categorical variables"
      ],
      "metadata": {
        "id": "eLFrPp8S3jST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all columns with object data type (categorical/string columns)\n",
        "string_col = df.select_dtypes(include=\"object\").columns\n",
        "\n",
        "# Convert these columns explicitly to string type for consistency\n",
        "df[string_col] = df[string_col].astype(\"string\")\n",
        "\n",
        "# Re-select string columns to ensure consistency after the conversion\n",
        "string_col = df.select_dtypes(\"string\").columns.to_list()\n",
        "\n",
        "# Create a list of all numeric columns excluding the target variable \"HeartDisease\"\n",
        "num_col = df.columns.to_list()  # Start with all columns\n",
        "for col in string_col:\n",
        "    num_col.remove(col)  # Remove string columns from the numeric list\n",
        "num_col.remove(\"HeartDisease\")  # Exclude the target variable\n"
      ],
      "metadata": {
        "id": "n0taGYj36Hxe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example for a more detailed descriptive analysis"
      ],
      "metadata": {
        "id": "vNmT5hLJSDQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate descriptive statistics for patients with and without heart disease\n",
        "yes = df[df['HeartDisease'] == 1].describe().T\n",
        "no = df[df['HeartDisease'] == 0].describe().T\n",
        "\n",
        "# Define new color palette\n",
        "colors = 'coolwarm'\n",
        "\n",
        "# Create subplots\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "\n",
        "# Plot heatmap for patients with heart disease\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(\n",
        "    yes[['mean']],\n",
        "    annot=True,\n",
        "    cmap=colors,\n",
        "    linewidths=0.5,\n",
        "    linecolor='white',\n",
        "    cbar=False,\n",
        "    fmt='.2f'\n",
        ")\n",
        "plt.title('Heart Disease', fontsize=14, color='darkred', weight='bold')\n",
        "\n",
        "# Plot heatmap for patients without heart disease\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(\n",
        "    no[['mean']],\n",
        "    annot=True,\n",
        "    cmap=colors,\n",
        "    linewidths=0.5,\n",
        "    linecolor='white',\n",
        "    cbar=False,\n",
        "    fmt='.2f'\n",
        ")\n",
        "plt.title('No Heart Disease', fontsize=14, color='darkblue', weight='bold')\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "fig.tight_layout(pad=3)\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CVJQ4LKA6AVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Matrix:\n",
        "What does `df.corr()` do? Try to visualize it with a diagram by using a heatmap. What does it represent?\n",
        "\n",
        "Note: `sns` is our helper to visualize our graphs and plots with the library *seaborn*"
      ],
      "metadata": {
        "id": "9oH6LAr-2KTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df.select_dtypes(\"int64\").corr()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "UaSjHwHVS6bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df.select_dtypes(\"int64\").corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Add your code here:\n",
        "# Hint: SNS Heatmap https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# End of your code\n",
        "\n",
        "# Add title\n",
        "plt.title(\"Correlation Plot of the Heart Failure Prediction\", fontsize=16)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "appwSTy-_9Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histograms\n",
        "Compare your columns with histogram plots. Therefore you can use `px.histogram(...)`. Which values provide some good overview?"
      ],
      "metadata": {
        "id": "H_G03lsPTKAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group data by Sex and plot histograms\n",
        "chest_pain_types = df[\"Sex\"].unique()\n",
        "for pain_type in chest_pain_types:\n",
        "    subset = df[df[\"Sex\"] == pain_type]\n",
        "    plt.hist(subset[\"Age\"], bins=20, alpha=0.7, label=pain_type)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Distribution of Age by Sex\", fontsize=16)\n",
        "plt.xlabel(\"Age\", fontsize=14)\n",
        "plt.ylabel(\"Count\", fontsize=14)\n",
        "plt.legend(title=\"Sex\", fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S5YNzHTm_-32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try some other plots on your own in the following code box.\n"
      ],
      "metadata": {
        "id": "yU3O_B0P56dA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "53NDYfa2RG_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pair Plot\n",
        "\n",
        "Create a pairplot to visualize relationships between variables in your dataset. Use the URL below to understand how to implement this visualization:\n",
        "Seaborn Pairplot Documentation\n",
        "\n",
        "Hint: Use the hue parameter to group data by a categorical variable like HeartDisease.\n",
        "\n",
        "Purpose: This plot will help you explore patterns, correlations, and distributions within your data.\n",
        "A pair plot is a powerful data visualization tool that displays relationships between multiple features in a dataset. It creates scatterplots for every pair of numerical features and histograms (or kernel density estimates) for individual features along the diagonal.\n",
        "\n",
        "Add your solution in the provided code section to complete the visualization.\n",
        "\n",
        "A pair plot can show pairwise bivariate distributions. What does this plot show in the end? (More information: https://seaborn.pydata.org/generated/seaborn.pairplot.html)"
      ],
      "metadata": {
        "id": "xslxOKct6dZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can enter your results for Question 3\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "# Add your code for the pairplot here.\n",
        "# Hint: Think about what do you try to say with that kind of visualization!\n",
        "# Hint: https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
        "\n",
        "\n",
        "# End of your code\n",
        "plt.tight_layout()\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "jN6vHYc0AAP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers\n",
        "A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables. The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution.The box plot is a standardized way of displaying the distribution of data based on the five number summary:\n",
        "\n",
        "- Minimum\n",
        "- First quartile\n",
        "- Median\n",
        "- Third quartile\n",
        "- Maximum.\n",
        "A segment inside the rectangle shows the median and “whiskers” above and below the box show the locations of the minimum and maximum.\n"
      ],
      "metadata": {
        "id": "KgFqPscdACXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a boxplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(\n",
        "    data=df,\n",
        "    x=\"HeartDisease\",\n",
        "    y=\"Age\",\n",
        "    palette=\"husl\"  # Color palette\n",
        ")\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Distribution of Age by Heart Disease Status\", fontsize=16)\n",
        "plt.xlabel(\"Heart Disease (0 = No, 1 = Yes)\", fontsize=14)\n",
        "plt.ylabel(\"Age\", fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6nxqjeOHAEHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to find other outliers within the dataset!"
      ],
      "metadata": {
        "id": "BUjepOmr7IIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "EGCxk9uD7MMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing\n",
        "Data preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n",
        "\n",
        "The concepts are\n",
        "\n",
        "1. Check for Null Values\n",
        "2. Feature Scaling\n",
        "3. Handling Categorical Variables"
      ],
      "metadata": {
        "id": "4f3NpJAWAGI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Null Values\n",
        "Do we have null values? (More information under: https://pandas.pydata.org/docs/reference/api/pandas.isnull.html)"
      ],
      "metadata": {
        "id": "C94URljSTwZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check null values. Your code goes here:\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "jU3-FmaJAHih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling\n",
        "Feature scaling is a preprocessing technique used in machine learning to standardize the range of values across different features in a dataset. This process ensures that features with varying units or magnitudes contribute equally to a model, preventing one feature from dominating due to its scale. For example, in a dataset with age measured in years and income in thousands, the larger range of income values could disproportionately affect the results unless the data is scaled.\n",
        "\n",
        "Feature scaling is crucial for models that rely on distance metrics, such as k-Nearest Neighbors or Support Vector Machines, because these methods calculate distances between points, and larger-scaled features can skew these calculations. Additionally, gradient-based models like neural networks benefit from scaled data, as it helps them converge faster during training by normalizing the gradient values.\n",
        "\n",
        "There are several methods to perform feature scaling. Min-Max scaling, also known as normalization, transforms values to a fixed range, typically between 0 and 1, preserving the distribution of the data. Standardization, on the other hand, centers features around a mean of 0 and scales them to have a standard deviation of 1, which is particularly useful when the data is assumed to follow a normal distribution. For datasets with outliers, robust scaling is an alternative that scales data based on the median and interquartile range, making it less sensitive to extreme values.\n",
        "\n",
        "Applying feature scaling ensures that all features contribute equally to the learning process and improves the performance and reliability of machine learning models. It is particularly essential when the algorithms depend on feature magnitude or require gradient-based optimization for training.\n",
        "\n",
        "Normalization of all features: for various machine learning methods it is necessary to have normalized values and clear distinction in types."
      ],
      "metadata": {
        "id": "BtwlPekqAJjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to transform text data to numerical values later to be able\n",
        "# to process them\n",
        "\n",
        "# textual columns\n",
        "string_col = df.select_dtypes(include=\"object\").columns\n",
        "df[string_col]=df[string_col].astype(\"string\")\n",
        "string_col=df.select_dtypes(\"string\").columns.to_list()\n",
        "\n",
        "# numerical columns\n",
        "num_col=df.columns.to_list()\n",
        "for col in string_col:\n",
        "    num_col.remove(col)\n",
        "num_col.remove(\"HeartDisease\")"
      ],
      "metadata": {
        "id": "lMXxLvntAKrq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As we will be using both types of approches for demonstration lets do First Label Ecoding\n",
        "# which will be used with Tree Based Algorthms\n",
        "df_tree = df.apply(LabelEncoder().fit_transform)\n",
        "df_tree.head()"
      ],
      "metadata": {
        "id": "cmaPSConANpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preperation of Categorization / Classification"
      ],
      "metadata": {
        "id": "3U4lwJ2xUBD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code performs one-hot encoding on categorical features to convert them into a numerical format suitable for non-tree-based algorithms like Logistic Regression, Support Vector Machines, or k-Nearest Neighbors. This transformation creates new binary columns for each category in the categorical variables, making the data fully numerical while preserving the information in the original categories.\n",
        "\n",
        "Additionally, the code ensures that the target variable (HeartDisease) is moved to the end of the DataFrame. This is done for better organization and to separate the target column from the features, which is helpful for data exploration and model training.\n",
        "\n",
        "**Why Is This Important?**\n",
        "\n",
        "Non-tree-based algorithms require numerical input to process data effectively, as they cannot inherently handle categorical features. One-hot encoding ensures that categorical information is represented in a machine-readable format without introducing ordinal relationships that do not exist. By organizing the target column at the end, the code enhances clarity and simplifies workflows for modeling and evaluation. This step is critical for maintaining a structured and interpretable dataset when working with machine learning pipelines."
      ],
      "metadata": {
        "id": "imcyFkxGULPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating one hot encoded features for working with non tree based algorithms\n",
        "df_nontree=pd.get_dummies(df, columns=string_col,drop_first=False)\n",
        "df_nontree.head()"
      ],
      "metadata": {
        "id": "9BKsEVeFAOld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the target column at the end\n",
        "target = \"HeartDisease\"\n",
        "y = df_nontree[target].values\n",
        "df_nontree.drop(\"HeartDisease\",axis=1,inplace=True)\n",
        "df_nontree=pd.concat([df_nontree,df[target]],axis=1)\n",
        "df_nontree.head()"
      ],
      "metadata": {
        "id": "gsXKptiwAPpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ML for classification"
      ],
      "metadata": {
        "id": "zjhF_4zKUyZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Metrics Table\n",
        "When we perform ML techniques we will see a certain output.\n",
        "The metrics for each class (0 and 1) as well as overall scores are explained as follows:\n",
        "\n",
        "**Precision**:\tThe proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
        "\n",
        "**Recall**:\tThe proportion of correctly predicted positive instances out of all actual positive instances.\n",
        "\n",
        "**F1-Score**:\tThe harmonic mean of precision and recall, providing a balanced measure of model performance.\n",
        "\n",
        "**Support**:\tThe number of true instances for each class in the validation set.\n",
        "\n",
        "The **ROC-AUC** score (Receiver Operating Characteristic - Area Under the Curve) measures the model's ability to distinguish between classes.\n",
        "For example a score of 0.88 indicates that the model has a high ability to differentiate between positive (1) and negative (0) classes, with 1.0 being a perfect score and 0.5 representing random guessing.\n"
      ],
      "metadata": {
        "id": "HlsPd17AIJwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance based ML\n",
        "- Logistic regression\n",
        "- SVM"
      ],
      "metadata": {
        "id": "bJ3u4o0hARFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare feature columns by excluding the target column\n",
        "feature_columns = df_nontree.columns.to_list()\n",
        "feature_columns.remove(target)"
      ],
      "metadata": {
        "id": "gYl1N8JTJx8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "Logistic Regression is a statistical method used for binary classification tasks, where the goal is to predict one of two possible outcomes (e.g., yes/no, 0/1, positive/negative) based on input features. Despite its name, Logistic Regression is a classification algorithm, not a regression algorithm. It models the relationship between the input features and the target variable by estimating probabilities using a logistic (sigmoid) function, which maps values to a range between 0 and 1.\n",
        "\n",
        "Logistic Regression is widely used in applications such as medical diagnosis (e.g., predicting the likelihood of a disease), customer behavior analysis (e.g., churn prediction), and risk assessment (e.g., likelihood of loan default). It provides interpretable results, as the coefficients of the features indicate their contribution to the predicted probability.\n",
        "\n",
        "One of its key strengths is simplicity and efficiency, making it a good baseline model for classification tasks. However, it assumes a linear relationship between the features and the log-odds of the outcome, which might limit its performance on complex, non-linear datasets without feature transformations or extensions like polynomial terms."
      ],
      "metadata": {
        "id": "ZETtcHW5IFSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize a list to store accuracy for each fold\n",
        "accuracy_log = []\n",
        "\n",
        "# Set up Stratified K-Fold cross-validation with 5 splits\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Iterate through each fold\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X=df_nontree, y=y)):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train = df_nontree.loc[train_idx, feature_columns]\n",
        "    y_train = df_nontree.loc[train_idx, target]\n",
        "    X_valid = df_nontree.loc[val_idx, feature_columns]\n",
        "    y_valid = df_nontree.loc[val_idx, target]\n",
        "\n",
        "    # Apply Min-Max Scaling to the features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_valid = scaler.transform(X_valid)\n",
        "\n",
        "    # Initialize and train a logistic regression model\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_pred = clf.predict(X_valid)\n",
        "\n",
        "    # Print classification metrics for the current fold\n",
        "    print(f\"The fold is: {fold}\")\n",
        "    print(classification_report(y_valid, y_pred))\n",
        "\n",
        "    # Calculate the ROC-AUC score and append to accuracy list\n",
        "    accuracy = roc_auc_score(y_valid, y_pred)\n",
        "    accuracy_log.append(accuracy)\n",
        "    print(f\"The ROC-AUC score for Fold {fold + 1}: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "wsbquXR8ASej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates data points from different classes with the largest possible margin. For non-linearly separable data, SVM can use kernel functions to map the data into higher-dimensional spaces, allowing it to handle more complex relationships.\n",
        "\n",
        "SVM is commonly used in tasks such as text classification, image recognition, and bioinformatics, as it is effective in high-dimensional spaces and robust to overfitting, especially in cases with a clear margin of separation.\n",
        "\n",
        "Now it’s your turn to write the code for implementing an SVM model! Use your knowledge of scikit-learn and the concepts discussed to create a working example\n"
      ],
      "metadata": {
        "id": "T0IKcavJHKsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "acc_svm=[]\n",
        "kf=model_selection.StratifiedKFold(n_splits=5)\n",
        "for fold , (trn_,val_) in enumerate(kf.split(X=df_nontree,y=y)):\n",
        "    # Your Code starts here...\n",
        "    # Hint: You find some help here: https://scikit-learn.org/1.5/modules/svm.html"
      ],
      "metadata": {
        "id": "AOSzBVnWATne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k-Nearest Neighbor (KNN)\n"
      ],
      "metadata": {
        "id": "9bh3clH2LaUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# List to store accuracy scores for each fold\n",
        "acc_KNN = []\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Iterate through each fold\n",
        "for fold, (trn_, val_) in enumerate(kf.split(X=df_nontree, y=y)):\n",
        "    # Your Code goes here:\n",
        "\n",
        "# Display average accuracy across folds\n",
        "print(f\"Average ROC-AUC across all folds: {sum(acc_KNN) / len(acc_KNN)}\")\n"
      ],
      "metadata": {
        "id": "jVOeX9-ZAUql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree based ML\n",
        "- Decision tree classifier\n",
        "- Random forest\n",
        "- XGBoost"
      ],
      "metadata": {
        "id": "dbtL4_o5AV4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier"
      ],
      "metadata": {
        "id": "RJPYmLN3M-J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the feature columns for the tree-based model\n",
        "feature_col_tree = df_tree.columns.to_list()\n",
        "feature_col_tree.remove(target)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# List to store accuracy scores for each fold\n",
        "acc_Dtree = []\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Iterate through each fold\n",
        "for fold, (trn_, val_) in enumerate(kf.split(X=df_tree, y=y)):\n",
        "    # Your code goes here:\n",
        "\n",
        "# Display average accuracy across folds\n",
        "print(f\"Average ROC-AUC across all folds: {sum(acc_Dtree) / len(acc_Dtree)}\")\n"
      ],
      "metadata": {
        "id": "-TAJoHP1AW_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have a trained Decision Tree Classifier (clf), let's visualize it\n",
        "\n",
        "# Import necessary libraries\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "# Export the Decision Tree as DOT data\n",
        "dot_data = export_graphviz(\n",
        "    clf,                          # Trained Decision Tree Classifier\n",
        "    out_file=None,                # No need to save the DOT file to disk\n",
        "    feature_names=feature_col_tree,  # Feature names for interpretability\n",
        "    class_names=[str(cls) for cls in clf.classes_],  # Class names as strings\n",
        "    filled=True,                  # Fill the nodes with colors based on class\n",
        "    rounded=True,                 # Rounded node borders for better aesthetics\n",
        "    special_characters=True       # Allow special characters in feature names\n",
        ")\n",
        "\n",
        "# Render the graph using Graphviz\n",
        "graph = graphviz.Source(dot_data, format=\"png\")  # Render graph as a PNG\n",
        "graph.render(\"decision_tree\")  # Save the graph to a file (optional)\n",
        "graph  # Display the graph in the notebook\n"
      ],
      "metadata": {
        "id": "6iOBIr7rAYRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "QwpUfj5uM4YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Initialize list to store accuracy scores\n",
        "acc_RandF = []\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Iterate through each fold\n",
        "for fold, (trn_, val_) in enumerate(kf.split(X=df_tree, y=y)):\n",
        "    # Your code goes here:\n",
        "\n",
        "# Display average accuracy across folds\n",
        "print(f\"Average ROC-AUC across all folds: {sum(acc_RandF) / len(acc_RandF)}\")\n"
      ],
      "metadata": {
        "id": "WqZ3hRQ-AZiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature importance:\n",
        "Run the following code and think about what does the plot say about the data?"
      ],
      "metadata": {
        "id": "GcmEz-6DNv54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Feature Importance from the Random Forest Classifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure clf (Random Forest Classifier) is already trained\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Extract feature importance from the trained classifier\n",
        "importance = clf.feature_importances_\n",
        "\n",
        "# Sort feature importance in ascending order for visualization\n",
        "idxs = np.argsort(importance)\n",
        "\n",
        "# Create a horizontal bar chart for feature importance\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.barh(range(len(idxs)), importance[idxs], align=\"center\")\n",
        "\n",
        "# Add feature names to the y-axis\n",
        "plt.yticks(range(len(idxs)), [feature_col_tree[i] for i in idxs])\n",
        "\n",
        "# Label the x-axis\n",
        "plt.xlabel(\"Random Forest Feature Importance\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3CWh4a3_AbBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XG Boost"
      ],
      "metadata": {
        "id": "fmUznqVXVXWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# List to store accuracy scores\n",
        "acc_XGB = []\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Iterate through each fold\n",
        "for fold, (trn_, val_) in enumerate(kf.split(X=df_tree, y=y)):\n",
        "    # Your code goes here:\n",
        "\n",
        "# Display average accuracy across folds\n",
        "print(f\"Average ROC-AUC across all folds: {sum(acc_XGB) / len(acc_XGB)}\")\n"
      ],
      "metadata": {
        "id": "IRO4yy_sAehf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for visualization\n",
        "from xgboost import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure clf (XGBoost Classifier) is already trained\n",
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "\n",
        "# Visualize the first tree (num_trees=0) from the trained XGBoost model\n",
        "plot_tree(clf, num_trees=0, rankdir=\"LR\", ax=ax)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DoUU2yDbAfnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "- Do you think that the results of the different ML methods are reasonable and overall \"good\"?\n",
        "- What do you think about the data quality?"
      ],
      "metadata": {
        "id": "4Z-tqXeBVjVh"
      }
    }
  ]
}