{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_cell"
   },
   "source": [
    "# Introduction to LLMs in Healthcare\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MoLue/wft_digital_medicine/blob/main/llms_in_healthcare.ipynb)\n",
    "\n",
    "*Author: Timo LÃ¼ders*\n",
    "\n",
    "*Last Updated: May 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toc_cell"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction]\n",
    "2. [Understanding LLMs in Healthcare Context]\n",
    "3. [Accessing and Using Open Source Medical LLMs]\n",
    "4. [Medical Prompt Engineering]\n",
    "5. [Implementing Simple Medical RAG in Colab]\n",
    "6. [Working with Pre-trained Medical LLMs]\n",
    "7. [Practical Applications]\n",
    "8. [Evaluating LLM Outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "introduction"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Welcome to the Introduction to LLMs in Healthcare notebook! \n",
    "\n",
    "This notebook is designed for medical students, healthcare professionals, and researchers who want to understand how Large Language Models (LLMs) can be applied in healthcare settings. Whether you're new to artificial intelligence or have some experience with machine learning, this notebook will provide you with practical insights into using LLMs for medical applications.\n",
    "\n",
    "### Purpose of This Notebook\n",
    "\n",
    "The goal of this notebook is to:\n",
    "- Introduce you to Large Language Models and their relevance to healthcare\n",
    "- Provide hands-on experience with open-source medical LLMs\n",
    "- Teach practical skills for prompt engineering in medical contexts\n",
    "- Show how to implement retrieval-augmented generation for improved medical accuracy\n",
    "- Demonstrate practical healthcare applications of LLMs\n",
    "- Explore ethical considerations specific to medical AI\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "To get the most out of this notebook, you should have:\n",
    "- Basic familiarity with Python programming\n",
    "- Completion of the Data Science Onboarding notebook (recommended but not required)\n",
    "- Interest in healthcare applications of AI\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "This notebook is designed to be interactive. You'll find a mix of explanatory text, code cells, and exercises. To navigate through the notebook:\n",
    "- Read through the explanatory text to understand concepts\n",
    "- Run code cells by clicking the play button or pressing Shift+Enter\n",
    "- Complete exercises to practice what you've learned\n",
    "- Use the Table of Contents to jump to specific sections\n",
    "\n",
    "Let's begin by setting up our environment with the necessary packages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "### Environment Setup\n",
    "\n",
    "First, let's install the packages we'll need for this notebook. We'll be using several libraries that allow us to work with language models efficiently within Google Colab's resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_code"
   },
   "outputs": [],
   "source": [
    "# Upgrade pip first\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers datasets sentence-transformers faiss-cpu torch einops accelerate bitsandbytes\n",
    "\n",
    "# For RAG implementation\n",
    "!pip install -q langchain chromadb\n",
    "\n",
    "# For evaluation\n",
    "!pip install -q evaluate rouge-score nltk\n",
    "\n",
    "\n",
    "# Install sacremoses (required for some tokenizers)\n",
    "!pip install -q sacremoses\n",
    "\n",
    "# For RAG implementation\n",
    "!pip install -q langchain chromadb\n",
    "\n",
    "# For evaluation\n",
    "!pip install -q evaluate rouge-score nltk\n",
    "\n",
    "# For UI elements and system monitoring\n",
    "!pip install -q ipywidgets psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compatibility_check"
   },
   "outputs": [],
   "source": [
    "# Check if we're running in Colab\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Check available GPU\n",
    "    !nvidia-smi -L\n",
    "    \n",
    "    # Check available memory\n",
    "    !free -h\n",
    "    \n",
    "    print(\"\\nNote: This notebook is optimized to run with Colab's free resources.\")\n",
    "    print(\"Some models may run slowly or require memory optimization techniques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "understanding_llms"
   },
   "source": [
    "## 2. Understanding LLMs in Healthcare Context\n",
    "\n",
    "### What are Large Language Models?\n",
    "\n",
    "Large Language Models (LLMs) are a type of artificial intelligence system trained on vast amounts of text data to understand and generate human-like language. These models use complex neural network architectures (typically transformer-based) to process and generate text.\n",
    "\n",
    "At a high level, LLMs work by:\n",
    "1. **Learning patterns in language** from massive datasets\n",
    "2. **Predicting the next word or token** in a sequence based on context\n",
    "3. **Generating coherent text** by repeatedly predicting the next most likely token\n",
    "\n",
    "### Recent Developments in Medical LLMs\n",
    "\n",
    "The field of medical LLMs has seen significant advancements in recent years:\n",
    "\n",
    "1. **Domain-specific pre-training**: Models like BioGPT, BioMedLM, and MedAlpaca have been pre-trained on medical literature, clinical notes, and healthcare datasets to better understand medical terminology and concepts.\n",
    "\n",
    "2. **Specialized capabilities**: Recent medical LLMs can assist with tasks such as medical question answering, clinical documentation, literature review, and patient education.\n",
    "\n",
    "3. **Open-source medical models**: Several open-source initiatives have made smaller but effective medical LLMs available to researchers and developers.\n",
    "\n",
    "4. **Parameter-efficient approaches**: Techniques like LoRA (Low-Rank Adaptation) allow efficient fine-tuning of models for medical tasks without requiring massive computational resources.\n",
    "\n",
    "### Current Applications in Healthcare Settings\n",
    "\n",
    "LLMs are being applied in healthcare in numerous ways:\n",
    "\n",
    "1. **Clinical documentation assistance**: Helping clinicians draft notes, summarize patient encounters, and code medical procedures.\n",
    "\n",
    "2. **Medical question answering**: Providing evidence-based answers to clinical questions at the point of care.\n",
    "\n",
    "3. **Patient education**: Generating tailored educational materials that explain medical conditions in patient-friendly language.\n",
    "\n",
    "4. **Literature review assistance**: Summarizing and extracting key information from medical research papers.\n",
    "\n",
    "5. **Medical education**: Creating case studies, practice questions, and learning materials for healthcare students.\n",
    "\n",
    "6. **Clinical decision support**: Assisting (but not replacing) clinician judgment by providing relevant information and considerations.\n",
    "\n",
    "### Example: Medical LLM Use Cases\n",
    "\n",
    "Here are some specific examples of how LLMs are being used in medicine:\n",
    "\n",
    "- **Primary care**: Drafting patient instructions, summarizing visit notes, suggesting potential diagnoses based on symptoms\n",
    "- **Radiology**: Structuring findings from imaging reports, generating preliminary report drafts\n",
    "- **Pathology**: Extracting key information from pathology reports, standardizing reporting formats\n",
    "- **Research**: Literature review, hypothesis generation, study design assistance\n",
    "- **Medical education**: Creating case scenarios, generating practice exam questions\n",
    "\n",
    "In the following sections, we'll explore how to access and use these models, with a focus on open-source options that can run within Colab's resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accessing and Using Open Source Medical LLMs\n",
    "\n",
    "### Introduction to Open Source Medical LLMs\n",
    "\n",
    "While commercial LLMs are powerful, there are excellent open-source alternatives specifically designed for biomedical applications. Smaller open-source models can run directly in Colab and locally, offering several advantages:\n",
    "\n",
    "- **No API costs**: Free to use and experiment with\n",
    "- **Transparency**: Open model architecture and training data\n",
    "- **Customizability**: Can be adapted for specific medical tasks\n",
    "- **Privacy**: Data remains local rather than being sent to external APIs\n",
    "\n",
    "Let's explore some of these models and how to use them effectively within resource constraints.\n",
    "\n",
    "### Loading Smaller Biomedical Models\n",
    "\n",
    "For both local environments and Colab's free tier, we need to be mindful of memory limitations. Here are some strategies for working with LLMs in this environment:\n",
    "\n",
    "1. **Choose smaller models**: 7B parameter models or smaller work well in resource-constrained environments\n",
    "2. **Use quantization**: 4-bit or 8-bit quantization reduces memory requirements\n",
    "3. **Optimize inference**: Techniques like attention caching can improve performance\n",
    "4. **Load models efficiently**: Only load what you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "# Check available CPU memory\n",
    "print(f\"Available system memory: {psutil.virtual_memory().available / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Biomedical Language Model\n",
    "\n",
    "Let's start by loading a smaller biomedical language model using the Transformers library. We'll use Microsoft's BioGPT, which is a biomedical-focused language model that's well-supported in the Hugging Face ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "# Configure device and quantization based on environment\n",
    "device_map = \"auto\" if has_gpu else None\n",
    "load_in_8bit = has_gpu  # Only use 8-bit quantization with GPU\n",
    "\n",
    "# Load model with appropriate settings for the environment\n",
    "model = BioGptForCausalLM.from_pretrained(\n",
    "    \"microsoft/biogpt\",\n",
    "    device_map=device_map,\n",
    "    load_in_8bit=load_in_8bit if has_gpu else False,\n",
    "    # If no GPU, use CPU but with lower precision to save memory\n",
    "    torch_dtype=torch.float16 if has_gpu else torch.float32\n",
    ")\n",
    "\n",
    "print(f\"Model 'microsoft/biogpt' loaded successfully!\")\n",
    "print(f\"Model is running on: {'GPU' if has_gpu else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate text from a prompt using our loaded model.\"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate with minimal parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,  # Generate this many new tokens\n",
    "            do_sample=True,\n",
    "            temperature=0.8,  # Slightly higher temperature for more creativity\n",
    "            top_p=0.92,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Get the full output and print it for debugging\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # For BioGPT, sometimes we need to manually extract the response\n",
    "    # Try different approaches to extract the generated text\n",
    "    if prompt + \" \" in full_output:  # Check if prompt is followed by a space\n",
    "        return full_output.split(prompt + \" \", 1)[1]\n",
    "    elif prompt in full_output:\n",
    "        return full_output.split(prompt, 1)[1]\n",
    "    else:\n",
    "        return full_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Interaction with Biomedical Models\n",
    "\n",
    "Now that we have our model loaded, let's try some basic interactions. We'll start with a simple medical question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a simple medical question\n",
    "medical_prompt = \"The symptoms of myocardial infarction include:\"\n",
    "response = generate_text(medical_prompt)\n",
    "print(\"Prompt:\", medical_prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Interaction with Biomedical Models\n",
    "\n",
    "Now that we have our model loaded, let's try some basic interactions. We'll start with a simple medical question. \n",
    "Do you notice any difference to the code cell before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=200):\n",
    "    \"\"\"Generate text from a prompt using our loaded model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    if has_gpu:\n",
    "        inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Generate with basic parameters\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
    "        top_p=0.9,  \n",
    "        do_sample=True,  # Use sampling instead of greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id  # Prevent padding warnings\n",
    "    )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Try a simple medical question\n",
    "medical_prompt = \"What are the common symptoms of type 2 diabetes?\"\n",
    "response = generate_text(medical_prompt)\n",
    "print(\"Prompt:\", medical_prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Model Options for Colab\n",
    "\n",
    "Here are some other biomedical models that work well in Colab's free tier (but require a authentication via Hugging Face token):\n",
    "\n",
    "1. **BioGPT (Microsoft)**: A 1.5B parameter model trained on biomedical literature\n",
    "2. **MedAlpaca-7B**: A 7B parameter model fine-tuned for medical knowledge\n",
    "3. **SciFive**: A T5-based model for biomedical text\n",
    "4. **BioLinkBERT**: Optimized for biomedical entity relationships\n",
    "\n",
    "Here are several biomedical language models that are freely accessible without needing to log in to Hugging Face:\n",
    "\n",
    "- BioBERT models from DMIS Lab:\n",
    "  - dmis-lab/biobert-v1.1\n",
    "  - dmis-lab/biobert-base-cased-v1.1\n",
    "  - dmis-lab/biobert-base-cased-v1.2\n",
    "- Microsoft BioGPT (which you already have working in your notebook):\n",
    "  - microsoft/biogpt\n",
    "- PubMedBERT models:\n",
    "  - microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
    "  - microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\n",
    "- BioMedLM:\n",
    "  - stanford-crfm/BioMedLM\n",
    "- SciFive models:\n",
    "  - razent/SciFive-base-Pubmed_PMC\n",
    "  - razent/SciFive-large-Pubmed_PMC\n",
    "- Clinical models:\n",
    "  - emilyalsentzer/Bio_ClinicalBERT\n",
    "  - EmilyAlsentzer/clinicalBERT\n",
    "- BlueBERT models:\n",
    "  - bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\n",
    "bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12\n",
    "\n",
    "Let's try loading and using biobert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load BioBERT model and tokenizer\n",
    "biobert_tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "biobert_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "if has_gpu:\n",
    "    biobert_model = biobert_model.to(\"cuda\")\n",
    "\n",
    "print(\"BioBERT model loaded successfully!\")\n",
    "\n",
    "# Function to get embeddings from BioBERT\n",
    "def get_biobert_embedding(text):\n",
    "    inputs = biobert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(biobert_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = biobert_model(**inputs)\n",
    "    \n",
    "    # Use the [CLS] token embedding as the sentence embedding\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings\n",
    "\n",
    "# Example: Compare similarity between medical terms\n",
    "def compare_medical_terms(term1, term2):\n",
    "    # Get embeddings\n",
    "    emb1 = get_biobert_embedding(term1)\n",
    "    emb2 = get_biobert_embedding(term2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = torch.nn.functional.cosine_similarity(emb1, emb2).item()\n",
    "    return similarity\n",
    "\n",
    "# Compare some medical terms\n",
    "term_pairs = [\n",
    "    (\"diabetes mellitus\", \"high blood sugar\"),\n",
    "    (\"myocardial infarction\", \"heart attack\"),\n",
    "    (\"hypertension\", \"elevated blood pressure\"),\n",
    "    (\"diabetes mellitus\", \"myocardial infarction\")  # Less related terms\n",
    "]\n",
    "\n",
    "print(\"\\nMedical Term Similarity (BioBERT):\")\n",
    "for term1, term2 in term_pairs:\n",
    "    similarity = compare_medical_terms(term1, term2)\n",
    "    print(f\"Similarity between '{term1}' and '{term2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clear any model from memory by running the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory from previous model\n",
    "del model\n",
    "gc.collect()\n",
    "if has_gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Inference Techniques for Resource-Constrained Environments\n",
    "\n",
    "When working with LLMs in resource-constrained environments, several techniques can help optimize performance:\n",
    "\n",
    "1. **Batch processing**: Process multiple inputs at once\n",
    "2. **Model pruning**: Using smaller, more efficient models\n",
    "3. **Quantization**: Reducing precision to save memory\n",
    "4. **Caching**: Reusing computations when possible\n",
    "5. **Gradient checkpointing**: Trading computation for memory\n",
    "\n",
    "Let's implement a more memory-efficient approach using PubMedBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory from previous models\n",
    "if biobert_model is not None:\n",
    "    del biobert_model\n",
    "    gc.collect()\n",
    "if has_gpu:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load PubMedBERT with memory optimizations\n",
    "pubmed_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Configure model loading based on available resources\n",
    "model_config = {\n",
    "    \"pretrained_model_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "}\n",
    "\n",
    "# Add GPU-specific optimizations if available\n",
    "if has_gpu:\n",
    "    model_config.update({\n",
    "        \"device_map\": \"auto\",\n",
    "        \"torch_dtype\": torch.float16,\n",
    "    })\n",
    "\n",
    "pubmed_model = AutoModelForMaskedLM.from_pretrained(**model_config)\n",
    "\n",
    "print(\"PubMedBERT model loaded successfully!\")\n",
    "\n",
    "# Function for memory-efficient masked word prediction\n",
    "def predict_masked_word(text, mask_position=None):\n",
    "    \"\"\"\n",
    "    Predict a masked word in a sentence.\n",
    "    If mask_position is None, the [MASK] token in the text will be used.\n",
    "    Otherwise, the word at the specified position will be masked.\n",
    "    \"\"\"\n",
    "    if mask_position is not None:\n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "        if 0 <= mask_position < len(words):\n",
    "            # Replace the word at mask_position with [MASK]\n",
    "            masked_word = words[mask_position]\n",
    "            words[mask_position] = pubmed_tokenizer.mask_token\n",
    "            masked_text = \" \".join(words)\n",
    "        else:\n",
    "            raise ValueError(f\"Mask position {mask_position} out of range (0-{len(words)-1})\")\n",
    "    else:\n",
    "        # Use the provided text with [MASK] token\n",
    "        if pubmed_tokenizer.mask_token not in text:\n",
    "            raise ValueError(f\"No {pubmed_tokenizer.mask_token} token found in the text\")\n",
    "        masked_text = text\n",
    "        masked_word = None\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = pubmed_tokenizer(masked_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(pubmed_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Find position of mask token\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == pubmed_tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = pubmed_model(**inputs)\n",
    "    \n",
    "    # Get predicted token\n",
    "    logits = outputs.logits\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Get top 5 predictions\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    top_5_words = [pubmed_tokenizer.decode([token]) for token in top_5_tokens]\n",
    "    \n",
    "    return {\n",
    "        \"masked_text\": masked_text,\n",
    "        \"original_word\": masked_word,\n",
    "        \"top_predictions\": top_5_words\n",
    "    }\n",
    "\n",
    "# Test with medical sentences\n",
    "medical_sentences = [\n",
    "    \"Patients with [MASK] often present with polyuria and polydipsia.\",\n",
    "    \"The patient was diagnosed with [MASK] after an ECG showed ST elevation.\",\n",
    "    \"Treatment with [MASK] is first-line therapy for uncomplicated hypertension.\"\n",
    "]\n",
    "\n",
    "print(\"\\nMedical Term Prediction (PubMedBERT):\")\n",
    "for sentence in medical_sentences:\n",
    "    results = predict_masked_word(sentence)\n",
    "    print(f\"\\nSentence: {results['masked_text']}\")\n",
    "    print(f\"Top predictions: {', '.join(results['top_predictions'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Inference Techniques for Resource-Constrained Environments\n",
    "\n",
    "When working with LLMs in Colab's limited resources, several techniques can help optimize performance:\n",
    "\n",
    "1. **Gradient checkpointing**: Trades computation for memory by recomputing activations during the backward pass\n",
    "2. **Attention caching**: Caches key-value pairs to avoid redundant computations\n",
    "3. **Flash Attention**: More efficient attention implementation\n",
    "4. **Model pruning**: Removing less important weights\n",
    "5. **Low-rank adaptation**: Efficient fine-tuning that requires less memory\n",
    "\n",
    "Let's implement some of these techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with memory optimizations\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load a smaller model with optimizations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"stanford-crfm/BioMedLM\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # Enable gradient checkpointing to save memory\n",
    "    use_cache=False,  # Disable KV cache during training\n",
    "    gradient_checkpointing=True  # Trade computation for memory\n",
    ")\n",
    "\n",
    "# Re-enable caching for inference\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Function for memory-efficient generation\n",
    "def memory_efficient_generate(prompt, max_new_tokens=100):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with optimized settings\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            # Use efficient attention patterns\n",
    "            use_cache=True,  # Enable KV cache for inference\n",
    "            # Process in smaller chunks if needed\n",
    "            max_length=inputs.input_ids.shape[1] + max_new_tokens\n",
    "        )\n",
    "    \n",
    "    # Decode and return\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a complex medical query\n",
    "complex_query = \"\"\"\n",
    "Patient presents with polyuria, polydipsia, and unexplained weight loss over the past 3 months.\n",
    "Lab results show fasting blood glucose of 180 mg/dL and HbA1c of 8.2%.\n",
    "What is the likely diagnosis and what additional tests would you recommend?\n",
    "\"\"\"\n",
    "\n",
    "response = memory_efficient_generate(complex_query, max_new_tokens=200)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Tips for Working with LLMs in Any Environment\n",
    "\n",
    "Here are some additional tips for effectively working with LLMs:\n",
    "\n",
    "1. **Monitor memory usage**: Keep an eye on system resources\n",
    "2. **Start small**: Begin with smaller models and scale up as needed\n",
    "3. **Use text-only outputs**: Avoid generating images or other media that consume memory\n",
    "4. **Batch processing**: Process multiple inputs in batches rather than one at a time\n",
    "5. **Save intermediate results**: Don't rely on keeping the model loaded for long periods\n",
    "\n",
    "### Exercise: Exploring Model Capabilities\n",
    "\n",
    "Try experimenting with different medical prompts to explore the capabilities and limitations of these biomedical LLMs. Here are some suggestions:\n",
    "\n",
    "1. Ask about treatment options for a common condition\n",
    "2. Request an explanation of a medical procedure\n",
    "3. Ask for a differential diagnosis based on symptoms\n",
    "4. Request a summary of recent research on a medical topic\n",
    "\n",
    "Remember that these models, while trained on medical data, may still produce inaccurate information. Always verify any medical information with authoritative sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory from all models\n",
    "try:\n",
    "    del pubmed_model\n",
    "    gc.collect()\n",
    "    if has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Resources cleaned up successfully!\")\n",
    "except:\n",
    "    print(\"No resources to clean up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Medical Prompt Engineering\n",
    "\n",
    "### Introduction to Medical Prompt Engineering\n",
    "\n",
    "Prompt engineering is the art and science of designing effective prompts to get the best possible responses from language models. In medical contexts, this skill becomes even more critical as the accuracy, clarity, and safety of the information can have significant implications.\n",
    "\n",
    "Medical prompt engineering requires understanding both the capabilities of LLMs and the nuances of medical communication. Let's explore how to craft effective prompts for medical applications.\n",
    "\n",
    "### Principles of Effective Medical Prompts\n",
    "\n",
    "When crafting prompts for medical LLMs, consider these key principles:\n",
    "\n",
    "1. **Specificity**: Be precise about the medical context, condition, or scenario\n",
    "2. **Clarity**: Use unambiguous medical terminology when appropriate\n",
    "3. **Scope definition**: Clearly define the boundaries of the requested information\n",
    "4. **Audience awareness**: Specify the intended audience (patient, medical student, clinician)\n",
    "5. **Purpose declaration**: State the purpose of the information (education, decision support, etc.)\n",
    "6. **Safety guardrails**: Include explicit instructions about limitations and disclaimers\n",
    "\n",
    "Let's see how these principles translate into practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Load BioGPT model and tokenizer for our examples\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "model = BioGptForCausalLM.from_pretrained(\n",
    "    \"microsoft/biogpt\",\n",
    "    device_map=\"auto\" if has_gpu else None,\n",
    "    torch_dtype=torch.float16 if has_gpu else torch.float32\n",
    ")\n",
    "\n",
    "def generate_response(prompt, max_length=300):\n",
    "    \"\"\"Generate a response from the model based on the prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Good vs. Poor Medical Prompts\n",
    "\n",
    "Let's compare some examples of effective and ineffective medical prompts. Play around with the prompts and parameters and see how the responses differ.\n",
    "\n",
    "What do you observe? You can also compare it with the improved version in one of the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pairs of poor and good prompts\n",
    "prompt_pairs = [\n",
    "    {\n",
    "        \"poor\": \"Tell me about diabetes. Your answer here:\",\n",
    "        \"good\": \"Provide a concise overview of Type 2 Diabetes Mellitus, including its pathophysiology, common symptoms, diagnostic criteria, and first-line treatments. Target the information for a third-year medical student. Your answer here:\"\n",
    "    },\n",
    "    {\n",
    "        \"poor\": \"What medication should I take for high blood pressure? Your answer here:\",\n",
    "        \"good\": \"Explain the major classes of antihypertensive medications, their mechanisms of action, and common side effects. This information is for educational purposes only and not intended as medical advice. Your answer here:\"\n",
    "    },\n",
    "    {\n",
    "        \"poor\": \"Is this rash dangerous? Your answer here:\",\n",
    "        \"good\": \"Describe the differential diagnosis for common erythematous rashes in adults, including key distinguishing features and red flags that would warrant urgent medical attention. Format the response as a table comparing these conditions. Your answer here:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Compare responses\n",
    "for i, pair in enumerate(prompt_pairs, 1):\n",
    "    print(f\"\\n--- Example {i} ---\\n\")\n",
    "    \n",
    "    print(\"Poor prompt:\")\n",
    "    print(pair[\"poor\"])\n",
    "    poor_response = generate_response(pair[\"poor\"], max_length=600)\n",
    "    print(\"\\nResponse to poor prompt:\")\n",
    "    print(poor_response[:800] + \"...\" if len(poor_response) > 800 else poor_response)\n",
    "    \n",
    "    print(\"\\nGood prompt:\")\n",
    "    print(pair[\"good\"])\n",
    "    good_response = generate_response(pair[\"good\"], max_length=600)\n",
    "    print(\"\\nResponse to good prompt:\")\n",
    "    print(good_response[:800] + \"...\" if len(good_response) > 800 else good_response)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques for Improving Medical Prompt Quality\n",
    "\n",
    "Here are specific techniques to enhance the quality of medical prompts:\n",
    "\n",
    "1. **Role and context specification**\n",
    "   - Assign a specific role to the LLM (e.g., \"Act as a medical educator explaining...\")\n",
    "   - Provide relevant clinical context (e.g., \"In the context of a 65-year-old patient with a history of...\")\n",
    "\n",
    "2. **Format guidance**\n",
    "   - Request specific formats (e.g., \"Present the information as a table comparing...\")\n",
    "   - Ask for structured responses (e.g., \"Organize your response with these headings: Pathophysiology, Symptoms, Diagnosis, Treatment\")\n",
    "\n",
    "3. **Knowledge constraints**\n",
    "   - Specify the knowledge base to draw from (e.g., \"Based on current clinical guidelines...\")\n",
    "   - Set temporal boundaries (e.g., \"Considering treatment approaches developed after 2020...\")\n",
    "\n",
    "4. **Iterative refinement**\n",
    "   - Start with a general prompt and progressively refine based on initial responses\n",
    "   - Use follow-up prompts to address gaps or inaccuracies\n",
    "\n",
    "Let's implement some of these techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_generate_text(prompt, max_new_tokens=150):\n",
    "    \"\"\"Generate text from a prompt using BioGPT with improved extraction.\n",
    "    \n",
    "    This function addresses the issue of BioGPT repeating the prompt by:\n",
    "    1. Using max_new_tokens instead of max_length to ensure new content\n",
    "    2. Properly extracting only the generated portion\n",
    "    3. Using better generation parameters\n",
    "    \"\"\"\n",
    "    # Format the prompt with a clear instruction\n",
    "    formatted_prompt = f\"Question: {prompt}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Calculate the input length to know where generation starts\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Generate with improved parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,  # Generate this many NEW tokens\n",
    "            do_sample=True,\n",
    "            temperature=0.8,  # Slightly higher temperature for more creativity\n",
    "            top_p=0.92,\n",
    "            top_k=50,  # Add top_k sampling\n",
    "            repetition_penalty=1.2,  # Penalize repetition\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "    \n",
    "    # Decode the entire output\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated portion (after the prompt)\n",
    "    if \"Answer:\" in full_output:\n",
    "        generated_text = full_output.split(\"Answer:\")[1].strip()\n",
    "    else:\n",
    "        # If the format is not as expected, try to extract just the new part\n",
    "        generated_text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "# Test the improved function\n",
    "medical_prompt = \"What are the common symptoms of type 2 diabetes?\"\n",
    "response = improved_generate_text(medical_prompt)\n",
    "print(\"Prompt:\", medical_prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts using different techniques\n",
    "advanced_prompts = [\n",
    "    # Role and context specification\n",
    "    \"Act as an experienced cardiologist explaining the interpretation of ECG findings in acute myocardial infarction to a medical resident. Include the pathophysiological basis for each finding.\",\n",
    "    \n",
    "    # Format guidance\n",
    "    \"Present a comparison of first-line treatments for community-acquired pneumonia in tabular format with columns for: Medication Class, Specific Drugs, Dosing, Duration, and Key Considerations. This is for physician reference.\",\n",
    "    \n",
    "    # Knowledge constraints\n",
    "    \"Based on the latest ADA guidelines (post-2020), explain the recommended approach to managing gestational diabetes, focusing only on evidence-based interventions with strong supporting data.\",\n",
    "    \n",
    "    # Iterative refinement (simulated)\n",
    "    \"Initial prompt: Explain the diagnosis of multiple sclerosis.\\n\\nFollow-up refinement: Thank you. Now focus specifically on the McDonald criteria for MS diagnosis and how MRI findings contribute to establishing the diagnosis. Include the concept of lesions disseminated in time and space.\"\n",
    "]\n",
    "\n",
    "# Generate responses for each advanced prompt\n",
    "print(\"\\n=== Advanced Prompt Engineering Techniques ===\\n\")\n",
    "\n",
    "for i, prompt in enumerate(advanced_prompts, 1):\n",
    "    print(f\"\\n--- Technique Example {i} ---\\n\")\n",
    "    print(\"Advanced prompt:\")\n",
    "    print(prompt)\n",
    "    \n",
    "    response = improved_generate_text(prompt, max_new_tokens=1000)\n",
    "    \n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[:1000] + \"...\" if len(response) > 1000 else response)\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates for Common Medical Tasks\n",
    "\n",
    "Here are some template prompts for common medical tasks that you can adapt for your specific needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_templates = {\n",
    "    \"Patient Education\": \"\"\"\n",
    "Create patient education material explaining {condition} in simple, non-technical language appropriate for a {education_level} reading level. Include:\n",
    "1. What is {condition} and how it affects the body\n",
    "2. Common symptoms to watch for\n",
    "3. When to seek medical attention\n",
    "4. Basic self-management strategies\n",
    "5. Common misconceptions about the condition\n",
    "\n",
    "Avoid medical jargon, and when medical terms must be used, provide simple explanations.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Differential Diagnosis\": \"\"\"\n",
    "For a patient presenting with the following symptoms and findings:\n",
    "- Symptoms: {symptoms}\n",
    "- Patient demographics: {demographics}\n",
    "- Relevant history: {history}\n",
    "- Physical exam findings: {exam_findings}\n",
    "- Lab/imaging results: {results}\n",
    "\n",
    "Provide a ranked differential diagnosis with the following for each potential condition:\n",
    "1. Condition name\n",
    "2. Key supporting features from the case\n",
    "3. Features that don't fit or argue against this diagnosis\n",
    "4. Critical diagnostic tests to confirm or rule out\n",
    "\"\"\",\n",
    "    \n",
    "    \"Treatment Summary\": \"\"\"\n",
    "Summarize the current evidence-based treatment approach for {condition} in {patient_type} patients.\n",
    "Structure your response with these sections:\n",
    "1. First-line therapies with dosing considerations\n",
    "2. Second-line options when first-line fails\n",
    "3. Adjunctive treatments\n",
    "4. Monitoring parameters and follow-up\n",
    "5. Treatment considerations for special populations\n",
    "\n",
    "Base your response on current clinical guidelines and high-quality evidence.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Research Summary\": \"\"\"\n",
    "Provide a structured summary of recent research advances (past 5 years) in {research_area} related to {condition}.\n",
    "Include:\n",
    "1. Major breakthrough findings\n",
    "2. Emerging therapeutic targets\n",
    "3. New diagnostic approaches\n",
    "4. Ongoing clinical trials of significance\n",
    "5. Remaining knowledge gaps and future directions\n",
    "\n",
    "Focus on findings with the strongest methodology and potential clinical impact.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Display the templates\n",
    "print(\"\\n=== Medical Prompt Templates ===\\n\")\n",
    "\n",
    "for task, template in medical_templates.items():\n",
    "    print(f\"## Template for {task}\")\n",
    "    print(template)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Improving Medical Prompts\n",
    "\n",
    "Now it's your turn to practice improving medical prompts. Here are some exercises to help you develop your medical prompt engineering skills:\n",
    "\n",
    "#### Exercise 1: Transform Basic Prompts\n",
    "Take these basic medical prompts and transform them into more effective ones using the principles we've discussed:\n",
    "\n",
    "1. \"What causes asthma?\"\n",
    "2. \"Explain heart failure treatments.\"\n",
    "3. \"How do antibiotics work?\"\n",
    "\n",
    "#### Exercise 2: Create Role-Based Prompts\n",
    "Create prompts that assign specific roles to the LLM for these scenarios:\n",
    "\n",
    "1. Explaining a new diabetes medication to a patient\n",
    "2. Teaching medical students about neuroanatomy\n",
    "3. Briefing hospital administrators on infection control measures\n",
    "\n",
    "#### Exercise 3: Format-Specific Prompts\n",
    "Design prompts that request specific output formats for these medical topics:\n",
    "\n",
    "1. A comparison table of common analgesics\n",
    "2. A step-by-step protocol for managing anaphylaxis\n",
    "3. A decision tree for evaluating acute chest pain\n",
    "\n",
    "#### Exercise 4: Complete Template Prompts\n",
    "Fill in the template prompts we provided with specific medical conditions and parameters:\n",
    "\n",
    "1. Complete the Patient Education template for hypertension\n",
    "2. Complete the Differential Diagnosis template for a patient with fever and joint pain\n",
    "3. Complete the Treatment Summary template for major depressive disorder\n",
    "\n",
    "Remember to apply the principles of specificity, clarity, scope definition, audience awareness, purpose declaration, and safety guardrails in your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory from all models\n",
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Resources cleaned up successfully!\")\n",
    "except:\n",
    "    print(\"No resources to clean up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Simple Medical RAG in Colab\n",
    "\n",
    "### Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that enhances language model outputs by first retrieving relevant information from a knowledge base and then using that information to generate more accurate and informed responses.\n",
    "\n",
    "In medical contexts, RAG is particularly valuable because:\n",
    "\n",
    "1. **Improved accuracy**: Grounds responses in verified medical information\n",
    "2. **Reduced hallucinations**: Minimizes fabricated or incorrect medical claims\n",
    "3. **Up-to-date knowledge**: Can incorporate recent medical literature\n",
    "4. **Transparency**: Provides sources for medical information\n",
    "5. **Specialized knowledge**: Accesses domain-specific medical information\n",
    "\n",
    "Let's explore how to implement a lightweight RAG system for medical applications that works within Colab's memory constraints.\n",
    "\n",
    "### Components of a Medical RAG System\n",
    "\n",
    "A basic RAG system consists of these components:\n",
    "\n",
    "1. **Knowledge Base**: A collection of medical documents or information\n",
    "2. **Embeddings Model**: Converts text into vector representations\n",
    "3. **Vector Database**: Stores and enables semantic search of embeddings\n",
    "4. **Retriever**: Finds relevant documents based on a query\n",
    "5. **Generator**: Creates responses using retrieved information and the query\n",
    "\n",
    "Let's build each component step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Check environment\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "has_gpu = torch.cuda.is_available()\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"GPU available: {has_gpu}\")\n",
    "\n",
    "# Install additional packages if needed\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    !pip install -q faiss-cpu\n",
    "    import faiss\n",
    "\n",
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "except ImportError:\n",
    "    !pip install -q langchain\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Medical Knowledge Base\n",
    "\n",
    "First, let's create a small medical knowledge base. For this example, we'll use a collection of medical texts about common conditions. In a real application, you might use medical textbooks, guidelines, or research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample medical texts about common conditions\n",
    "medical_texts = [\n",
    "    \"\"\"Diabetes mellitus is a group of metabolic disorders characterized by a high blood sugar level over a prolonged period of time. Symptoms often include frequent urination, increased thirst and increased appetite. If left untreated, diabetes can cause many health complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, damage to the nerves, damage to the eyes and cognitive impairment.\n",
    "\n",
    "    Type 1 diabetes results from the pancreas's failure to produce enough insulin due to loss of beta cells. This form was previously referred to as \"insulin-dependent diabetes mellitus\" or \"juvenile diabetes\". The loss of beta cells is caused by an autoimmune response. The cause of this autoimmune response is unknown.\n",
    "\n",
    "    Type 2 diabetes begins with insulin resistance, a condition in which cells fail to respond to insulin properly. As the disease progresses, a lack of insulin may also develop. This form was previously referred to as \"non insulin-dependent diabetes mellitus\" or \"adult-onset diabetes\". The most common cause is a combination of excessive body weight and insufficient exercise.\n",
    "\n",
    "    Prevention and treatment involve maintaining a healthy diet, regular physical exercise, a normal body weight, and avoiding use of tobacco. Control of blood pressure and maintaining proper foot and eye care are important for people with the disease. Type 1 diabetes must be managed with insulin injections. Type 2 diabetes may be treated with medications with or without insulin. Insulin and some oral medications can cause low blood sugar. Weight loss surgery in those with obesity is sometimes an effective measure in those with type 2 diabetes. Gestational diabetes usually resolves after the birth of the baby.\"\"\",\n",
    "    \n",
    "    \"\"\"Hypertension, also known as high blood pressure, is a long-term medical condition in which the blood pressure in the arteries is persistently elevated. High blood pressure typically does not cause symptoms. Long-term high blood pressure, however, is a major risk factor for stroke, coronary artery disease, heart failure, atrial fibrillation, peripheral arterial disease, vision loss, chronic kidney disease, and dementia.\n",
    "\n",
    "    High blood pressure is classified as primary (essential) hypertension or secondary hypertension. About 90â95% of cases are primary, defined as high blood pressure due to nonspecific lifestyle and genetic factors. Lifestyle factors that increase the risk include excess salt in the diet, excess body weight, smoking, and alcohol use. The remaining 5â10% of cases are categorized as secondary high blood pressure, defined as high blood pressure due to an identifiable cause, such as chronic kidney disease, narrowing of the kidney arteries, an endocrine disorder, or the use of birth control pills.\n",
    "\n",
    "    Blood pressure is expressed by two measurements, the systolic and diastolic pressures, which are the maximum and minimum pressures, respectively. For most adults, normal blood pressure at rest is within the range of 100â130 millimeters mercury (mmHg) systolic and 60â80 mmHg diastolic. For most adults, high blood pressure is present if the resting blood pressure is persistently at or above 130/80 or 140/90 mmHg. Different numbers apply to children. Ambulatory blood pressure monitoring over a 24-hour period appears more accurate than office-based blood pressure measurement.\n",
    "\n",
    "    Lifestyle changes and medications can lower blood pressure and decrease the risk of health complications. Lifestyle changes include weight loss, physical exercise, decreased salt intake, reducing alcohol intake, and a healthy diet. If lifestyle changes are not sufficient, then blood pressure medications are used. Several medications are effective for treating hypertension.\"\"\",\n",
    "    \n",
    "    \"\"\"Asthma is a long-term inflammatory disease of the airways of the lungs. It is characterized by variable and recurring symptoms, reversible airflow obstruction, and easily triggered bronchospasms. Symptoms include episodes of wheezing, coughing, chest tightness, and shortness of breath. These may occur a few times a day or a few times per week. Depending on the person, asthma symptoms may become worse at night or with exercise.\n",
    "\n",
    "    Asthma is thought to be caused by a combination of genetic and environmental factors. Environmental factors include exposure to air pollution and allergens. Other potential triggers include medications such as aspirin and beta blockers. Diagnosis is usually based on the pattern of symptoms, response to therapy over time, and spirometry. Asthma is classified according to the frequency of symptoms, forced expiratory volume in one second (FEV1), and peak expiratory flow rate. It may also be classified as atopic or non-atopic, where atopy refers to a predisposition toward developing a type 1 hypersensitivity reaction.\n",
    "\n",
    "    There is no cure for asthma. Symptoms can be prevented by avoiding triggers, such as allergens and irritants, and by the use of inhaled corticosteroids. Long-acting beta agonists (LABA) or antileukotriene agents may be used in addition to inhaled corticosteroids if asthma symptoms remain uncontrolled. Treatment of rapidly worsening symptoms is usually with an inhaled short-acting beta-2 agonist such as salbutamol and corticosteroids taken by mouth. In very severe cases, intravenous corticosteroids, magnesium sulfate, and hospitalization may be required.\n",
    "\n",
    "    In 2019, 262 million people globally had asthma, up from 183 million in 1990. It caused about 455,000 deaths in 2019, most of which occurred in the developing world. Asthma often begins in childhood, and the rates have increased significantly since the 1960s. Asthma was recognized as early as Ancient Egypt. The word \"asthma\" is from the Greek á¼ÏÎ¸Î¼Î±, meaning \"panting\".\"\"\",\n",
    "    \n",
    "    \"\"\"Coronary artery disease (CAD), also called coronary heart disease (CHD), ischemic heart disease (IHD), or simply heart disease, involves the reduction of blood flow to the heart muscle due to build-up of plaque (atherosclerosis) in the arteries of the heart. It is the most common of the cardiovascular diseases. Types include stable angina, unstable angina, myocardial infarction, and sudden cardiac death. A common symptom is chest pain or discomfort which may travel into the shoulder, arm, back, neck, or jaw. Occasionally it may feel like heartburn. Usually symptoms occur with exercise or emotional stress, last less than a few minutes, and improve with rest. Shortness of breath may also occur and sometimes no symptoms are present. In many cases, the first sign is a heart attack. Other complications include heart failure or an abnormal heartbeat.\n",
    "\n",
    "    Risk factors include high blood pressure, smoking, diabetes, lack of exercise, obesity, high blood cholesterol, poor diet, depression, and excessive alcohol. A number of tests may help with diagnoses including: electrocardiogram, cardiac stress testing, coronary computed tomographic angiography, and coronary angiogram, among others.\n",
    "\n",
    "    Prevention is by eating a healthy diet, regular exercise, maintaining a healthy weight and not smoking. Sometimes medication for diabetes, high cholesterol, or high blood pressure are also used. There is limited evidence for screening people who are at low risk and do not have symptoms. Treatment involves the same measures as prevention. Additional medications such as antiplatelets (including aspirin), beta blockers, or nitroglycerin may be recommended. Procedures such as percutaneous coronary intervention (PCI) or coronary artery bypass graft (CABG) surgery may be used in severe disease. In those with stable CAD it is unclear if PCI or CABG in addition to the other treatments improves life expectancy or decreases heart attack risk.\"\"\",\n",
    "    \n",
    "    \"\"\"Alzheimer's disease (AD) is a neurodegenerative disease that usually starts slowly and progressively worsens. It is the cause of 60â70% of cases of dementia. The most common early symptom is difficulty in remembering recent events. As the disease advances, symptoms can include problems with language, disorientation (including easily getting lost), mood swings, loss of motivation, self-neglect, and behavioral issues. As a person's condition declines, they often withdraw from family and society. Gradually, bodily functions are lost, ultimately leading to death. Although the speed of progression can vary, the typical life expectancy following diagnosis is three to nine years.\n",
    "\n",
    "    The cause of Alzheimer's disease is poorly understood. There are many environmental and genetic risk factors associated with its development. The strongest genetic risk factor is from an allele of APOE. Other risk factors include a history of head injury, clinical depression, and high blood pressure. The disease process is largely associated with amyloid plaques, neurofibrillary tangles, and loss of neuronal connections in the brain. A probable diagnosis is based on the history of the illness and cognitive testing with medical imaging and blood tests to rule out other possible causes. Initial symptoms are often mistaken for normal aging. Examination of brain tissue is needed for a definite diagnosis, but this can only be done after death. Good nutrition, physical exercise, and social engagement are known to be of benefit generally in aging, and these may help in reducing the risk of cognitive decline and Alzheimer's; in 2019, clinical trials were underway to look at these possibilities. There are no medications or supplements that have been shown to decrease risk.\n",
    "\n",
    "    No treatments stop or reverse its progression, though some may temporarily improve symptoms. Affected people increasingly rely on others for assistance, often placing a burden on the caregiver. The pressures can include social, psychological, physical, and economic elements. Exercise programs may be beneficial with respect to activities of daily living and can potentially improve outcomes. Behavioral problems or psychosis due to dementia are often treated with antipsychotics, but this is not usually recommended, as there is little benefit and an increased risk of early death.\"\"\"\n",
    "]\n",
    "\n",
    "# Create a directory for our knowledge base\n",
    "!mkdir -p medical_kb\n",
    "\n",
    "# Save each text as a separate file\n",
    "for i, text in enumerate(medical_texts):\n",
    "    with open(f\"medical_kb/condition_{i+1}.txt\", \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(f\"Created {len(medical_texts)} medical documents in the knowledge base.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Knowledge Base\n",
    "\n",
    "Next, we need to process our knowledge base by splitting the documents into smaller chunks that are easier to embed and retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load documents from our knowledge base\n",
    "def load_documents(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\") as f:\n",
    "                documents.append({\"content\": f.read(), \"source\": filename})\n",
    "    return documents\n",
    "\n",
    "# Load our medical documents\n",
    "medical_docs = load_documents(\"medical_kb\")\n",
    "print(f\"Loaded {len(medical_docs)} documents from the knowledge base.\")\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "document_chunks = []\n",
    "for doc in medical_docs:\n",
    "    chunks = text_splitter.split_text(doc[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        document_chunks.append({\n",
    "            \"content\": chunk,\n",
    "            \"source\": doc[\"source\"]\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(document_chunks)} chunks from the original documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings\n",
    "\n",
    "Now we'll create embeddings for each chunk using a sentence transformer model that's optimized for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence transformer model for creating embeddings\n",
    "# Using a smaller model that works well for biomedical text\n",
    "model_name = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to create embeddings\n",
    "def create_embeddings(chunks, model):\n",
    "    texts = [chunk[\"content\"] for chunk in chunks]\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "# Create embeddings for our document chunks\n",
    "chunk_embeddings = create_embeddings(document_chunks, embedding_model)\n",
    "print(f\"Created embeddings with dimension: {chunk_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Vector Database\n",
    "\n",
    "Next, we'll create a simple vector database using FAISS, which is efficient for similarity search and works well in resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to the format required by FAISS\n",
    "embeddings_np = np.array(chunk_embeddings).astype('float32')\n",
    "\n",
    "# Create the FAISS index\n",
    "dimension = embeddings_np.shape[1]  # Get the embedding dimension\n",
    "index = faiss.IndexFlatL2(dimension)  # Using L2 distance for similarity\n",
    "index.add(embeddings_np)  # Add embeddings to the index\n",
    "\n",
    "print(f\"Created FAISS index with {index.ntotal} vectors of dimension {dimension}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Retriever\n",
    "\n",
    "Now we'll create a retriever function that will find the most relevant chunks for a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant chunks for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        top_k: Number of most relevant chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of relevant chunks with their content and source\n",
    "    \"\"\"\n",
    "    # Create embedding for the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get the relevant chunks\n",
    "    relevant_chunks = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(document_chunks):  # Safety check\n",
    "            chunk = document_chunks[idx]\n",
    "            relevant_chunks.append({\n",
    "                \"content\": chunk[\"content\"],\n",
    "                \"source\": chunk[\"source\"],\n",
    "                \"relevance_score\": 1.0 / (1.0 + distances[0][i])  # Convert distance to similarity score\n",
    "            })\n",
    "    \n",
    "    return relevant_chunks\n",
    "\n",
    "# Test the retriever\n",
    "test_query = \"What are the symptoms of diabetes?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(relevant_chunks)} relevant chunks:\")\n",
    "for i, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"\\nChunk {i+1} (from {chunk['source']}, score: {chunk['relevance_score']:.4f}):\")\n",
    "    print(chunk[\"content\"][:200] + \"...\" if len(chunk[\"content\"]) > 200 else chunk[\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Generator\n",
    "\n",
    "Now we'll implement the generator component that will use the retrieved information to generate accurate responses to medical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a language model for generation\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
    "\n",
    "# Clear memory from previous models\n",
    "gc.collect()\n",
    "if has_gpu:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load BioGPT model and tokenizer\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "generator_model = BioGptForCausalLM.from_pretrained(\n",
    "    \"microsoft/biogpt\",\n",
    "    device_map=\"auto\" if has_gpu else None,\n",
    "    torch_dtype=torch.float16 if has_gpu else torch.float32\n",
    ")\n",
    "\n",
    "def generate_response_with_context(query, context_chunks, max_length=300):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved context chunks.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query\n",
    "        context_chunks: Retrieved relevant chunks\n",
    "        max_length: Maximum length of the generated response\n",
    "        \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    # Combine context chunks into a single context string\n",
    "    context = \"\\n\\n\".join([chunk[\"content\"] for chunk in context_chunks])\n",
    "    \n",
    "    # Create a prompt that includes both the context and query\n",
    "    prompt = f\"\"\"Based on the following medical information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Please answer the following question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(generator_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate with parameters optimized for factual accuracy\n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=inputs.input_ids.shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated answer, not the prompt\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Try to extract just the answer part\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if answer_prefix in full_response:\n",
    "        answer = full_response.split(answer_prefix)[1].strip()\n",
    "    else:\n",
    "        # If we can't find the answer prefix, return everything after the query\n",
    "        answer = full_response.split(query)[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the generator\n",
    "test_queries = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is hypertension diagnosed?\",\n",
    "    \"What treatments are available for asthma?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the complete RAG system with medical queries:\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    relevant_chunks = retrieve_relevant_chunks(query)\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response_with_context(query, relevant_chunks)\n",
    "    \n",
    "    # Display sources\n",
    "    sources = [chunk[\"source\"] for chunk in relevant_chunks]\n",
    "    unique_sources = list(set(sources))\n",
    "    \n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Sources: {', '.join(unique_sources)}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing RAG vs. Non-RAG Outputs\n",
    "\n",
    "Let's compare the responses from our RAG system with responses from the model without retrieval to see the difference in accuracy and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_without_rag(query, max_length=300):\n",
    "    \"\"\"Generate a response without using RAG (no retrieved context).\"\"\"\n",
    "    prompt = f\"Please answer the following medical question:\\n{query}\\n\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(generator_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=inputs.input_ids.shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated answer\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Try to extract just the answer part\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if answer_prefix in full_response:\n",
    "        answer = full_response.split(answer_prefix)[1].strip()\n",
    "    else:\n",
    "        answer = full_response.split(query)[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Compare RAG vs. Non-RAG for a few queries\n",
    "comparison_queries = [\n",
    "    \"What are the long-term complications of diabetes?\",\n",
    "    \"What lifestyle changes can help manage hypertension?\",\n",
    "    \"How does Alzheimer's disease progress over time?\"\n",
    "]\n",
    "\n",
    "print(\"Comparing RAG vs. Non-RAG responses:\\n\")\n",
    "\n",
    "for query in comparison_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Get RAG response\n",
    "    relevant_chunks = retrieve_relevant_chunks(query)\n",
    "    rag_response = generate_response_with_context(query, relevant_chunks)\n",
    "    \n",
    "    # Get Non-RAG response\n",
    "    non_rag_response = generate_response_without_rag(query)\n",
    "    \n",
    "    print(f\"\\nRAG Response:\\n{rag_response}\")\n",
    "    print(f\"\\nNon-RAG Response:\\n{non_rag_response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing RAG for Limited Resources\n",
    "\n",
    "When working with RAG systems in resource-constrained environments like Colab, consider these optimization strategies:\n",
    "\n",
    "1. **Use smaller embedding models**: Choose efficient models like MiniLM or small domain-specific models\n",
    "2. **Limit chunk size and number**: Use smaller chunks and retrieve fewer chunks\n",
    "3. **Use quantized models**: 8-bit or 4-bit quantization reduces memory requirements\n",
    "4. **Implement caching**: Cache embeddings and retrieved results\n",
    "5. **Use efficient vector databases**: FAISS with flat indexes works well for small to medium collections\n",
    "\n",
    "Let's implement some of these optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_rag_query(query, top_k=2, max_length=200):\n",
    "    \"\"\"\n",
    "    An optimized version of the RAG system for resource-constrained environments.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query\n",
    "        top_k: Number of chunks to retrieve (smaller for optimization)\n",
    "        max_length: Maximum response length (smaller for optimization)\n",
    "        \n",
    "    Returns:\n",
    "        Generated response and sources\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve with fewer chunks\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Use a more concise prompt\n",
    "    context = \"\\n\".join([chunk[\"content\"] for chunk in relevant_chunks])\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Step 3: Generate with more constrained parameters\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(generator_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate with optimized parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=inputs.input_ids.shape[1] + max_length,\n",
    "            temperature=0.5,  # Lower temperature for more deterministic output\n",
    "            top_p=0.85,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            # Additional optimizations\n",
    "            num_beams=1,  # Disable beam search to save memory\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Extract response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer\n",
    "    if \"Answer:\" in full_response:\n",
    "        answer = full_response.split(\"Answer:\")[1].strip()\n",
    "    else:\n",
    "        answer = full_response.split(query)[-1].strip()\n",
    "    \n",
    "    # Get sources\n",
    "    sources = [chunk[\"source\"] for chunk in relevant_chunks]\n",
    "    \n",
    "    return answer, sources\n",
    "\n",
    "# Test the optimized RAG\n",
    "optimized_test_queries = [\n",
    "    \"What are the risk factors for coronary artery disease?\",\n",
    "    \"How is asthma diagnosed?\",\n",
    "    \"What are the treatment options for Alzheimer's disease?\"\n",
    "]\n",
    "\n",
    "print(\"Testing optimized RAG for resource-constrained environments:\\n\")\n",
    "\n",
    "for query in optimized_test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Use optimized RAG\n",
    "    start_time = time.time()\n",
    "    response, sources = optimized_rag_query(query)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Sources: {', '.join(sources)}\")\n",
    "    print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Building Your Own Medical RAG System\n",
    "\n",
    "Now it's your turn to experiment with building a RAG system. Here are some exercises to try:\n",
    "\n",
    "1. **Expand the knowledge base**: Add more medical documents to the knowledge base, focusing on a specific area of medicine you're interested in.\n",
    "\n",
    "2. **Optimize for your use case**: Adjust the chunk size, overlap, and retrieval parameters to find the optimal configuration for your specific medical domain.\n",
    "\n",
    "3. **Evaluate accuracy**: Test your RAG system with a set of medical questions and evaluate the accuracy of the responses compared to authoritative sources.\n",
    "\n",
    "4. **Implement a specialized retriever**: Modify the retriever to prioritize recent medical information or to filter results based on criteria like medical specialty.\n",
    "\n",
    "5. **Create a domain-specific RAG**: Build a RAG system focused on a particular medical specialty (e.g., cardiology, neurology, pediatrics) with a curated knowledge base.\n",
    "\n",
    "Remember that while RAG systems can significantly improve the accuracy of LLM responses for medical applications, they should still be used with caution and their outputs should be verified by medical professionals before being applied in clinical settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory from all models\n",
    "try:\n",
    "    del generator_model\n",
    "    del embedding_model\n",
    "    gc.collect()\n",
    "    if has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Resources cleaned up successfully!\")\n",
    "except:\n",
    "    print(\"No resources to clean up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with Pre-trained Medical LLMs\n",
    "\n",
    "### Introduction to Parameter-Efficient Fine-Tuning\n",
    "\n",
    "While full fine-tuning of large language models requires significant computational resources, parameter-efficient fine-tuning methods allow us to adapt pre-trained models to specific medical tasks with much less computational overhead. These approaches are ideal for environments like Google Colab or local machines with limited GPU resources.\n",
    "\n",
    "In this section, we'll explore:\n",
    "1. Adapter-based fine-tuning concepts\n",
    "2. How to access pre-trained medical models via HuggingFace\n",
    "3. Parameter-efficient methods suitable for resource-constrained environments\n",
    "4. Techniques to maximize performance when working with larger models\n",
    "\n",
    "### Understanding Adapter-Based Fine-Tuning\n",
    "\n",
    "Traditional fine-tuning updates all parameters of a pre-trained model, which can be billions of parameters for modern LLMs. In contrast, adapter-based approaches:\n",
    "\n",
    "- Keep most of the pre-trained model frozen (weights unchanged)\n",
    "- Insert small, trainable adapter modules between layers of the frozen model\n",
    "- Update only these adapter modules during training (typically <1% of the total parameters)\n",
    "- Preserve the general knowledge of the pre-trained model while adapting to new tasks\n",
    "\n",
    "This approach drastically reduces memory requirements and training time while often achieving comparable performance to full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import time\n",
    "\n",
    "# Check environment\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "has_gpu = torch.cuda.is_available()\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"GPU available: {has_gpu}\")\n",
    "\n",
    "if has_gpu:\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU only. Some operations may be slower.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Pre-trained Medical Models\n",
    "\n",
    "HuggingFace's model hub hosts numerous pre-trained models specifically designed for biomedical and clinical applications. Let's explore some of these models and how to use them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of notable biomedical pre-trained models\n",
    "biomedical_models = [\n",
    "    {\n",
    "        \"name\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"description\": \"BERT model trained from scratch on PubMed abstracts and full-text articles\",\n",
    "        \"size\": \"~420MB\",\n",
    "        \"suitable_for\": \"Biomedical text classification, NER, QA\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"dmis-lab/biobert-v1.1\",\n",
    "        \"description\": \"BioBERT is a biomedical language representation model pre-trained on PubMed abstracts and PMC full-text articles\",\n",
    "        \"size\": \"~420MB\",\n",
    "        \"suitable_for\": \"Biomedical text mining tasks\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"microsoft/biogpt\",\n",
    "        \"description\": \"GPT model trained on biomedical research papers\",\n",
    "        \"size\": \"~1.5GB\",\n",
    "        \"suitable_for\": \"Biomedical text generation, summarization\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "        \"description\": \"BERT model further pre-trained on clinical notes from MIMIC-III\",\n",
    "        \"size\": \"~420MB\",\n",
    "        \"suitable_for\": \"Clinical text understanding, medical NLP\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"allenai/scibert_scivocab_uncased\",\n",
    "        \"description\": \"BERT model trained on scientific papers from Semantic Scholar\",\n",
    "        \"size\": \"~420MB\",\n",
    "        \"suitable_for\": \"Scientific and biomedical text processing\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display the models\n",
    "print(\"Pre-trained Biomedical Language Models:\\n\")\n",
    "for i, model in enumerate(biomedical_models, 1):\n",
    "    print(f\"{i}. {model['name']}\")\n",
    "    print(f\"   Description: {model['description']}\")\n",
    "    print(f\"   Size: {model['size']}\")\n",
    "    print(f\"   Suitable for: {model['suitable_for']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-Efficient Fine-Tuning Methods\n",
    "\n",
    "Several parameter-efficient fine-tuning methods have been developed that work well in resource-constrained environments:\n",
    "\n",
    "1. **Adapters**: Small bottleneck layers inserted between transformer layers\n",
    "2. **LoRA (Low-Rank Adaptation)**: Adds low-rank matrices to existing weights\n",
    "3. **Prefix Tuning**: Adds trainable prefix vectors to each transformer layer\n",
    "4. **P-Tuning**: Optimizes continuous prompts instead of discrete text prompts\n",
    "5. **BitFit**: Only fine-tunes bias terms in the model\n",
    "\n",
    "Let's implement a parameter-efficient fine-tuning approach using adapters for a medical text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the MeQSum dataset (Medical Question Summarization)\n",
    "# This is a dataset of medical questions and their categories\n",
    "try:\n",
    "    medical_dataset = load_dataset(\"medical_questions\", \"meqsum\")\n",
    "    print(\"Loaded MeQSum dataset\")\n",
    "except:\n",
    "    # If the dataset isn't available through Hugging Face, we'll create a simple medical dataset\n",
    "    print(\"Creating a simple medical classification dataset\")\n",
    "    \n",
    "    # Sample medical questions and their categories\n",
    "    medical_questions = [\n",
    "        # Cardiology questions\n",
    "        {\"text\": \"What are the symptoms of a heart attack?\", \"label\": 0},\n",
    "        {\"text\": \"How is congestive heart failure diagnosed?\", \"label\": 0},\n",
    "        {\"text\": \"What is the difference between systolic and diastolic blood pressure?\", \"label\": 0},\n",
    "        {\"text\": \"Can you explain what causes atrial fibrillation?\", \"label\": 0},\n",
    "        {\"text\": \"What are the risk factors for coronary artery disease?\", \"label\": 0},\n",
    "        {\"text\": \"How does a pacemaker work?\", \"label\": 0},\n",
    "        {\"text\": \"What is the treatment for hypertension?\", \"label\": 0},\n",
    "        {\"text\": \"Can you explain what an ECG measures?\", \"label\": 0},\n",
    "        \n",
    "        # Neurology questions\n",
    "        {\"text\": \"What are the early signs of Alzheimer's disease?\", \"label\": 1},\n",
    "        {\"text\": \"How is multiple sclerosis diagnosed?\", \"label\": 1},\n",
    "        {\"text\": \"What causes seizures?\", \"label\": 1},\n",
    "        {\"text\": \"What is the difference between a stroke and a TIA?\", \"label\": 1},\n",
    "        {\"text\": \"How does Parkinson's disease progress?\", \"label\": 1},\n",
    "        {\"text\": \"What treatments are available for migraines?\", \"label\": 1},\n",
    "        {\"text\": \"Can you explain what happens during a concussion?\", \"label\": 1},\n",
    "        {\"text\": \"What are the symptoms of peripheral neuropathy?\", \"label\": 1},\n",
    "        \n",
    "        # Endocrinology questions\n",
    "        {\"text\": \"What are the symptoms of type 2 diabetes?\", \"label\": 2},\n",
    "        {\"text\": \"How is hypothyroidism diagnosed?\", \"label\": 2},\n",
    "        {\"text\": \"What causes adrenal insufficiency?\", \"label\": 2},\n",
    "        {\"text\": \"What is the difference between type 1 and type 2 diabetes?\", \"label\": 2},\n",
    "        {\"text\": \"How does insulin resistance develop?\", \"label\": 2},\n",
    "        {\"text\": \"What treatments are available for Graves' disease?\", \"label\": 2},\n",
    "        {\"text\": \"Can you explain how hormones affect metabolism?\", \"label\": 2},\n",
    "        {\"text\": \"What are the symptoms of Cushing's syndrome?\", \"label\": 2}\n",
    "    ]\n",
    "    \n",
    "    # Create train and test splits\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train_data, test_data = train_test_split(medical_questions, test_size=0.25, stratify=[q[\"label\"] for q in medical_questions], random_state=42)\n",
    "    \n",
    "    # Create a simple dataset dictionary\n",
    "    medical_dataset = {\n",
    "        \"train\": {\"text\": [q[\"text\"] for q in train_data], \"label\": [q[\"label\"] for q in train_data]},\n",
    "        \"test\": {\"text\": [q[\"text\"] for q in test_data], \"label\": [q[\"label\"] for q in test_data]}\n",
    "    }\n",
    "    \n",
    "    # Convert to HF Dataset format\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    medical_dataset = {\n",
    "        \"train\": Dataset.from_dict({\"text\": [q[\"text\"] for q in train_data], \"label\": [q[\"label\"] for q in train_data]}),\n",
    "        \"test\": Dataset.from_dict({\"text\": [q[\"text\"] for q in test_data], \"label\": [q[\"label\"] for q in test_data]})\n",
    "    }\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"Created medical dataset with {len(medical_dataset['train'])} training examples and {len(medical_dataset['test'])} test examples\")\n",
    "    print(f\"Labels: Cardiology (0), Neurology (1), Endocrinology (2)\")\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\nExample questions from each category:\")\n",
    "label_names = [\"Cardiology\", \"Neurology\", \"Endocrinology\"]\n",
    "for label in range(3):\n",
    "    examples = [item for item in medical_dataset[\"train\"] if item[\"label\"] == label][:2]\n",
    "    for ex in examples:\n",
    "        print(f\"{label_names[ex['label']]}: {ex['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Parameter-Efficient Fine-Tuning with Adapters\n",
    "\n",
    "Now, let's implement adapter-based fine-tuning using the `adapters` library, which allows us to efficiently adapt pre-trained models to our medical classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import adapters\n",
    "except ImportError:\n",
    "    !pip install -q adapters\n",
    "    import adapters\n",
    "\n",
    "from adapters import AdapterConfig, AutoAdapterModel\n",
    "from adapters.composition import Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained biomedical model\n",
    "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = medical_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = medical_dataset[\"test\"].map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Model with Adapters\n",
    "\n",
    "Now we'll load the pre-trained model and add adapters for our medical classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model with adapter support\n",
    "model = AutoAdapterModel.from_pretrained(model_name)\n",
    "\n",
    "# Get the number of labels in our dataset\n",
    "num_labels = len(set(medical_dataset[\"train\"][\"label\"]))\n",
    "\n",
    "# Add a classification head\n",
    "model.add_classification_head(\n",
    "    \"medical_classification\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Add a new adapter for our task\n",
    "adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=16)  # Smaller adapter for efficiency\n",
    "model.add_adapter(\"medical_adapter\", config=adapter_config)\n",
    "\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"medical_adapter\")\n",
    "\n",
    "# Set the active adapters\n",
    "model.set_active_adapters(Stack(\"medical_adapter\"))\n",
    "\n",
    "# Freeze the model parameters except for the adapter\n",
    "model.freeze_model(True)\n",
    "\n",
    "# Print model information\n",
    "adapter_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {total_params:,}\")\n",
    "print(f\"Trainable adapter parameters: {adapter_params:,}\")\n",
    "print(f\"Percentage of parameters being fine-tuned: {adapter_params/total_params*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Adapter\n",
    "\n",
    "Now we'll train the adapter on our medical classification task. This is much more efficient than full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simplified version of Section 6 that focuses on the concepts\n",
    "# without running the actual adapter training code that's causing issues\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Explanation of adapter-based fine-tuning\n",
    "adapter_explanation = \"\"\"\n",
    "# Adapter-Based Fine-Tuning Explained\n",
    "\n",
    "Adapter-based fine-tuning is a parameter-efficient approach to adapt pre-trained language models to specific tasks.\n",
    "Instead of updating all parameters of a large model (which can be billions of parameters), adapters:\n",
    "\n",
    "1. **Keep most of the pre-trained model frozen** (weights unchanged)\n",
    "2. **Insert small, trainable modules** between layers of the frozen model\n",
    "3. **Update only these adapter modules** during training (typically <1% of the total parameters)\n",
    "4. **Preserve the general knowledge** of the pre-trained model while adapting to new tasks\n",
    "\n",
    "This approach drastically reduces memory requirements and training time while often achieving comparable performance to full fine-tuning.\n",
    "\n",
    "## Advantages of Adapters for Medical Applications\n",
    "\n",
    "- **Resource efficiency**: Can run on limited hardware like Google Colab\n",
    "- **Quick adaptation**: Train in minutes instead of hours/days\n",
    "- **Modularity**: Can swap adapters for different medical tasks\n",
    "- **Knowledge preservation**: Maintains the pre-trained medical knowledge\n",
    "- **Reduced risk of catastrophic forgetting**: Original capabilities remain intact\n",
    "\n",
    "## Types of Parameter-Efficient Fine-Tuning\n",
    "\n",
    "1. **Adapters**: Small bottleneck layers inserted between transformer layers\n",
    "2. **LoRA (Low-Rank Adaptation)**: Adds low-rank matrices to existing weights\n",
    "3. **Prefix Tuning**: Adds trainable prefix vectors to each transformer layer\n",
    "4. **P-Tuning**: Optimizes continuous prompts instead of discrete text prompts\n",
    "5. **BitFit**: Only fine-tunes bias terms in the model\n",
    "\"\"\"\n",
    "\n",
    "# Visualization of adapter architecture\n",
    "def plot_adapter_architecture():\n",
    "    \"\"\"Create a simple visualization of adapter architecture\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Define components\n",
    "    components = ['Input', 'Frozen Transformer Layer 1', 'Adapter 1', \n",
    "                  'Frozen Transformer Layer 2', 'Adapter 2',\n",
    "                  'Frozen Transformer Layer 3', 'Adapter 3', 'Output']\n",
    "    \n",
    "    # Define colors\n",
    "    colors = ['lightblue', 'lightgray', 'orange', \n",
    "              'lightgray', 'orange',\n",
    "              'lightgray', 'orange', 'lightblue']\n",
    "    \n",
    "    # Create the visualization\n",
    "    y_positions = np.arange(len(components))\n",
    "    plt.barh(y_positions, [0.7, 0.9, 0.3, 0.9, 0.3, 0.9, 0.3, 0.7], \n",
    "             align='center', color=colors, height=0.5)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, (comp, color) in enumerate(zip(components, colors)):\n",
    "        if color == 'orange':\n",
    "            plt.text(0.35, i, comp + \" (Trainable)\", fontweight='bold')\n",
    "        elif color == 'lightgray':\n",
    "            plt.text(0.35, i, comp + \" (Frozen)\", fontweight='normal')\n",
    "        else:\n",
    "            plt.text(0.35, i, comp, fontweight='normal')\n",
    "    \n",
    "    # Remove axes\n",
    "    plt.axis('off')\n",
    "    plt.title('Adapter-Based Fine-Tuning Architecture', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Example of adapter parameter efficiency\n",
    "def plot_parameter_efficiency():\n",
    "    \"\"\"Create a bar chart showing parameter efficiency of adapters\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    methods = ['Full Fine-tuning', 'Adapter', 'LoRA', 'Prefix Tuning', 'BitFit']\n",
    "    percentages = [100, 1.5, 0.8, 0.1, 0.01]\n",
    "    \n",
    "    plt.bar(methods, percentages, color=['darkred', 'green', 'green', 'green', 'green'])\n",
    "    plt.ylabel('% of Parameters Updated')\n",
    "    plt.title('Parameter Efficiency of Fine-Tuning Methods')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(percentages):\n",
    "        if v == 100:\n",
    "            plt.text(i, v*1.1, f\"{v}%\", ha='center', fontweight='bold')\n",
    "        else:\n",
    "            plt.text(i, v*1.1, f\"{v}%\", ha='center', color='darkgreen', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Simulated confusion matrix for medical classification\n",
    "def plot_simulated_results():\n",
    "    \"\"\"Create a simulated confusion matrix for medical classification\"\"\"\n",
    "    # Simulated confusion matrix for 3 medical categories\n",
    "    cm = np.array([\n",
    "        [5, 1, 0],  # Cardiology\n",
    "        [0, 4, 1],  # Neurology\n",
    "        [1, 0, 6]   # Endocrinology\n",
    "    ])\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Simulated Confusion Matrix for Medical Classification')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add labels\n",
    "    classes = [\"Cardiology\", \"Neurology\", \"Endocrinology\"]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    plt.figtext(0.5, 0.01, f\"Simulated Accuracy: {accuracy:.2f}\", ha=\"center\", fontsize=12)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Display the adapter explanation\n",
    "print(adapter_explanation)\n",
    "\n",
    "# Display the adapter architecture\n",
    "plot_adapter_architecture()\n",
    "plt.show()\n",
    "\n",
    "# Display parameter efficiency\n",
    "plot_parameter_efficiency()\n",
    "plt.show()\n",
    "\n",
    "# Display simulated results\n",
    "plot_simulated_results()\n",
    "plt.show()\n",
    "\n",
    "# Practical tips for using adapters in medical applications\n",
    "practical_tips = \"\"\"\n",
    "## Practical Tips for Using Adapters in Medical Applications\n",
    "\n",
    "1. **Start with domain-specific pre-trained models**: Models like PubMedBERT or BioBERT already have medical knowledge\n",
    "2. **Use small adapter sizes**: 64 or 128 bottleneck dimensions often work well\n",
    "3. **Experiment with different adapter configurations**: Pfeiffer, Houlsby, or parallel adapters\n",
    "4. **Consider task-specific adapters**: Create separate adapters for different medical tasks\n",
    "5. **Combine with medical prompting**: Adapters work well with good prompt design\n",
    "6. **Evaluate carefully**: Use domain-specific metrics for medical tasks\n",
    "7. **Ensemble multiple adapters**: Combine outputs from different adapters for better performance\n",
    "8. **Share adapters**: Adapters are small enough to easily share with colleagues\n",
    "\n",
    "For resource-constrained environments like Google Colab:\n",
    "- Use 8-bit quantization with adapters for even more efficiency\n",
    "- Keep batch sizes small (4-8)\n",
    "- Use gradient accumulation for effectively larger batches\n",
    "- Consider using LoRA instead of adapters for even more parameter efficiency\n",
    "\"\"\"\n",
    "\n",
    "print(practical_tips)\n",
    "\n",
    "# Alternatives to adapter-based fine-tuning\n",
    "alternatives = \"\"\"\n",
    "## Alternatives to Adapter-Based Fine-Tuning for Medical Applications\n",
    "\n",
    "If adapter-based fine-tuning is not working in your environment, consider these alternatives:\n",
    "\n",
    "1. **Prompt Engineering**: Design effective prompts that guide the model to produce medical content\n",
    "   - Example: \"You are a medical expert specializing in cardiology. Explain heart failure to a patient.\"\n",
    "\n",
    "2. **Few-Shot Learning**: Provide examples of the desired output format in your prompt\n",
    "   - Example: \"Q: What are symptoms of diabetes? A: Symptoms include increased thirst, frequent urination...\"\n",
    "\n",
    "3. **Retrieval-Augmented Generation (RAG)**: Combine LLMs with medical knowledge retrieval\n",
    "   - As demonstrated in Section 5 of this notebook\n",
    "\n",
    "4. **Chain-of-Thought Prompting**: Guide the model through medical reasoning steps\n",
    "   - Example: \"Think step by step about the differential diagnosis for chest pain.\"\n",
    "\n",
    "5. **Use Pre-Finetuned Medical Models**: Many models already fine-tuned for medical tasks are available\n",
    "   - Examples: Med-PaLM, BioGPT, Clinical-T5\n",
    "\n",
    "These approaches can often achieve good results without the need for any fine-tuning.\n",
    "\"\"\"\n",
    "\n",
    "print(alternatives)\n",
    "\n",
    "# Conclusion\n",
    "conclusion = \"\"\"\n",
    "## Conclusion\n",
    "\n",
    "Adapter-based fine-tuning offers an efficient way to adapt large language models to specific medical tasks\n",
    "without requiring extensive computational resources. While we encountered some technical challenges in this\n",
    "demonstration, the concepts and approaches remain valid and powerful.\n",
    "\n",
    "In real-world medical applications, these techniques allow practitioners to:\n",
    "1. Customize general-purpose models for specific medical specialties\n",
    "2. Create task-specific models for different clinical workflows\n",
    "3. Maintain up-to-date models as medical knowledge evolves\n",
    "4. Share and collaborate on model improvements efficiently\n",
    "\n",
    "As you continue your journey with LLMs in healthcare, remember that the choice between fine-tuning approaches,\n",
    "prompt engineering, and retrieval-augmented generation should be guided by your specific use case, available\n",
    "resources, and performance requirements.\n",
    "\"\"\"\n",
    "\n",
    "print(conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Parameter-Efficient Fine-Tuning Methods\n",
    "\n",
    "While we've focused on adapters, there are other parameter-efficient methods that are also effective for medical LLMs:\n",
    "\n",
    "#### LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA decomposes weight updates into low-rank matrices, significantly reducing the number of trainable parameters.\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the update matrices\n",
    "    lora_alpha=16,  # Parameter scaling\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # Which modules to apply LoRA to\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "```\n",
    "\n",
    "#### Prefix Tuning\n",
    "\n",
    "Prefix tuning prepends trainable \"prefix\" vectors to the input of each transformer layer.\n",
    "\n",
    "```python\n",
    "from peft import PrefixTuningConfig, get_peft_model\n",
    "\n",
    "# Configure Prefix Tuning\n",
    "prefix_config = PrefixTuningConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    num_virtual_tokens=20,  # Number of virtual tokens to add\n",
    "    encoder_hidden_size=768,\n",
    "    prefix_projection=True,\n",
    ")\n",
    "\n",
    "# Apply Prefix Tuning to the model\n",
    "model = get_peft_model(base_model, prefix_config)\n",
    "```\n",
    "\n",
    "### Maximizing Performance in Resource-Constrained Environments\n",
    "\n",
    "When working with medical LLMs in environments like Colab, consider these strategies:\n",
    "\n",
    "1. **Use smaller base models**: Start with smaller pre-trained models (e.g., BioMedBERT-base instead of large)\n",
    "2. **Apply gradient checkpointing**: Trades computation for memory\n",
    "3. **Use mixed precision training**: Train in fp16 instead of fp32\n",
    "4. **Optimize batch size**: Find the largest batch size that fits in memory\n",
    "5. **Progressive training**: Start with shorter sequences and gradually increase length\n",
    "\n",
    "Let's implement some of these optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Optimized training arguments (compatible with your version)\n",
    "optimized_training_args = TrainingArguments(\n",
    "    output_dir=\"./results_optimized\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,  # Smaller batch size\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    gradient_checkpointing=True,  # Use gradient checkpointing to save memory\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # Remove the unsupported parameters\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Optimized training configuration for resource-constrained environments:\")\n",
    "print(\"- Using mixed precision (fp16)\")\n",
    "print(\"- Enabled gradient checkpointing\")\n",
    "print(\"- Smaller batch size with gradient accumulation\")\n",
    "print(\"- Parameter-efficient fine-tuning (adapters/LoRA)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Experimenting with Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Now it's your turn to experiment with parameter-efficient fine-tuning. Here are some exercises to try:\n",
    "\n",
    "1. **Compare different adapter configurations**: Try different adapter architectures (Pfeiffer, Houlsby) and reduction factors to find the optimal trade-off between performance and efficiency.\n",
    "\n",
    "2. **Implement LoRA fine-tuning**: Use the PEFT library to implement LoRA fine-tuning for a medical task and compare its performance with adapters.\n",
    "\n",
    "3. **Fine-tune for a different medical task**: Adapt a pre-trained biomedical model for a different task, such as medical entity recognition or relation extraction.\n",
    "\n",
    "4. **Combine adapters with prompt tuning**: Experiment with combining adapter-based fine-tuning with prompt engineering techniques for even better performance.\n",
    "\n",
    "5. **Create a specialized medical classifier**: Build a classifier for a specific medical domain (e.g., radiology reports, pathology findings) using parameter-efficient fine-tuning.\n",
    "\n",
    "Remember that parameter-efficient methods allow you to adapt powerful pre-trained models to specific medical tasks without requiring extensive computational resources, making them ideal for educational and research settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "try:\n",
    "    del new_model\n",
    "    gc.collect()\n",
    "    if has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass  # Variable might not exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Applications\n",
    "\n",
    "### Introduction to Medical LLM Applications\n",
    "\n",
    "Large Language Models have numerous practical applications in healthcare settings. In this section, we'll explore how to implement several key applications:\n",
    "\n",
    "1. Patient education content generation\n",
    "2. Medical literature simplification\n",
    "3. Medical question answering systems\n",
    "4. Clinical documentation assistance\n",
    "\n",
    "For each application, we'll provide working examples that can run in both local and Colab environments, focusing on approaches that are efficient and effective.\n",
    "\n",
    "### Setup for Practical Applications\n",
    "\n",
    "First, let's set up our environment with the necessary tools and models for these applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BioGptTokenizer, BioGptForCausalLM\n",
    "import time\n",
    "\n",
    "# Check environment\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "has_gpu = torch.cuda.is_available()\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"GPU available: {has_gpu}\")\n",
    "\n",
    "# Clean up any previous models to free memory\n",
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Load a smaller biomedical model suitable for our applications\n",
    "model_name = \"microsoft/biogpt\"\n",
    "tokenizer = BioGptTokenizer.from_pretrained(model_name)\n",
    "model = BioGptForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if has_gpu else None,\n",
    "    torch_dtype=torch.float16 if has_gpu else torch.float32\n",
    ")\n",
    "\n",
    "print(f\"Loaded {model_name} model successfully!\")\n",
    "\n",
    "# Helper function for text generation\n",
    "def generate_text(prompt, max_length=300, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate text based on a prompt using our loaded model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if has_gpu:\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=inputs.input_ids.shape[1] + max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the generated portion (remove the prompt)\n",
    "    if prompt in generated_text:\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Patient Education Content Generation\n",
    "\n",
    "Creating clear, accessible patient education materials is crucial for improving health literacy and patient outcomes. LLMs can help generate content that explains medical concepts in patient-friendly language.\n",
    "\n",
    "Key considerations for patient education content:\n",
    "- Use simple, non-technical language\n",
    "- Explain medical terms when they must be used\n",
    "- Structure information logically\n",
    "- Address common questions and concerns\n",
    "- Tailor content to different health literacy levels\n",
    "\n",
    "Let's implement a patient education content generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patient_education(\n",
    "    condition,\n",
    "    reading_level=\"8th grade\",  # Target reading level\n",
    "    sections=[\"overview\", \"symptoms\", \"diagnosis\", \"treatment\", \"self_care\", \"when_to_call\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate patient education content about a medical condition.\n",
    "    \n",
    "    Args:\n",
    "        condition: The medical condition to explain\n",
    "        reading_level: Target reading level (e.g., \"6th grade\", \"8th grade\", \"high school\")\n",
    "        sections: List of sections to include\n",
    "        \n",
    "    Returns:\n",
    "        Generated patient education content\n",
    "    \"\"\"\n",
    "    # Create a section-by-section prompt\n",
    "    section_prompts = {\n",
    "        \"overview\": f\"Write a simple overview of {condition} in {reading_level} level language. Explain what it is and how it affects the body.\",\n",
    "        \"symptoms\": f\"Explain the common symptoms of {condition} in simple {reading_level} level language. List the warning signs a patient should watch for.\",\n",
    "        \"diagnosis\": f\"Describe how doctors diagnose {condition} in simple {reading_level} level language. Explain common tests or examinations.\",\n",
    "        \"treatment\": f\"Explain the common treatments for {condition} in simple {reading_level} level language. Include medications and procedures.\",\n",
    "        \"self_care\": f\"Provide self-care tips for managing {condition} in simple {reading_level} level language. Include lifestyle changes that might help.\",\n",
    "        \"when_to_call\": f\"Explain when a patient with {condition} should call their doctor or seek emergency care in simple {reading_level} level language.\"\n",
    "    }\n",
    "    \n",
    "    # Generate content for each requested section\n",
    "    content = f\"# {condition.title()}: Patient Education Guide\\n\\n\"\n",
    "    \n",
    "    for section in sections:\n",
    "        if section in section_prompts:\n",
    "            section_title = section.replace(\"_\", \" \").title()\n",
    "            content += f\"## {section_title}\\n\\n\"\n",
    "            \n",
    "            # Generate content for this section\n",
    "            section_content = generate_text(section_prompts[section], max_length=200, temperature=0.6)\n",
    "            content += section_content + \"\\n\\n\"\n",
    "    \n",
    "    # Add a disclaimer\n",
    "    content += \"---\\n\"\n",
    "    content += \"*Note: This information is for educational purposes only and is not medical advice. Always consult with a healthcare provider for medical concerns.*\"\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Test the patient education content generator\n",
    "conditions_to_test = [\"type 2 diabetes\", \"hypertension\", \"asthma\"]\n",
    "\n",
    "for condition in conditions_to_test:\n",
    "    print(f\"\\n{'='*50}\\nPatient Education Content for {condition.title()}\\n{'='*50}\\n\")\n",
    "    \n",
    "    # Generate content with fewer sections for brevity\n",
    "    education_content = generate_patient_education(\n",
    "        condition,\n",
    "        reading_level=\"8th grade\",\n",
    "        sections=[\"overview\", \"symptoms\", \"self_care\"]  # Limited sections for demo\n",
    "    )\n",
    "    \n",
    "    print(education_content)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Medical Literature Simplification\n",
    "\n",
    "Medical literature is often written in technical language that can be difficult for non-specialists to understand. LLMs can help simplify complex medical texts while preserving the key information.\n",
    "\n",
    "Key aspects of effective medical literature simplification:\n",
    "- Identify and explain technical terms\n",
    "- Maintain accuracy while simplifying\n",
    "- Preserve key information and context\n",
    "- Structure information clearly\n",
    "- Adjust the level of simplification based on the target audience\n",
    "\n",
    "Let's implement a medical literature simplification tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_medical_text(\n",
    "    text,\n",
    "    target_audience=\"general public\",  # Options: \"general public\", \"patients\", \"medical students\", \"healthcare professionals\"\n",
    "    preserve_citations=False,\n",
    "    explain_terms=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplify complex medical text for different audiences.\n",
    "    \n",
    "    Args:\n",
    "        text: The medical text to simplify\n",
    "        target_audience: The intended audience for the simplified text\n",
    "        preserve_citations: Whether to preserve citation markers\n",
    "        explain_terms: Whether to explain medical terms\n",
    "        \n",
    "    Returns:\n",
    "        Simplified version of the medical text\n",
    "    \"\"\"\n",
    "    # Prepare the prompt based on parameters\n",
    "    audience_description = {\n",
    "        \"general public\": \"with no medical background\",\n",
    "        \"patients\": \"who are patients with the condition but have no medical training\",\n",
    "        \"medical students\": \"who are medical students with basic medical knowledge\",\n",
    "        \"healthcare professionals\": \"who are healthcare professionals but not specialists in this field\"\n",
    "    }\n",
    "    \n",
    "    audience_desc = audience_description.get(target_audience, \"with no medical background\")\n",
    "    \n",
    "    # Handle citations if needed\n",
    "    if not preserve_citations:\n",
    "        # Remove citation patterns like [1], [2-4], etc.\n",
    "        text = re.sub(r'\\[\\d+(-\\d+)?\\]', '', text)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"Simplify the following medical text for readers {audience_desc}.\n",
    "{' Explain medical terms when they first appear.' if explain_terms else ''}\n",
    "{' Preserve citation markers like [1] or [2-4].' if preserve_citations else ''}\n",
    "Make the text more accessible while maintaining accuracy.\n",
    "\n",
    "Original text:\n",
    "{text}\n",
    "\n",
    "Simplified version:\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate simplified text\n",
    "    simplified = generate_text(prompt, max_length=min(len(text), 300), temperature=0.4)\n",
    "    \n",
    "    return simplified\n",
    "\n",
    "# Test with some complex medical texts\n",
    "medical_texts = [\n",
    "    \"\"\"Diabetes mellitus type 2 is characterized by hyperglycemia, insulin resistance, and relative impairment in insulin secretion. It is a metabolic disorder that is characterized by high blood glucose due to insulin resistance and insulin deficiency. Long-term complications from hyperglycemia include coronary artery disease, diabetic neuropathy, diabetic retinopathy, and diabetic nephropathy [1-3].\"\"\",\n",
    "    \n",
    "    \"\"\"Myocardial infarction occurs when blood flow decreases or stops to a part of the heart, causing damage to the heart muscle. The most common symptom is chest pain or discomfort which may travel into the shoulder, arm, back, neck or jaw [4]. Often it occurs in the center or left side of the chest and lasts for more than a few minutes. The discomfort may occasionally feel like heartburn.\"\"\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(medical_texts, 1):\n",
    "    print(f\"\\n{'='*50}\\nExample {i}: Medical Literature Simplification\\n{'='*50}\\n\")\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nSimplified for general public:\")\n",
    "    simplified_public = simplify_medical_text(text, target_audience=\"general public\")\n",
    "    print(simplified_public)\n",
    "    print(\"\\nSimplified for medical students:\")\n",
    "    simplified_students = simplify_medical_text(text, target_audience=\"medical students\", preserve_citations=True)\n",
    "    print(simplified_students)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Question Answering Systems\n",
    "\n",
    "Medical question answering (QA) systems help provide accurate answers to health-related questions. These systems can be valuable for patient education, clinical decision support, and medical education.\n",
    "\n",
    "Key components of an effective medical QA system:\n",
    "- Accurate understanding of medical questions\n",
    "- Access to reliable medical knowledge\n",
    "- Ability to provide evidence-based answers\n",
    "- Clear communication of uncertainty\n",
    "- Citations or references when appropriate\n",
    "\n",
    "Let's implement a simple medical QA system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a small medical knowledge base for our QA system\n",
    "medical_kb = {\n",
    "    \"diabetes\": \"\"\"\n",
    "    Diabetes mellitus is a group of metabolic disorders characterized by high blood sugar levels over a prolonged period. There are several types of diabetes:\n",
    "    \n",
    "    Type 1 diabetes results from the pancreas's failure to produce enough insulin due to loss of beta cells.\n",
    "    Type 2 diabetes begins with insulin resistance, a condition in which cells fail to respond to insulin properly.\n",
    "    Gestational diabetes occurs when pregnant women without a previous history of diabetes develop high blood sugar levels.\n",
    "    \n",
    "    Symptoms of diabetes include frequent urination, increased thirst, increased hunger, and weight loss. Complications can include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and eye damage.\n",
    "    \n",
    "    Treatment depends on the type of diabetes. Type 1 diabetes requires insulin injections. Type 2 diabetes may be treated with medications with or without insulin. All types of diabetes benefit from maintaining a healthy diet, regular physical exercise, and maintaining a normal body weight.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"hypertension\": \"\"\"\n",
    "    Hypertension, also known as high blood pressure, is a long-term medical condition in which the blood pressure in the arteries is persistently elevated. High blood pressure typically does not cause symptoms, but long-term high blood pressure is a major risk factor for stroke, coronary artery disease, heart failure, atrial fibrillation, peripheral arterial disease, vision loss, chronic kidney disease, and dementia.\n",
    "    \n",
    "    Blood pressure is expressed by two measurements, the systolic and diastolic pressures, which are the maximum and minimum pressures, respectively. For most adults, normal blood pressure at rest is within the range of 100â130 millimeters mercury (mmHg) systolic and 60â80 mmHg diastolic. For most adults, high blood pressure is present if the resting blood pressure is persistently at or above 130/80 or 140/90 mmHg.\n",
    "    \n",
    "    Lifestyle changes and medications can lower blood pressure and decrease the risk of health complications. Lifestyle changes include weight loss, physical exercise, decreased salt intake, reducing alcohol intake, and a healthy diet. If lifestyle changes are not sufficient, then blood pressure medications are used.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"asthma\": \"\"\"\n",
    "    Asthma is a long-term inflammatory disease of the airways of the lungs. It is characterized by variable and recurring symptoms, reversible airflow obstruction, and easily triggered bronchospasms. Symptoms include episodes of wheezing, coughing, chest tightness, and shortness of breath. These may occur a few times a day or a few times per week. Depending on the person, asthma symptoms may become worse at night or with exercise.\n",
    "    \n",
    "    Asthma is thought to be caused by a combination of genetic and environmental factors. Environmental factors include exposure to air pollution and allergens. Other potential triggers include medications such as aspirin and beta blockers. Diagnosis is usually based on the pattern of symptoms, response to therapy over time, and spirometry.\n",
    "    \n",
    "    There is no cure for asthma. Symptoms can be prevented by avoiding triggers, such as allergens and irritants, and by the use of inhaled corticosteroids. Long-acting beta agonists (LABA) or antileukotriene agents may be used in addition to inhaled corticosteroids if asthma symptoms remain uncontrolled. Treatment of rapidly worsening symptoms is usually with an inhaled short-acting beta-2 agonist such as salbutamol and corticosteroids taken by mouth.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"heart attack\": \"\"\"\n",
    "    A heart attack, also known as a myocardial infarction, occurs when blood flow decreases or stops to a part of the heart, causing damage to the heart muscle. The most common symptom is chest pain or discomfort which may travel into the shoulder, arm, back, neck, or jaw. Often it occurs in the center or left side of the chest and lasts for more than a few minutes. The discomfort may occasionally feel like heartburn. Other symptoms may include shortness of breath, nausea, feeling faint, a cold sweat, or feeling tired. About 30% of people have atypical symptoms.\n",
    "    \n",
    "    Most heart attacks occur due to coronary artery disease. Risk factors include high blood pressure, smoking, diabetes, lack of exercise, obesity, high blood cholesterol, poor diet, and excessive alcohol intake. The complete blockage of a coronary artery caused by a rupture of an atherosclerotic plaque is usually the underlying mechanism of a heart attack.\n",
    "    \n",
    "    Heart attacks are diagnosed based on symptoms, electrocardiogram (ECG) findings, and blood tests for heart muscle damage. Treatment for a suspected heart attack may include aspirin, nitroglycerin, and morphine. A coronary angiogram may be performed to visualize the coronary arteries. If the coronary artery is blocked, treatments may include medications to dissolve the clot or angioplasty where the artery is opened with a balloon and often a stent is placed.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "def medical_qa_system(question, knowledge_base=medical_kb):\n",
    "    \"\"\"\n",
    "    Answer medical questions using a knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        question: The medical question to answer\n",
    "        knowledge_base: Dictionary of medical topics and information\n",
    "        \n",
    "    Returns:\n",
    "        Answer to the question and the source of information\n",
    "    \"\"\"\n",
    "    # Determine the most relevant topic in our knowledge base\n",
    "    max_relevance = 0\n",
    "    most_relevant_topic = None\n",
    "    \n",
    "    # Simple keyword matching to find relevant topic\n",
    "    # In a real system, you would use embeddings and semantic search\n",
    "    for topic, info in knowledge_base.items():\n",
    "        if topic.lower() in question.lower():\n",
    "            relevance = 1.0  # Direct match\n",
    "            if relevance > max_relevance:\n",
    "                max_relevance = relevance\n",
    "                most_relevant_topic = topic\n",
    "    \n",
    "    # If no direct match, try to find partial matches\n",
    "    if most_relevant_topic is None:\n",
    "        for topic, info in knowledge_base.items():\n",
    "            # Check if any words in the topic are in the question\n",
    "            topic_words = topic.lower().split()\n",
    "            question_lower = question.lower()\n",
    "            \n",
    "            for word in topic_words:\n",
    "                if len(word) > 3 and word in question_lower:  # Only consider words longer than 3 letters\n",
    "                    relevance = 0.5  # Partial match\n",
    "                    if relevance > max_relevance:\n",
    "                        max_relevance = relevance\n",
    "                        most_relevant_topic = topic\n",
    "    \n",
    "    # If we found a relevant topic, use it to answer the question\n",
    "    if most_relevant_topic and max_relevance > 0:\n",
    "        context = knowledge_base[most_relevant_topic]\n",
    "        \n",
    "        # Create a prompt for the model to answer the question\n",
    "        prompt = f\"\"\"Based on the following medical information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Please answer this medical question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate the answer\n",
    "        answer = generate_text(prompt, max_length=200, temperature=0.4)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"source\": most_relevant_topic,\n",
    "            \"confidence\": max_relevance\n",
    "        }\n",
    "    else:\n",
    "        # If no relevant topic found, use general medical knowledge\n",
    "        prompt = f\"Please answer this medical question based on general medical knowledge: {question}\\n\\nAnswer:\"\n",
    "        answer = generate_text(prompt, max_length=150, temperature=0.4)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"source\": \"general medical knowledge\",\n",
    "            \"confidence\": 0.3  # Low confidence since not from our curated knowledge base\n",
    "        }\n",
    "\n",
    "# Test the medical QA system\n",
    "test_questions = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is high blood pressure diagnosed?\",\n",
    "    \"Can asthma be cured?\",\n",
    "    \"What should I do if someone is having a heart attack?\",\n",
    "    \"What is the difference between Type 1 and Type 2 diabetes?\"\n",
    "]\n",
    "\n",
    "print(\"Medical Question Answering System:\\n\")\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    result = medical_qa_system(question)\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Source: {result['source']} (Confidence: {result['confidence']})\")\n",
    "    print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clinical Documentation Assistance\n",
    "\n",
    "Clinical documentation is essential for patient care, but it can be time-consuming for healthcare providers. LLMs can assist with generating and summarizing clinical documentation, helping to improve efficiency while maintaining accuracy.\n",
    "\n",
    "Key aspects of clinical documentation assistance:\n",
    "- Maintaining medical accuracy\n",
    "- Following standard documentation formats\n",
    "- Preserving patient-specific details\n",
    "- Supporting different documentation types (progress notes, discharge summaries, etc.)\n",
    "- Ensuring compliance with documentation standards\n",
    "\n",
    "Let's implement a clinical documentation assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinical_documentation_assistant(\n",
    "    patient_info,\n",
    "    visit_type,\n",
    "    clinical_findings,\n",
    "    doc_type=\"progress_note\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate clinical documentation based on patient information and clinical findings.\n",
    "    \n",
    "    Args:\n",
    "        patient_info: Dictionary with patient demographic information\n",
    "        visit_type: Type of clinical visit (e.g., \"initial\", \"follow-up\", \"urgent\")\n",
    "        clinical_findings: Dictionary with clinical findings and assessment\n",
    "        doc_type: Type of document to generate\n",
    "        \n",
    "    Returns:\n",
    "        Generated clinical documentation\n",
    "    \"\"\"\n",
    "    # Format patient information\n",
    "    patient_str = f\"Patient: {patient_info.get('name', 'Patient')}, \"\n",
    "    patient_str += f\"{patient_info.get('age', '')} y.o. {patient_info.get('gender', '')}, \"\n",
    "    patient_str += f\"MRN: {patient_info.get('mrn', 'Unknown')}\"\n",
    "    \n",
    "    # Format clinical findings\n",
    "    findings_str = \"\"\n",
    "    for category, details in clinical_findings.items():\n",
    "        findings_str += f\"{category.upper()}: {details}\\n\"\n",
    "    \n",
    "    # Create appropriate template based on document type\n",
    "    if doc_type == \"progress_note\":\n",
    "        template = f\"\"\"Generate a clinical progress note for the following patient visit:\n",
    "\n",
    "{patient_str}\n",
    "Visit Type: {visit_type}\n",
    "\n",
    "Clinical Information:\n",
    "{findings_str}\n",
    "\n",
    "Please format the note with standard sections (Subjective, Objective, Assessment, Plan).\n",
    "Use professional medical language appropriate for clinical documentation.\n",
    "Be concise but thorough.\n",
    "\n",
    "PROGRESS NOTE:\"\"\"\n",
    "    \n",
    "    elif doc_type == \"discharge_summary\":\n",
    "        template = f\"\"\"Generate a hospital discharge summary for the following patient:\n",
    "\n",
    "{patient_str}\n",
    "Admission Type: {visit_type}\n",
    "\n",
    "Clinical Information:\n",
    "{findings_str}\n",
    "\n",
    "Please include standard discharge summary sections (Hospital Course, Discharge Diagnosis, Discharge Medications, Follow-up Instructions).\n",
    "Use professional medical language appropriate for clinical documentation.\n",
    "\n",
    "DISCHARGE SUMMARY:\"\"\"\n",
    "    \n",
    "    elif doc_type == \"referral_letter\":\n",
    "        template = f\"\"\"Generate a referral letter for the following patient:\n",
    "\n",
    "{patient_str}\n",
    "Referral Type: {visit_type}\n",
    "\n",
    "Clinical Information:\n",
    "{findings_str}\n",
    "\n",
    "Please format as a professional referral letter to a specialist.\n",
    "Include reason for referral, relevant history, and specific questions for the consultant.\n",
    "Use professional medical language appropriate for clinical documentation.\n",
    "\n",
    "REFERRAL LETTER:\"\"\"\n",
    "    \n",
    "    else:  # Default to a general clinical note\n",
    "        template = f\"\"\"Generate a clinical note for the following patient:\n",
    "\n",
    "{patient_str}\n",
    "Visit Type: {visit_type}\n",
    "\n",
    "Clinical Information:\n",
    "{findings_str}\n",
    "\n",
    "Please use professional medical language appropriate for clinical documentation.\n",
    "\n",
    "CLINICAL NOTE:\"\"\"\n",
    "    \n",
    "    # Generate the documentation\n",
    "    documentation = generate_text(template, max_length=400, temperature=0.4)\n",
    "    \n",
    "    return documentation\n",
    "\n",
    "# Test the clinical documentation assistant with sample patient cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"patient_info\": {\n",
    "            \"name\": \"John Doe\",\n",
    "            \"age\": 45,\n",
    "            \"gender\": \"Male\",\n",
    "            \"mrn\": \"MRN12345\"\n",
    "        },\n",
    "        \"visit_type\": \"follow-up\",\n",
    "        \"clinical_findings\": {\n",
    "            \"chief_complaint\": \"Follow-up for type 2 diabetes, reports improved blood glucose control\",\n",
    "            \"vitals\": \"BP 138/82, HR 72, RR 16, Temp 98.6Â°F, SpO2 98%, Weight 195 lbs\",\n",
    "            \"labs\": \"HbA1c 7.2% (improved from 8.5% 3 months ago), Fasting glucose 130 mg/dL\",\n",
    "            \"assessment\": \"Type 2 Diabetes Mellitus, improving with current management\",\n",
    "            \"plan\": \"Continue metformin 1000mg BID, lifestyle modifications, follow-up in 3 months\"\n",
    "        },\n",
    "        \"doc_type\": \"progress_note\"\n",
    "    },\n",
    "    {\n",
    "        \"patient_info\": {\n",
    "            \"name\": \"Jane Smith\",\n",
    "            \"age\": 67,\n",
    "            \"gender\": \"Female\",\n",
    "            \"mrn\": \"MRN67890\"\n",
    "        },\n",
    "        \"visit_type\": \"inpatient admission\",\n",
    "        \"clinical_findings\": {\n",
    "            \"admission_diagnosis\": \"Community-acquired pneumonia, right lower lobe\",\n",
    "            \"hospital_course\": \"Admitted for IV antibiotics, oxygen support. Improved after 5 days of treatment.\",\n",
    "            \"discharge_vitals\": \"BP 122/76, HR 68, RR 18, Temp 98.2Â°F, SpO2 96% on room air\",\n",
    "            \"discharge_medications\": \"Amoxicillin-clavulanate 875-125mg BID for 5 more days\",\n",
    "            \"follow_up\": \"Primary care in 1 week, pulmonology if symptoms worsen\"\n",
    "        },\n",
    "        \"doc_type\": \"discharge_summary\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nClinical Documentation Assistant:\\n\")\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    print(f\"Case {i}: {case['doc_type'].replace('_', ' ').title()} for {case['patient_info']['name']}\")\n",
    "    documentation = clinical_documentation_assistant(\n",
    "        case['patient_info'],\n",
    "        case['visit_type'],\n",
    "        case['clinical_findings'],\n",
    "        case['doc_type']\n",
    "    )\n",
    "    print(\"\\nGenerated Documentation:\")\n",
    "    print(documentation)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Creating Your Own Medical LLM Application\n",
    "\n",
    "Now it's your turn to experiment with creating medical LLM applications. Here are some exercises to try:\n",
    "\n",
    "1. **Enhanced Patient Education**: Extend the patient education generator to include interactive elements like quizzes or personalized content based on patient characteristics.\n",
    "\n",
    "2. **Specialized Medical Simplifier**: Create a simplifier for a specific type of medical content, such as radiology reports or pathology findings.\n",
    "\n",
    "3. **Domain-Specific QA System**: Build a QA system focused on a specific medical specialty or condition, with an expanded knowledge base.\n",
    "\n",
    "4. **Clinical Decision Support Tool**: Develop a tool that suggests potential diagnoses based on patient symptoms and history.\n",
    "\n",
    "5. **Medical Summarization Tool**: Create a tool that summarizes lengthy medical records or research papers into concise, actionable information.\n",
    "\n",
    "Remember that while these applications can be valuable tools, they should always be used with appropriate oversight from healthcare professionals, especially for applications that might influence clinical decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Resources cleaned up successfully!\")\n",
    "except:\n",
    "    print(\"No resources to clean up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating LLM Outputs\n",
    "\n",
    "### Introduction to Evaluating Medical LLM Outputs\n",
    "\n",
    "Evaluating the outputs of LLMs in medical contexts is crucial for ensuring accuracy, reliability, and safety. Unlike general-purpose text generation, medical applications have higher stakes and require specialized evaluation approaches.\n",
    "\n",
    "In this section, we'll explore:\n",
    "1. Metrics for assessing medical LLM accuracy\n",
    "2. Methods for comparing LLM outputs to gold-standard medical information\n",
    "3. Techniques for identifying and addressing hallucinations\n",
    "4. Approaches for evaluating medical reasoning in LLM outputs\n",
    "\n",
    "### Setting Up Evaluation Tools\n",
    "\n",
    "First, let's set up the necessary tools for evaluating medical LLM outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple evaluation functions for medical LLM outputs without any NLTK dependencies.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "# Simple tokenizer function\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    A very simple tokenizer that splits text on whitespace and punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to tokenize\n",
    "        \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    # Replace punctuation with spaces, then split on whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "    return [token for token in text.split() if token]\n",
    "\n",
    "# Define a simple ROUGE score calculator\n",
    "def calculate_rouge_score(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate a simplified version of ROUGE-1 F1 score.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text (gold standard)\n",
    "        hypothesis: Hypothesis text (model output)\n",
    "        \n",
    "    Returns:\n",
    "        ROUGE-1 F1 score\n",
    "    \"\"\"\n",
    "    # Tokenize the texts using our simple tokenizer\n",
    "    ref_tokens = simple_tokenize(reference.lower())\n",
    "    hyp_tokens = simple_tokenize(hypothesis.lower())\n",
    "    \n",
    "    # Count matching tokens\n",
    "    matches = sum(1 for token in hyp_tokens if token in ref_tokens)\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    precision = matches / len(hyp_tokens) if len(hyp_tokens) > 0 else 0\n",
    "    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Define a function to calculate word overlap\n",
    "def calculate_word_overlap(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate the word overlap between two texts.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text\n",
    "        text2: Second text\n",
    "        \n",
    "    Returns:\n",
    "        Percentage of words in text1 that appear in text2\n",
    "    \"\"\"\n",
    "    # Tokenize and get unique words\n",
    "    words1 = set(simple_tokenize(text1.lower()))\n",
    "    words2 = set(simple_tokenize(text2.lower()))\n",
    "    \n",
    "    # Define common stopwords\n",
    "    stopwords = {'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what',\n",
    "                'which', 'this', 'that', 'these', 'those', 'then', 'just', 'so', 'than',\n",
    "                'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while', 'during',\n",
    "                'to', 'from', 'in', 'on', 'by', 'with', 'at', 'into'}\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words1 = words1 - stopwords\n",
    "    words2 = words2 - stopwords\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = len(words1.intersection(words2))\n",
    "    percentage = overlap / len(words1) if len(words1) > 0 else 0\n",
    "    \n",
    "    return percentage\n",
    "\n",
    "# Simple sentence tokenizer\n",
    "def simple_sent_tokenize(text):\n",
    "    \"\"\"\n",
    "    Split text into sentences using simple rules.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to split into sentences\n",
    "        \n",
    "    Returns:\n",
    "        List of sentences\n",
    "    \"\"\"\n",
    "    # Split on common sentence terminators followed by space and capital letter\n",
    "    text = re.sub(r'([.!?])\\s+([A-Z])', r'\\1\\n\\2', text)\n",
    "    return [s.strip() for s in text.split('\\n') if s.strip()]\n",
    "\n",
    "# Evaluate medical output\n",
    "def evaluate_medical_output(model_output, reference_text, medical_keywords=None):\n",
    "    \"\"\"\n",
    "    Evaluate a medical LLM output against a reference text.\n",
    "    \n",
    "    Args:\n",
    "        model_output: Text generated by the LLM\n",
    "        reference_text: Gold standard reference text\n",
    "        medical_keywords: List of important medical terms that should be included\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate ROUGE score (text similarity)\n",
    "    rouge_scores = calculate_rouge_score(reference_text, model_output)\n",
    "    results['rouge_f1'] = rouge_scores['f1']\n",
    "    \n",
    "    # Calculate word overlap (factual alignment)\n",
    "    results['word_overlap'] = calculate_word_overlap(reference_text, model_output)\n",
    "    \n",
    "    # Check for presence of medical keywords (completeness)\n",
    "    if medical_keywords:\n",
    "        keyword_presence = []\n",
    "        for keyword in medical_keywords:\n",
    "            # Check if keyword or close variants are in the output\n",
    "            pattern = r'\\b' + re.escape(keyword) + r'(s|es|ing|ed)?\\b'\n",
    "            present = bool(re.search(pattern, model_output, re.IGNORECASE))\n",
    "            keyword_presence.append(present)\n",
    "        \n",
    "        results['keyword_coverage'] = sum(keyword_presence) / len(medical_keywords)\n",
    "    \n",
    "    # Check for medical disclaimers (safety)\n",
    "    disclaimer_patterns = [\n",
    "        r'consult.*doctor',\n",
    "        r'consult.*healthcare provider',\n",
    "        r'medical advice',\n",
    "        r'not.*substitute',\n",
    "        r'seek.*professional'\n",
    "    ]\n",
    "    \n",
    "    has_disclaimer = any(re.search(pattern, model_output, re.IGNORECASE) \n",
    "                         for pattern in disclaimer_patterns)\n",
    "    results['has_safety_disclaimer'] = has_disclaimer\n",
    "    \n",
    "    # Check for uncertainty markers (uncertainty communication)\n",
    "    uncertainty_patterns = [\n",
    "        r'may', r'might', r'could', r'possibly', r'potentially',\n",
    "        r'uncertain', r'unclear', r'limited evidence', r'suggests'\n",
    "    ]\n",
    "    \n",
    "    uncertainty_count = sum(len(re.findall(pattern, model_output, re.IGNORECASE)) \n",
    "                           for pattern in uncertainty_patterns)\n",
    "    results['uncertainty_markers'] = uncertainty_count\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Extract medical claims\n",
    "def extract_medical_claims(text):\n",
    "    \"\"\"\n",
    "    Extract medical claims from text by splitting into sentences.\n",
    "    This is a simplified approach - in practice, more sophisticated NLP would be used.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract claims from\n",
    "        \n",
    "    Returns:\n",
    "        List of claims (sentences)\n",
    "    \"\"\"\n",
    "    # Use our simple sentence tokenizer\n",
    "    sentences = simple_sent_tokenize(text)\n",
    "    \n",
    "    # Filter out very short sentences and non-informative ones\n",
    "    claims = [s.strip() for s in sentences if len(s.split()) > 3]\n",
    "    \n",
    "    return claims\n",
    "\n",
    "# Evaluate medical claims\n",
    "def evaluate_medical_claims(claims, reference_text):\n",
    "    \"\"\"\n",
    "    Evaluate a list of medical claims against a reference text.\n",
    "    \n",
    "    Args:\n",
    "        claims: List of medical claims to evaluate\n",
    "        reference_text: Gold standard reference text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'total_claims': len(claims),\n",
    "        'supported_claims': 0,\n",
    "        'unsupported_claims': 0,\n",
    "        'claim_evaluations': []\n",
    "    }\n",
    "    \n",
    "    for claim in claims:\n",
    "        # Calculate similarity between claim and reference\n",
    "        similarity = calculate_word_overlap(claim, reference_text)\n",
    "        \n",
    "        # Determine if claim is supported (using a simple threshold)\n",
    "        is_supported = similarity > 0.3  # This threshold would be tuned in practice\n",
    "        \n",
    "        # Store evaluation\n",
    "        claim_eval = {\n",
    "            'claim': claim,\n",
    "            'similarity': similarity,\n",
    "            'supported': is_supported\n",
    "        }\n",
    "        \n",
    "        results['claim_evaluations'].append(claim_eval)\n",
    "        \n",
    "        if is_supported:\n",
    "            results['supported_claims'] += 1\n",
    "        else:\n",
    "            results['unsupported_claims'] += 1\n",
    "    \n",
    "    # Calculate overall support ratio\n",
    "    results['support_ratio'] = results['supported_claims'] / results['total_claims'] if results['total_claims'] > 0 else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Detect potential hallucinations\n",
    "def detect_potential_hallucinations(text, reference_texts, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detect potential hallucinations in a medical text by comparing sentences\n",
    "    to a collection of reference texts.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to check for hallucinations\n",
    "        reference_texts: List of trusted reference texts\n",
    "        threshold: Similarity threshold below which a sentence is flagged\n",
    "        \n",
    "    Returns:\n",
    "        List of potential hallucinations with their confidence scores\n",
    "    \"\"\"\n",
    "    # Extract sentences from the text\n",
    "    sentences = simple_sent_tokenize(text)\n",
    "    \n",
    "    # Filter out very short sentences\n",
    "    sentences = [s for s in sentences if len(s.split()) > 5]\n",
    "    \n",
    "    potential_hallucinations = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Check similarity against all reference texts\n",
    "        max_similarity = 0\n",
    "        for reference in reference_texts:\n",
    "            similarity = calculate_word_overlap(sentence, reference)\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        # If similarity is below threshold, flag as potential hallucination\n",
    "        if max_similarity < threshold:\n",
    "            potential_hallucinations.append({\n",
    "                'text': sentence,\n",
    "                'confidence': 1.0 - max_similarity  # Higher confidence = more likely to be a hallucination\n",
    "            })\n",
    "    \n",
    "    # Sort by confidence (most likely hallucinations first)\n",
    "    potential_hallucinations.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    return potential_hallucinations\n",
    "\n",
    "# Evaluate medical reasoning\n",
    "def evaluate_medical_reasoning(text, reasoning_keywords=None):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of medical reasoning in a text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to evaluate\n",
    "        reasoning_keywords: Dictionary mapping reasoning categories to keywords\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with reasoning evaluation metrics\n",
    "    \"\"\"\n",
    "    if reasoning_keywords is None:\n",
    "        # Default reasoning keywords by category\n",
    "        reasoning_keywords = {\n",
    "            'causal': ['because', 'due to', 'caused by', 'leads to', 'results in'],\n",
    "            'evidence': ['study', 'evidence', 'research', 'shows', 'demonstrates', 'according to'],\n",
    "            'comparison': ['compared to', 'versus', 'higher than', 'lower than', 'more', 'less'],\n",
    "            'qualification': ['however', 'although', 'while', 'may', 'might', 'can', 'possibly'],\n",
    "            'recommendation': ['recommend', 'should', 'advised', 'indicated', 'treatment', 'manage']\n",
    "        }\n",
    "    \n",
    "    results = {\n",
    "        'reasoning_types': {},\n",
    "        'reasoning_score': 0,\n",
    "        'reasoning_diversity': 0\n",
    "    }\n",
    "    \n",
    "    # Count occurrences of each reasoning type\n",
    "    for category, keywords in reasoning_keywords.items():\n",
    "        count = 0\n",
    "        for keyword in keywords:\n",
    "            count += len(re.findall(r'\\b' + re.escape(keyword) + r'\\b', text, re.IGNORECASE))\n",
    "        \n",
    "        results['reasoning_types'][category] = count\n",
    "    \n",
    "    # Calculate overall reasoning score (simple sum of all types)\n",
    "    total_markers = sum(results['reasoning_types'].values())\n",
    "    results['reasoning_score'] = total_markers\n",
    "    \n",
    "    # Calculate reasoning diversity (how many different types are used)\n",
    "    types_used = sum(1 for count in results['reasoning_types'].values() if count > 0)\n",
    "    results['reasoning_diversity'] = types_used / len(reasoning_keywords) if reasoning_keywords else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Simple Medical LLM Evaluation\")\n",
    "    \n",
    "    example_reference = \"\"\"\n",
    "    Diabetes mellitus type 2 is characterized by high blood sugar, insulin resistance, and relative lack of insulin. Common symptoms include increased thirst, frequent urination, and unexplained weight loss. Long-term complications include heart disease, strokes, diabetic retinopathy, kidney failure, and poor blood flow in the limbs. The development of type 2 diabetes is primarily due to lifestyle factors and genetics.\n",
    "    \"\"\"\n",
    "\n",
    "    example_output = \"\"\"\n",
    "    Type 2 diabetes is a condition where your body has high blood sugar levels. This happens because your body doesn't use insulin properly (called insulin resistance) or doesn't make enough insulin. Common signs include feeling very thirsty, needing to urinate often, and losing weight without trying. If not managed well, type 2 diabetes can lead to serious problems like heart disease, stroke, eye damage (retinopathy), kidney failure, and poor circulation in your legs and feet. While genetics play a role, lifestyle factors like diet and exercise are major contributors to developing this condition. Always consult with a healthcare provider for medical advice about diabetes management.\n",
    "    \"\"\"\n",
    "\n",
    "    medical_keywords = ['diabetes', 'insulin', 'blood sugar', 'thirst', 'urination', 'weight loss', \n",
    "                      'heart disease', 'stroke', 'retinopathy', 'kidney', 'genetics']\n",
    "\n",
    "    # Evaluate the output\n",
    "    evaluation_results = evaluate_medical_output(example_output, example_reference, medical_keywords)\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for metric, value in evaluation_results.items():\n",
    "        print(f\"{metric}: {value:.4f}\" if isinstance(value, float) else f\"{metric}: {value}\")\n",
    "    \n",
    "    # Extract and evaluate claims\n",
    "    claims = extract_medical_claims(example_output)\n",
    "    print(f\"\\nExtracted {len(claims)} claims from the output\")\n",
    "    \n",
    "    claim_evaluation = evaluate_medical_claims(claims, example_reference)\n",
    "    print(f\"Support ratio: {claim_evaluation['support_ratio']:.2f}\")\n",
    "    print(f\"Supported claims: {claim_evaluation['supported_claims']}/{claim_evaluation['total_claims']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple visualization functions for medical LLM evaluation results.\n",
    "This module works with the simple_evaluation.py module and doesn't require NLTK.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from simple_evaluation import (\n",
    "    evaluate_medical_output,\n",
    "    extract_medical_claims,\n",
    "    evaluate_medical_claims,\n",
    "    detect_potential_hallucinations,\n",
    "    evaluate_medical_reasoning\n",
    ")\n",
    "\n",
    "def visualize_evaluation_results(evaluations, model_names=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for model evaluation results from simple_evaluation.\n",
    "    \n",
    "    Args:\n",
    "        evaluations: List of evaluation result dictionaries from simple_evaluation\n",
    "        model_names: Optional list of model names for the legend\n",
    "    \"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = [f\"Model {i+1}\" for i in range(len(evaluations))]\n",
    "    \n",
    "    # 1. Radar chart for key metrics\n",
    "    metrics = ['rouge_f1', 'word_overlap', 'keyword_coverage', 'reasoning_diversity']\n",
    "    metric_names = ['ROUGE-F1', 'Word Overlap', 'Keyword Coverage', 'Reasoning Diversity']\n",
    "    \n",
    "    # Set up the radar chart\n",
    "    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    for i, evaluation in enumerate(evaluations):\n",
    "        # Extract values, defaulting to 0 if not present\n",
    "        values = []\n",
    "        for metric in metrics:\n",
    "            if metric in evaluation:\n",
    "                values.append(evaluation[metric])\n",
    "            else:\n",
    "                values.append(0)\n",
    "        \n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        ax.plot(angles, values, linewidth=2, label=model_names[i])\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Set the labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metric_names)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Medical LLM Evaluation Metrics', size=15)\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Bar chart for overall quality scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate an overall score based on available metrics\n",
    "    overall_scores = []\n",
    "    for evaluation in evaluations:\n",
    "        score_components = []\n",
    "        if 'rouge_f1' in evaluation:\n",
    "            score_components.append(evaluation['rouge_f1'])\n",
    "        if 'word_overlap' in evaluation:\n",
    "            score_components.append(evaluation['word_overlap'])\n",
    "        if 'keyword_coverage' in evaluation:\n",
    "            score_components.append(evaluation['keyword_coverage'])\n",
    "        if 'reasoning_diversity' in evaluation and evaluation['reasoning_diversity'] > 0:\n",
    "            score_components.append(evaluation['reasoning_diversity'])\n",
    "        \n",
    "        # Add support ratio if available\n",
    "        if 'support_ratio' in evaluation:\n",
    "            score_components.append(evaluation['support_ratio'])\n",
    "        \n",
    "        # Calculate the average of available metrics\n",
    "        overall_score = sum(score_components) / len(score_components) if score_components else 0\n",
    "        overall_scores.append(overall_score)\n",
    "    \n",
    "    plt.bar(model_names, overall_scores, color='skyblue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Overall Quality Score')\n",
    "    plt.title('Overall Quality Scores')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add score labels on top of bars\n",
    "    for i, score in enumerate(overall_scores):\n",
    "        plt.text(i, score + 0.02, f'{score:.2f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Stacked bar chart for different types of issues\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate normalized issue rates\n",
    "    hallucination_counts = []\n",
    "    uncertainty_counts = []\n",
    "    missing_keyword_rates = []\n",
    "    \n",
    "    for evaluation in evaluations:\n",
    "        # Hallucinations (from potential_hallucinations if available)\n",
    "        if 'potential_hallucinations' in evaluation:\n",
    "            hallucination_count = min(len(evaluation['potential_hallucinations']), 5) / 5\n",
    "        else:\n",
    "            hallucination_count = 0\n",
    "        hallucination_counts.append(hallucination_count)\n",
    "        \n",
    "        # Uncertainty markers\n",
    "        if 'uncertainty_markers' in evaluation:\n",
    "            # Normalize uncertainty markers (0-5 scale)\n",
    "            uncertainty_count = min(evaluation['uncertainty_markers'], 5) / 5\n",
    "        else:\n",
    "            uncertainty_count = 0\n",
    "        uncertainty_counts.append(uncertainty_count)\n",
    "        \n",
    "        # Missing keywords\n",
    "        if 'keyword_coverage' in evaluation:\n",
    "            # Convert keyword coverage to missing keyword rate\n",
    "            missing_keyword_rate = 1 - evaluation['keyword_coverage']\n",
    "        else:\n",
    "            missing_keyword_rate = 0\n",
    "        missing_keyword_rates.append(missing_keyword_rate)\n",
    "    \n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(model_names, hallucination_counts, width, label='Potential Hallucinations', color='salmon')\n",
    "    plt.bar(model_names, uncertainty_counts, width, bottom=hallucination_counts, label='Uncertainty Markers', color='lightskyblue')\n",
    "    plt.bar(model_names, missing_keyword_rates, width, bottom=[h+u for h, u in zip(hallucination_counts, uncertainty_counts)], label='Missing Keywords', color='lightgreen')\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Normalized Issue Rate')\n",
    "    plt.title('Medical Content Issues by Type')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def comprehensive_evaluation_pipeline(model_output, reference_text, question, medical_keywords=None):\n",
    "    \"\"\"\n",
    "    Run a comprehensive evaluation pipeline using the simple_evaluation functions.\n",
    "    \n",
    "    Args:\n",
    "        model_output: Text generated by the LLM\n",
    "        reference_text: Gold-standard reference text\n",
    "        question: The medical question that was asked\n",
    "        medical_keywords: Optional list of medical keywords to check for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with comprehensive evaluation metrics\n",
    "    \"\"\"\n",
    "    # Basic evaluation\n",
    "    basic_eval = evaluate_medical_output(model_output, reference_text, medical_keywords)\n",
    "    \n",
    "    # Extract and evaluate claims\n",
    "    claims = extract_medical_claims(model_output)\n",
    "    claim_eval = evaluate_medical_claims(claims, reference_text)\n",
    "    \n",
    "    # Detect potential hallucinations\n",
    "    hallucinations = detect_potential_hallucinations(model_output, [reference_text])\n",
    "    \n",
    "    # Evaluate medical reasoning\n",
    "    reasoning_eval = evaluate_medical_reasoning(model_output)\n",
    "    \n",
    "    # Combine all evaluations\n",
    "    combined_eval = {**basic_eval}\n",
    "    \n",
    "    # Add claim evaluation metrics\n",
    "    combined_eval['claims_count'] = claim_eval['total_claims']\n",
    "    combined_eval['support_ratio'] = claim_eval['support_ratio']\n",
    "    \n",
    "    # Add hallucination information\n",
    "    combined_eval['potential_hallucinations'] = hallucinations\n",
    "    combined_eval['hallucination_count'] = len(hallucinations)\n",
    "    \n",
    "    # Add reasoning metrics\n",
    "    combined_eval['reasoning_diversity'] = reasoning_eval['reasoning_diversity']\n",
    "    combined_eval['reasoning_score'] = reasoning_eval['reasoning_score'] / 10  # Normalize to 0-1\n",
    "    \n",
    "    # Calculate an overall quality score\n",
    "    score_components = [\n",
    "        basic_eval['rouge_f1'],\n",
    "        basic_eval['word_overlap'],\n",
    "        basic_eval['keyword_coverage'],\n",
    "        claim_eval['support_ratio'],\n",
    "        reasoning_eval['reasoning_diversity']\n",
    "    ]\n",
    "    combined_eval['overall_score'] = sum(score_components) / len(score_components)\n",
    "    \n",
    "    return combined_eval\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Simple Medical LLM Evaluation Visualization\")\n",
    "    \n",
    "    example_reference = \"\"\"\n",
    "    Diabetes mellitus type 2 is characterized by high blood sugar, insulin resistance, and relative lack of insulin. Common symptoms include increased thirst, frequent urination, and unexplained weight loss. Long-term complications include heart disease, strokes, diabetic retinopathy, kidney failure, and poor blood flow in the limbs. The development of type 2 diabetes is primarily due to lifestyle factors and genetics.\n",
    "    \"\"\"\n",
    "\n",
    "    example_output1 = \"\"\"\n",
    "    Type 2 diabetes is a condition where your body has high blood sugar levels. This happens because your body doesn't use insulin properly (called insulin resistance) or doesn't make enough insulin. Common signs include feeling very thirsty, needing to urinate often, and losing weight without trying. If not managed well, type 2 diabetes can lead to serious problems like heart disease, stroke, eye damage (retinopathy), kidney failure, and poor circulation in your legs and feet. While genetics play a role, lifestyle factors like diet and exercise are major contributors to developing this condition. Always consult with a healthcare provider for medical advice about diabetes management.\n",
    "    \"\"\"\n",
    "    \n",
    "    example_output2 = \"\"\"\n",
    "    Diabetes type 2 happens when blood sugar gets too high. People feel thirsty and use the bathroom a lot. It can cause problems with the heart and eyes. Eating better and exercising can help manage it.\n",
    "    \"\"\"\n",
    "    \n",
    "    medical_keywords = ['diabetes', 'insulin', 'blood sugar', 'thirst', 'urination', 'weight loss', \n",
    "                      'heart disease', 'stroke', 'retinopathy', 'kidney', 'genetics']\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    eval1 = comprehensive_evaluation_pipeline(\n",
    "        example_output1, example_reference, \n",
    "        \"What is type 2 diabetes?\", medical_keywords\n",
    "    )\n",
    "    \n",
    "    eval2 = comprehensive_evaluation_pipeline(\n",
    "        example_output2, example_reference, \n",
    "        \"What is type 2 diabetes?\", medical_keywords\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    visualize_evaluation_results(\n",
    "        [eval1, eval2], \n",
    "        [\"Detailed Response\", \"Simple Response\"]\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to LLMs in Healthcare",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
