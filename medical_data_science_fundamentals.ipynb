{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MoLue/wft_digital_medicine/blob/main/medical_data_science_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "*Author: Timo Lüders*\n",
    "\n",
    "*Last Updated: May 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Onboarding for Medical Students\n",
    "\n",
    "## Overview\n",
    "Welcome to this introductory notebook on data science for medical students! This notebook is designed to serve as an entry point into the world of data science, with a specific focus on applications in medicine and healthcare. Whether you have little to no programming experience or are looking to refresh your knowledge, this notebook will guide you through the fundamentals of data science in a medical context.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction]\n",
    "2. [Python Basics for Medical Data Science]\n",
    "3. [Data Acquisition and Preparation]\n",
    "4. [Exploratory Data Analysis]\n",
    "5. [Introduction to Medical Data Analysis]\n",
    "6. [Simple Predictive Modeling]\n",
    "7. [Ethical Considerations]\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the packages we'll need throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas matplotlib seaborn plotly scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configure visualizations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# 1. Introduction\n",
    "\n",
    "## What is Data Science?\n",
    "\n",
    "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. In the medical field, data science can help with:\n",
    "\n",
    "- **Disease diagnosis and prediction**\n",
    "- **Treatment optimization**\n",
    "- **Patient monitoring**\n",
    "- **Drug discovery and development**\n",
    "- **Healthcare operations and resource allocation**\n",
    "- **Public health surveillance**\n",
    "\n",
    "## Why Learn Data Science as a Medical Student?\n",
    "\n",
    "As healthcare becomes increasingly data-driven, understanding data science principles can help you:\n",
    "\n",
    "1. **Make evidence-based decisions** by critically evaluating research and clinical data\n",
    "2. **Contribute to medical research** by applying data analysis techniques\n",
    "3. **Improve patient care** through personalized medicine approaches\n",
    "4. **Collaborate effectively** with data scientists and informaticians\n",
    "5. **Innovate in your field** by identifying patterns and opportunities in healthcare data\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "This notebook is designed to be interactive. You'll learn best by:\n",
    "\n",
    "- **Reading the explanations** to understand concepts\n",
    "- **Running the code cells** to see results (click the cell and press Shift+Enter)\n",
    "- **Modifying the code** to experiment and deepen your understanding\n",
    "- **Completing the exercises** at the end of each section\n",
    "\n",
    "Let's start with some basic Python concepts that will form the foundation of your data science journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"python-basics\"></a>\n",
    "# 2. Some Python Basics\n",
    "\n",
    "Python has become one of the most popular languages for data science due to its simplicity, readability, and powerful libraries. In this section, we'll cover the basic Python concepts you'll need for data analysis in a medical context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Python Variables and Data Types\n",
    "\n",
    "In Python, you can store information in variables. Let's look at some examples relevant to healthcare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric data types\n",
    "patient_age = 45                  # Integer (whole number)\n",
    "body_temperature = 37.2           # Float (decimal number)\n",
    "\n",
    "# String data type\n",
    "patient_name = \"Jane Doe\"\n",
    "diagnosis = \"Type 2 Diabetes\"\n",
    "\n",
    "# Boolean data type\n",
    "is_admitted = True\n",
    "has_allergies = False\n",
    "\n",
    "# Print the variables and their types\n",
    "print(f\"Patient: {patient_name}, Type: {type(patient_name)}\")\n",
    "print(f\"Age: {patient_age}, Type: {type(patient_age)}\")\n",
    "print(f\"Temperature: {body_temperature}°C, Type: {type(body_temperature)}\")\n",
    "print(f\"Diagnosis: {diagnosis}, Type: {type(diagnosis)}\")\n",
    "print(f\"Admitted: {is_admitted}, Type: {type(is_admitted)}\")\n",
    "print(f\"Has Allergies: {has_allergies}, Type: {type(has_allergies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Structures in Python\n",
    "\n",
    "Python has several built-in data structures that are useful for organizing and manipulating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "Lists are ordered, mutable collections that can contain different data types. In a medical context, you might use lists to store a series of measurements or observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of blood glucose readings (mg/dL) over a week\n",
    "glucose_readings = [95, 105, 110, 98, 102, 115, 107]\n",
    "\n",
    "# Accessing elements (indexing starts at 0)\n",
    "print(f\"First reading: {glucose_readings[0]} mg/dL\")\n",
    "print(f\"Last reading: {glucose_readings[-1]} mg/dL\")\n",
    "\n",
    "# Slicing a list (get readings from day 2 to day 5)\n",
    "print(f\"Readings from day 2 to day 5: {glucose_readings[1:5]} mg/dL\")\n",
    "\n",
    "# Adding a new reading\n",
    "glucose_readings.append(101)\n",
    "print(f\"Updated readings: {glucose_readings}\")\n",
    "\n",
    "# Calculating statistics\n",
    "average_glucose = sum(glucose_readings) / len(glucose_readings)\n",
    "print(f\"Average glucose level: {average_glucose:.1f} mg/dL\")\n",
    "print(f\"Minimum glucose level: {min(glucose_readings)} mg/dL\")\n",
    "print(f\"Maximum glucose level: {max(glucose_readings)} mg/dL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Dictionaries store key-value pairs and are excellent for representing structured data, such as patient records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a patient record as a dictionary\n",
    "patient = {\n",
    "    \"id\": \"P12345\",\n",
    "    \"name\": \"John Smith\",\n",
    "    \"age\": 58,\n",
    "    \"gender\": \"Male\",\n",
    "    \"diagnosis\": \"Hypertension\",\n",
    "    \"medications\": [\"Lisinopril\", \"Hydrochlorothiazide\"],\n",
    "    \"vital_signs\": {\n",
    "        \"blood_pressure\": \"140/90\",\n",
    "        \"heart_rate\": 72,\n",
    "        \"temperature\": 36.8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Accessing dictionary values\n",
    "print(f\"Patient: {patient['name']}\")\n",
    "print(f\"Diagnosis: {patient['diagnosis']}\")\n",
    "print(f\"Medications: {', '.join(patient['medications'])}\")\n",
    "print(f\"Blood Pressure: {patient['vital_signs']['blood_pressure']} mmHg\")\n",
    "\n",
    "# Adding new information\n",
    "patient[\"allergies\"] = [\"Penicillin\"]\n",
    "print(f\"Allergies: {', '.join(patient['allergies'])}\")\n",
    "\n",
    "# Updating information\n",
    "patient[\"vital_signs\"][\"blood_pressure\"] = \"135/85\"\n",
    "print(f\"Updated Blood Pressure: {patient['vital_signs']['blood_pressure']} mmHg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Basic Python Functions\n",
    "\n",
    "Functions allow you to encapsulate code that performs specific tasks. In medical data analysis, you might create functions to calculate health metrics or process patient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Body Mass Index (BMI)\n",
    "def calculate_bmi(weight_kg, height_m):\n",
    "    \"\"\"Calculate BMI given weight in kg and height in meters.\"\"\"\n",
    "    if height_m <= 0:\n",
    "        return \"Height must be positive\"\n",
    "    bmi = weight_kg / (height_m ** 2)\n",
    "    return bmi\n",
    "\n",
    "# Function to interpret BMI\n",
    "def interpret_bmi(bmi):\n",
    "    \"\"\"Interpret BMI according to WHO classification.\"\"\"\n",
    "    if bmi < 18.5:\n",
    "        return \"Underweight\"\n",
    "    elif bmi < 25:\n",
    "        return \"Normal weight\"\n",
    "    elif bmi < 30:\n",
    "        return \"Overweight\"\n",
    "    else:\n",
    "        return \"Obese\"\n",
    "\n",
    "# Using the functions\n",
    "weight = 70  # kg\n",
    "height = 1.75  # m\n",
    "\n",
    "patient_bmi = calculate_bmi(weight, height)\n",
    "bmi_category = interpret_bmi(patient_bmi)\n",
    "\n",
    "print(f\"Weight: {weight} kg, Height: {height} m\")\n",
    "print(f\"BMI: {patient_bmi:.1f}\")\n",
    "print(f\"Category: {bmi_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Introduction to NumPy\n",
    "\n",
    "NumPy (Numerical Python) is a fundamental library for scientific computing in Python. It provides support for arrays, matrices, and many mathematical functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NumPy arrays for medical data\n",
    "import numpy as np\n",
    "\n",
    "# Blood pressure readings (systolic) for 10 patients\n",
    "systolic_bp = np.array([120, 135, 142, 118, 125, 131, 145, 122, 119, 138])\n",
    "print(f\"Systolic BP readings: {systolic_bp}\")\n",
    "\n",
    "# Basic statistics with NumPy\n",
    "print(f\"Mean systolic BP: {np.mean(systolic_bp):.1f} mmHg\")\n",
    "print(f\"Median systolic BP: {np.median(systolic_bp):.1f} mmHg\")\n",
    "print(f\"Standard deviation: {np.std(systolic_bp):.2f} mmHg\")\n",
    "\n",
    "# Filtering data (patients with high blood pressure > 130 mmHg)\n",
    "high_bp = systolic_bp[systolic_bp > 130]\n",
    "print(f\"High BP readings: {high_bp}\")\n",
    "print(f\"Number of patients with high BP: {len(high_bp)}\")\n",
    "print(f\"Percentage with high BP: {(len(high_bp) / len(systolic_bp)) * 100:.1f}%\")\n",
    "\n",
    "# Creating a 2D array for multiple measurements\n",
    "# Rows: patients, Columns: [systolic BP, diastolic BP, heart rate]\n",
    "vital_signs = np.array([\n",
    "    [120, 80, 72],  # Patient 1\n",
    "    [135, 85, 78],  # Patient 2\n",
    "    [142, 92, 84],  # Patient 3\n",
    "    [118, 75, 68],  # Patient 4\n",
    "    [125, 82, 70]   # Patient 5\n",
    "])\n",
    "\n",
    "print(\"\\nVital signs for 5 patients:\")\n",
    "print(vital_signs)\n",
    "\n",
    "# Accessing data for specific patients\n",
    "print(f\"\\nPatient 3's vital signs: {vital_signs[2]}\")\n",
    "\n",
    "# Accessing specific measurements across all patients\n",
    "print(f\"\\nAll systolic BP readings: {vital_signs[:, 0]}\")\n",
    "print(f\"All diastolic BP readings: {vital_signs[:, 1]}\")\n",
    "print(f\"All heart rates: {vital_signs[:, 2]}\")\n",
    "\n",
    "# Calculating mean values for each measurement\n",
    "print(f\"\\nMean systolic BP: {np.mean(vital_signs[:, 0]):.1f} mmHg\")\n",
    "print(f\"Mean diastolic BP: {np.mean(vital_signs[:, 1]):.1f} mmHg\")\n",
    "print(f\"Mean heart rate: {np.mean(vital_signs[:, 2]):.1f} bpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Introduction to Pandas\n",
    "\n",
    "Pandas is a powerful data manipulation library built on top of NumPy. It provides data structures like DataFrames that are ideal for working with tabular data, such as patient records or clinical trial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame for patient data\n",
    "import pandas as pd\n",
    "\n",
    "# Sample patient data\n",
    "data = {\n",
    "    'PatientID': ['P001', 'P002', 'P003', 'P004', 'P005', 'P006', 'P007', 'P008'],\n",
    "    'Age': [45, 62, 35, 58, 41, 72, 29, 53],\n",
    "    'Gender': ['M', 'F', 'F', 'M', 'M', 'F', 'M', 'F'],\n",
    "    'BloodType': ['A+', 'O-', 'B+', 'AB+', 'A-', 'O+', 'B-', 'A+'],\n",
    "    'Cholesterol': [185, 220, 166, 240, 190, 205, 172, 210],\n",
    "    'BloodPressure': ['120/80', '135/90', '118/75', '142/92', '125/82', '148/95', '115/78', '130/85'],\n",
    "    'Smoker': [False, False, False, True, True, False, True, False],\n",
    "    'Diabetic': [False, True, False, True, False, True, False, False]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "patients_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Patient Records:\")\n",
    "patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DataFrame operations\n",
    "\n",
    "# View basic information about the DataFrame\n",
    "print(\"DataFrame Information:\")\n",
    "patients_df.info()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "patients_df.describe()\n",
    "\n",
    "# Selecting specific columns\n",
    "print(\"\\nPatient IDs and Ages:\")\n",
    "patients_df[['PatientID', 'Age']].head()\n",
    "\n",
    "# Filtering data (patients who are diabetic)\n",
    "diabetic_patients = patients_df[patients_df['Diabetic'] == True]\n",
    "print(\"\\nDiabetic Patients:\")\n",
    "diabetic_patients\n",
    "\n",
    "# Multiple conditions (diabetic patients with high cholesterol > 200)\n",
    "high_risk_patients = patients_df[(patients_df['Diabetic'] == True) & (patients_df['Cholesterol'] > 200)]\n",
    "print(\"\\nHigh Risk Patients (Diabetic with Cholesterol > 200):\")\n",
    "high_risk_patients\n",
    "\n",
    "# Grouping and aggregation\n",
    "print(\"\\nAverage Cholesterol by Gender:\")\n",
    "patients_df.groupby('Gender')['Cholesterol'].mean()\n",
    "\n",
    "# More complex grouping\n",
    "print(\"\\nStatistics by Gender and Diabetic Status:\")\n",
    "patients_df.groupby(['Gender', 'Diabetic'])[['Age', 'Cholesterol']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Working with Medical Data\n",
    "\n",
    "Now it's your turn to practice! Try to complete the following tasks using the patients DataFrame we created above:\n",
    "\n",
    "1. Calculate the average age of smokers vs. non-smokers\n",
    "2. Find the patient with the highest cholesterol level\n",
    "3. Create a new column called 'Risk_Category' that categorizes patients as:\n",
    "   - 'High Risk' if they are both diabetic and smokers\n",
    "   - 'Medium Risk' if they are either diabetic or smokers (but not both)\n",
    "   - 'Low Risk' if they are neither diabetic nor smokers\n",
    "4. Count how many patients fall into each risk category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate average age of smokers vs. non-smokers\n",
    "print(\"\\nAverage Age by Smoking Status:\")\n",
    "\n",
    "# 2. Find patient with highest cholesterol\n",
    "print(\"\\nPatient with Highest Cholesterol:\")\n",
    "\n",
    "# 3. Create Risk_Category column\n",
    "print(\"\\Risk Category Column:\")\n",
    "\n",
    "# 4. Count patients in each risk category\n",
    "print(\"\\nPatients in Each Risk Category:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-acquisition\"></a>\n",
    "# 3. Data Acquisition and Preparation\n",
    "\n",
    "In this section, we'll learn how to acquire data from different sources and prepare it for analysis. These are crucial steps in any data science project, especially in healthcare where data quality directly impacts patient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading Data from Different Sources\n",
    "\n",
    "Let's explore how to load data from various sources commonly used in healthcare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV (Comma-Separated Values) files are one of the most common formats for storing tabular data. Let's load a synthetic diabetes dataset.\n",
    "\n",
    "We use here a synthetic dataset. Synthetic data refers to artificially generated data that mimics the statistical properties and patterns of real-world data without containing any actual patient information. In healthcare, synthetic data is created using various techniques like statistical modeling, machine learning algorithms, or generative AI to produce realistic but entirely fictional medical records, images, or clinical narratives.\n",
    "\n",
    "Key points about synthetic data in healthcare:\n",
    "\n",
    "- It preserves privacy by eliminating the risk of exposing protected health information (PHI)\n",
    "- It can be used to augment limited datasets for training AI models\n",
    "- It helps overcome data sharing restrictions in medical research\n",
    "- It can be designed to represent rare conditions or diverse patient populations\n",
    "- It enables testing of algorithms in scenarios that might be rare in real data\n",
    "- It addresses regulatory concerns related to HIPAA and other privacy laws\n",
    "\n",
    "It contains the following columns:\n",
    "- PatientAge: Age of the patient in years\n",
    "- BodyMassIndex: BMI value (weight in kg / height in m²)\n",
    "- BloodGlucose: Blood glucose level in mg/dL\n",
    "- SystolicBP: Systolic blood pressure in mmHg\n",
    "- InsulinLevel: Insulin level in μU/mL\n",
    "- SkinFold: Skin fold thickness in mm\n",
    "- FamilyHistory: Diabetes pedigree function (a score of diabetes genetic influence)\n",
    "- ActivityLevel: Physical activity level (0-4, where 0=Sedentary, 4=Very Active)\n",
    "- DiabetesStatus: Target variable (1=Diabetic, 0=Non-diabetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset path\n",
    "synthetic_diabetes_url = \"https://raw.githubusercontent.com/MoLue/wft_digital_medicine/main/data/synthetic_diabetes.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "diabetes_df = pd.read_csv(synthetic_diabetes_url)\n",
    "\n",
    "\n",
    "# Uncomment the following lines if you want to load the dataset from a local file\n",
    "# Define the path to the local file\n",
    "# This assumes the file is in the 'data' directory of your project\n",
    "#data_dir = os.path.join(os.path.dirname('.'), 'data')\n",
    "#file_path = os.path.join(data_dir, 'synthetic_diabetes.csv')\n",
    "\n",
    "# Load the dataset\n",
    "# diabetes_df = pd.read_csv(synthetic_diabetes_url)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Synthetic Diabetes Dataset:\")\n",
    "diabetes_df.head()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "diabetes_df.info()\n",
    "\n",
    "# Display statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "diabetes_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Handling Missing Values in Medical Data\n",
    "\n",
    "Missing values are common in healthcare datasets and can significantly impact analysis results. Let's explore different approaches to identify and handle missing values in our Synthetic Diabetes Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(diabetes_df.isnull().sum())\n",
    "\n",
    "# In medical datasets, missing values might be represented as zeros or extreme values\n",
    "# Let's identify potential implicit missing values (zeros in certain columns where zero is not physiologically possible)\n",
    "print(\"\\nPotential implicit missing values (zeros in columns where zero is physiologically unlikely):\")\n",
    "for column in ['BloodGlucose', 'SystolicBP', 'InsulinLevel', 'BodyMassIndex']:\n",
    "    zero_count = len(diabetes_df[diabetes_df[column] == 0])\n",
    "    print(f\"{column}: {zero_count} zeros ({zero_count/len(diabetes_df)*100:.1f}% of data)\")\n",
    "\n",
    "# Create a copy of the dataset to work with\n",
    "diabetes_clean = diabetes_df.copy()\n",
    "\n",
    "# Method 1: Replace with mean (for numerical data)\n",
    "print(\"\\nMethod 1: Replace with mean\")\n",
    "for column in ['BloodGlucose', 'SystolicBP', 'InsulinLevel', 'BodyMassIndex']:\n",
    "    # Replace zeros with NaN\n",
    "    diabetes_clean[column] = diabetes_clean[column].replace(0, np.nan)\n",
    "    # Replace NaN with mean\n",
    "    mean_value = diabetes_clean[column].mean()\n",
    "    diabetes_clean[column] = diabetes_clean[column].fillna(mean_value)\n",
    "    print(f\"{column} - Mean value used for replacement: {mean_value:.2f}\")\n",
    "\n",
    "# Method 2: Replace with median (more robust to outliers)\n",
    "print(\"\\nMethod 2: Replace with median (more robust to outliers)\")\n",
    "diabetes_clean2 = diabetes_df.copy()\n",
    "for column in ['BloodGlucose', 'SystolicBP', 'InsulinLevel', 'BodyMassIndex']:\n",
    "    # Replace zeros with NaN\n",
    "    diabetes_clean2[column] = diabetes_clean2[column].replace(0, np.nan)\n",
    "    # Replace NaN with median\n",
    "    median_value = diabetes_clean2[column].median()\n",
    "    diabetes_clean2[column] = diabetes_clean2[column].fillna(median_value)\n",
    "    print(f\"{column} - Median value used for replacement: {median_value:.2f}\")\n",
    "\n",
    "# Method 3: KNN Imputation (more sophisticated approach)\n",
    "print(\"\\nMethod 3: KNN Imputation\")\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a copy of the dataset\n",
    "diabetes_clean3 = diabetes_df.copy()\n",
    "\n",
    "# Replace zeros with NaN\n",
    "for column in ['BloodGlucose', 'SystolicBP', 'InsulinLevel', 'BodyMassIndex']:\n",
    "    diabetes_clean3[column] = diabetes_clean3[column].replace(0, np.nan)\n",
    "\n",
    "# Apply KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "diabetes_imputed = pd.DataFrame(imputer.fit_transform(diabetes_clean3), \n",
    "                                columns=diabetes_clean3.columns)\n",
    "\n",
    "print(\"Before and after KNN imputation (first 5 rows):\")\n",
    "print(\"\\nBefore imputation:\")\n",
    "print(diabetes_clean3.head())\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(diabetes_imputed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Cleaning Techniques for Medical Data\n",
    "\n",
    "Clean data is crucial for accurate analysis and modeling, especially in healthcare where decisions can impact patient care. Let's explore techniques to clean our Frankfurt diabetes dataset.\n",
    "## Data Cleaning and Preprocessing Fundamentals\n",
    "\n",
    "Data cleaning is a critical step in any data science workflow, especially in healthcare where data quality directly impacts analysis outcomes. The following techniques demonstrate essential preprocessing steps for medical datasets:\n",
    "\n",
    "### 1. Outlier Detection and Handling\n",
    "\n",
    "Outliers are extreme values that deviate significantly from other observations. In medical data, outliers might represent measurement errors or genuinely unusual cases.\n",
    "\n",
    "**The Interquartile Range (IQR) Method:**\n",
    "- Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "- Compute IQR = Q3 - Q1\n",
    "- Define boundaries: Lower bound = Q1 - 1.5×IQR, Upper bound = Q3 + 1.5×IQR\n",
    "- Values outside these boundaries are considered outliers\n",
    "\n",
    "This method is particularly useful for clinical variables like blood glucose, BMI, and insulin levels where extreme values can skew analysis.\n",
    "\n",
    "### 2. Outlier Treatment with Winsorization\n",
    "\n",
    "Winsorization (capping) replaces extreme values with less extreme ones rather than removing them entirely:\n",
    "- Values below the lower bound are set to the lower bound\n",
    "- Values above the upper bound are set to the upper bound\n",
    "\n",
    "This approach preserves the overall data structure while reducing the impact of extreme values on statistical analyses and machine learning models.\n",
    "\n",
    "### 3. Standardizing Categorical Data\n",
    "\n",
    "In healthcare datasets, categorical variables often need standardization to ensure consistency:\n",
    "- Converting numeric codes to meaningful labels (e.g., activity levels)\n",
    "- Ensuring consistent terminology across the dataset\n",
    "- Standardizing units and measurement scales\n",
    "\n",
    "This improves interpretability and facilitates proper analysis of categorical medical data.\n",
    "\n",
    "### 4. Duplicate Detection and Removal\n",
    "\n",
    "Duplicate records can arise from:\n",
    "- Multiple patient visits\n",
    "- Data entry errors\n",
    "- Merging of datasets\n",
    "\n",
    "Removing duplicates ensures that each observation is counted only once, preventing bias in statistical analyses and machine learning models.\n",
    "\n",
    "These preprocessing techniques form the foundation of reliable healthcare data analysis, ensuring that subsequent modeling efforts are based on clean, consistent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy\n",
    "diabetes_working = diabetes_imputed.copy()\n",
    "\n",
    "# 1. Handling Outliers\n",
    "print(\"\\n1. Detecting and Handling Outliers\")\n",
    "\n",
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in key columns\n",
    "for column in ['BloodGlucose', 'BodyMassIndex', 'InsulinLevel']:\n",
    "    outliers, lower, upper = detect_outliers_iqr(diabetes_working, column)\n",
    "    print(f\"\\nOutliers in {column}: {len(outliers)} values\")\n",
    "    print(f\"Lower bound: {lower:.2f}, Upper bound: {upper:.2f}\")\n",
    "    print(f\"Min value: {diabetes_working[column].min():.2f}, Max value: {diabetes_working[column].max():.2f}\")\n",
    "    \n",
    "    # Display a few outliers if they exist\n",
    "    if len(outliers) > 0:\n",
    "        print(\"Sample outliers:\")\n",
    "        print(outliers[[column]].head(3))\n",
    "\n",
    "# 2. Handling outliers - Capping method\n",
    "print(\"\\n2. Handling outliers - Capping method (Winsorization)\")\n",
    "diabetes_capped = diabetes_working.copy()\n",
    "\n",
    "for column in ['BloodGlucose', 'BodyMassIndex', 'InsulinLevel']:\n",
    "    _, lower, upper = detect_outliers_iqr(diabetes_capped, column)\n",
    "    # Cap the values\n",
    "    diabetes_capped[column] = diabetes_capped[column].clip(lower=lower, upper=upper)\n",
    "    print(f\"{column} after capping - Min: {diabetes_capped[column].min():.2f}, Max: {diabetes_capped[column].max():.2f}\")\n",
    "\n",
    "# 3. Standardizing text data (if applicable)\n",
    "print(\"\\n3. Standardizing Text Data (Example)\")\n",
    "print(\"In medical datasets, text fields like diagnoses or medications often need standardization.\")\n",
    "print(\"Example code for standardizing a categorical variable:\")\n",
    "print(\"diabetes_df['PhysicalActivity'] = diabetes_df['PhysicalActivity'].replace({0: 'Sedentary', 1: 'Light', 2: 'Moderate', 3: 'Active', 4: 'Very Active'})\")\n",
    "\n",
    "# 4. Removing duplicates\n",
    "print(\"\\n4. Checking and Removing Duplicates\")\n",
    "duplicates = diabetes_working.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    diabetes_working = diabetes_working.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {len(diabetes_working)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Data Validation Methods\n",
    "\n",
    "Data validation ensures the quality and reliability of our dataset. This is particularly important in medical research where data quality directly impacts conclusions that may affect patient care.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE:\n",
    "# 1. Range validation\n",
    "print(\"\\n1. Range Validation\")\n",
    "# TODO: Choose appropriate ranges for each variable based on medical knowledge\n",
    "validation_ranges = {\n",
    "    'PatientAge': (0, 0),\n",
    "    'BloodGlucose': (0, 0),\n",
    "    'SystolicBP': (0, 0),\n",
    "    'InsulinLevel': (0, 0),\n",
    "    'BodyMassIndex': (0, 0),\n",
    "    'FamilyHistory': (0, 0),\n",
    "    'ActivityLevel': (0, 0)\n",
    "}\n",
    "\n",
    "# Check if values are within expected ranges\n",
    "for column, (min_val, max_val) in validation_ranges.items():\n",
    "    out_of_range = diabetes_working[(diabetes_working[column] < min_val) | \n",
    "                                   (diabetes_working[column] > max_val)]\n",
    "    if len(out_of_range) > 0:\n",
    "        print(f\"{column}: {len(out_of_range)} values out of expected range ({min_val}-{max_val})\")\n",
    "    else:\n",
    "        print(f\"{column}: All values within expected range\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More validation approaches\n",
    "\n",
    "# 2. Consistency checks\n",
    "print(\"\\n2. Consistency Checks\")\n",
    "# Example: In diabetes data, patients with high glucose should generally have higher insulin levels\n",
    "# This is a simplified example and not always medically accurate\n",
    "high_glucose = diabetes_working[diabetes_working['BloodGlucose'] > 180]\n",
    "low_insulin_count = high_glucose[high_glucose['InsulinLevel'] < 50].shape[0]\n",
    "print(f\"Potential inconsistency: {low_insulin_count} patients with high glucose (>180) but low insulin (<50)\")\n",
    "\n",
    "# 3. Completeness check\n",
    "print(\"\\n3. Completeness Check\")\n",
    "completeness = (diabetes_working.count() / len(diabetes_working)) * 100\n",
    "print(\"Completeness percentage for each column:\")\n",
    "for column, percentage in completeness.items():\n",
    "    print(f\"{column}: {percentage:.2f}%\")\n",
    "\n",
    "# 4. Data type validation\n",
    "print(\"\\n4. Data Type Validation\")\n",
    "print(diabetes_working.dtypes)\n",
    "\n",
    "# 5. Creating a validation report function\n",
    "print(\"\\n5. Creating a Validation Report Function\")\n",
    "\n",
    "def validate_diabetes_data(df):\n",
    "    \"\"\"Validate diabetes dataset and return a report of issues found.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        for column, count in missing.items():\n",
    "            if count > 0:\n",
    "                issues.append(f\"Missing values in {column}: {count}\")\n",
    "    \n",
    "    # Check for out-of-range values\n",
    "    for column, (min_val, max_val) in validation_ranges.items():\n",
    "        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "        if len(out_of_range) > 0:\n",
    "            issues.append(f\"Out-of-range values in {column}: {len(out_of_range)}\")\n",
    "    \n",
    "    # Check for potential inconsistencies\n",
    "    high_glucose = df[df['BloodGlucose'] > 180]\n",
    "    low_insulin_count = high_glucose[high_glucose['InsulinLevel'] < 50].shape[0]\n",
    "    if low_insulin_count > 0:\n",
    "        issues.append(f\"Potential inconsistency: {low_insulin_count} patients with high glucose but low insulin\")\n",
    "    \n",
    "    # Return report\n",
    "    if issues:\n",
    "        return f\"Validation issues found ({len(issues)}):\\n\" + \"\\n\".join(issues)\n",
    "    else:\n",
    "        return \"Validation passed: No issues found.\"\n",
    "\n",
    "# Run validation on our cleaned dataset\n",
    "validation_report = validate_diabetes_data(diabetes_working)\n",
    "print(validation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Data Transformation Techniques\n",
    "\n",
    "Data transformation is often necessary to prepare data for analysis and modeling. Let's explore common transformation techniques used in medical data science.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy\n",
    "diabetes_transform = diabetes_working.copy()\n",
    "\n",
    "# 1. Normalization (Min-Max Scaling)\n",
    "print(\"\\n1. Normalization (Min-Max Scaling)\")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select numerical columns to normalize\n",
    "numeric_cols = ['PatientAge', 'BloodGlucose', 'SystolicBP', 'InsulinLevel', \n",
    "                'BodyMassIndex', 'FamilyHistory']\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "diabetes_transform[numeric_cols] = scaler.fit_transform(diabetes_transform[numeric_cols])\n",
    "\n",
    "print(\"After normalization (first 5 rows):\")\n",
    "print(diabetes_transform.head())\n",
    "\n",
    "# 2. Standardization (Z-score)\n",
    "print(\"\\n2. Standardization (Z-score)\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a new copy for standardization\n",
    "diabetes_standardized = diabetes_working.copy()\n",
    "\n",
    "# Apply standardization\n",
    "std_scaler = StandardScaler()\n",
    "diabetes_standardized[numeric_cols] = std_scaler.fit_transform(diabetes_standardized[numeric_cols])\n",
    "\n",
    "print(\"After standardization (first 5 rows):\")\n",
    "print(diabetes_standardized.head())\n",
    "\n",
    "# 3. Log Transformation (useful for skewed data)\n",
    "print(\"\\n3. Log Transformation\")\n",
    "# Create a copy for log transformation\n",
    "diabetes_log = diabetes_working.copy()\n",
    "\n",
    "# Apply log transformation to insulin (often skewed in diabetes data)\n",
    "# Add a small constant to handle zeros\n",
    "diabetes_log['Insulin_Log'] = np.log1p(diabetes_log['InsulinLevel'])\n",
    "\n",
    "# Compare distributions\n",
    "print(\"Insulin distribution before and after log transformation:\")\n",
    "print(f\"Original - Mean: {diabetes_log['InsulinLevel'].mean():.2f}, Std: {diabetes_log['InsulinLevel'].std():.2f}\")\n",
    "print(f\"Log transformed - Mean: {diabetes_log['Insulin_Log'].mean():.2f}, Std: {diabetes_log['Insulin_Log'].std():.2f}\")\n",
    "\n",
    "# 4. Binning/Categorization\n",
    "print(\"\\n4. Binning/Categorization\")\n",
    "# Create age groups\n",
    "diabetes_binned = diabetes_working.copy()\n",
    "diabetes_binned['Age_Group'] = pd.cut(diabetes_binned['PatientAge'], \n",
    "                                     bins=[18, 35, 50, 65, 90],\n",
    "                                     labels=['18-35', '36-50', '51-65', '66+'])\n",
    "\n",
    "# Create BMI categories according to WHO classification\n",
    "diabetes_binned['BMI_Category'] = pd.cut(diabetes_binned['BodyMassIndex'],\n",
    "                                        bins=[0, 18.5, 25, 30, 100],\n",
    "                                        labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "\n",
    "# Show the distribution of these categories\n",
    "print(\"\\nAge Group Distribution:\")\n",
    "print(diabetes_binned['Age_Group'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nBMI Category Distribution:\")\n",
    "print(diabetes_binned['BMI_Category'].value_counts().sort_index())\n",
    "\n",
    "# 5. One-Hot Encoding\n",
    "print(\"\\n5. One-Hot Encoding\")\n",
    "# One-hot encode the categorical variables we created\n",
    "diabetes_encoded = pd.get_dummies(diabetes_binned, columns=['Age_Group', 'BMI_Category'])\n",
    "\n",
    "# Display the new columns created\n",
    "print(\"New columns after one-hot encoding:\")\n",
    "new_columns = [col for col in diabetes_encoded.columns if 'Age_Group' in col or 'BMI_Category' in col]\n",
    "print(new_columns)\n",
    "print(diabetes_encoded[new_columns].head())\n",
    "\n",
    "# 6. Feature Engineering\n",
    "print(\"\\n6. Feature Engineering\")\n",
    "# Create new features that might be relevant for diabetes\n",
    "diabetes_features = diabetes_working.copy()\n",
    "\n",
    "# BMI * Glucose interaction (higher values might indicate higher risk)\n",
    "diabetes_features['BMI_Glucose_Interaction'] = diabetes_features['BodyMassIndex'] * diabetes_features['BloodGlucose'] / 100\n",
    "\n",
    "# Insulin to Glucose ratio (measure of insulin resistance)\n",
    "diabetes_features['Insulin_Glucose_Ratio'] = diabetes_features['InsulinLevel'] / diabetes_features['BloodGlucose']\n",
    "\n",
    "# Age-adjusted diabetes pedigree\n",
    "diabetes_features['Age_Adjusted_Pedigree'] = diabetes_features['FamilyHistory'] * (diabetes_features['PatientAge'] / 50)\n",
    "\n",
    "print(\"Newly engineered features (first 5 rows):\")\n",
    "print(diabetes_features[['BMI_Glucose_Interaction', 'Insulin_Glucose_Ratio', 'Age_Adjusted_Pedigree']].head())\n",
    "\n",
    "# 7. Saving the processed dataset\n",
    "print(\"\\n7. Saving the Processed Dataset\")\n",
    "print(\"Code to save the processed dataset:\")\n",
    "print(\"diabetes_processed = diabetes_features  # or whichever version you prefer\")\n",
    "print(\"diabetes_processed.to_csv('processed_frankfurt_diabetes.csv', index=False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Simple Predictive Modeling\n",
    "This section introduces basic predictive modeling concepts using the diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Introduction to Predictive Modeling Concepts\n",
    "\n",
    "Predictive modeling uses statistical techniques to predict outcomes from current data. In healthcare, predictive models can help:\n",
    "\n",
    "- Identify patients at risk for developing certain conditions\n",
    "- Predict treatment outcomes\n",
    "- Forecast disease progression\n",
    "- Support clinical decision-making\n",
    "\n",
    "Let's explore the key concepts of predictive modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score\n",
    "import os\n",
    "\n",
    "# Set some styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling Workflow in Healthcare\n",
    "\n",
    "### The Predictive Modeling Process\n",
    "\n",
    "1. **Problem Definition**: Determine what we want to predict\n",
    "   - In healthcare, this might be disease risk, treatment outcomes, or patient readmission probability\n",
    "\n",
    "2. **Data Collection**: Gather relevant data\n",
    "   - Medical records, lab results, imaging data, patient surveys, etc.\n",
    "\n",
    "3. **Data Preprocessing**: Clean and prepare data for modeling\n",
    "   - Handle missing values, outliers, and inconsistencies\n",
    "   - Normalize or standardize numerical features\n",
    "   - Encode categorical variables\n",
    "\n",
    "4. **Feature Selection**: Identify variables most relevant to our prediction\n",
    "   - Reduce dimensionality and focus on clinically relevant features\n",
    "   - Use statistical methods or domain knowledge to select important predictors\n",
    "\n",
    "5. **Model Selection**: Choose appropriate algorithm(s)\n",
    "   - Based on the nature of the prediction task and data characteristics\n",
    "   - Consider interpretability requirements in medical contexts\n",
    "\n",
    "6. **Training**: Fit the model to training data\n",
    "   - Optimize model parameters to best capture patterns in the data\n",
    "\n",
    "7. **Evaluation**: Assess model performance\n",
    "   - Use appropriate metrics (accuracy, sensitivity, specificity, AUC, etc.)\n",
    "   - Validate against clinical standards\n",
    "\n",
    "8. **Deployment**: Apply the model to make predictions\n",
    "   - Integrate into clinical workflows or decision support systems\n",
    "\n",
    "9. **Monitoring**: Track model performance over time\n",
    "   - Ensure continued accuracy as patient populations change\n",
    "   - Update as medical knowledge evolves\n",
    "\n",
    "### Types of Predictive Models\n",
    "\n",
    "1. **Classification Models**: Predict categorical outcomes\n",
    "   - Example: Predicting whether a patient will develop diabetes (yes/no)\n",
    "   - Common algorithms: Logistic Regression, Random Forest, Support Vector Machines\n",
    "   - Evaluation metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC\n",
    "\n",
    "2. **Regression Models**: Predict continuous values\n",
    "   - Example: Predicting a patient's future blood glucose level\n",
    "   - Common algorithms: Linear Regression, Ridge/Lasso Regression, Gradient Boosting\n",
    "   - Evaluation metrics: RMSE, MAE, R-squared\n",
    "\n",
    "### Training and Testing Data\n",
    "\n",
    "- **Training data**: Used to build the model (~70-80% of data)\n",
    "  - The model learns patterns and relationships from this subset\n",
    "\n",
    "- **Testing data**: Used to evaluate model performance (~20-30% of data)\n",
    "  - Simulates how the model will perform on new, unseen patients\n",
    "  - Helps detect overfitting (when a model performs well on training data but poorly on new data)\n",
    "\n",
    "- This split is crucial in healthcare applications where model generalizability directly impacts patient care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning for demonstration\n",
    "# Replace missing values and zeros with NaN\n",
    "diabetes_df = diabetes_df.replace('', np.nan)\n",
    "numeric_cols = ['PatientAge', 'BodyMassIndex', 'BloodGlucose', 'SystolicBP', \n",
    "                'InsulinLevel', 'SkinFold', 'FamilyHistory', 'ActivityLevel']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    diabetes_df[col] = pd.to_numeric(diabetes_df[col], errors='coerce')\n",
    "    # Replace zeros with NaN for columns where zero is not physiologically possible\n",
    "    if col in ['BloodGlucose', 'SystolicBP', 'InsulinLevel', 'BodyMassIndex']:\n",
    "        diabetes_df[col] = diabetes_df[col].replace(0, np.nan)\n",
    "    # Fill NaN with mean\n",
    "    diabetes_df[col] = diabetes_df[col].fillna(diabetes_df[col].mean())\n",
    "\n",
    "# Ensure target variable is properly formatted\n",
    "# First fill any NaN values in DiabetesStatus with 0 (or another appropriate value)\n",
    "diabetes_df['DiabetesStatus'] = diabetes_df['DiabetesStatus'].fillna(0).astype(int)\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(\"Cleaned Diabetes Dataset:\")\n",
    "display(diabetes_df.head())\n",
    "\n",
    "# Demonstrate the train-test split\n",
    "# Select your features and target variable\n",
    "# Your code here:\n",
    "# Set the following elements to the features (X) and target variable (y)\n",
    "# 'PatientAge', 'BodyMassIndex', 'BloodGlucose', 'SystolicBP', 'DiabetesStatus',\n",
    "# 'InsulinLevel', 'SkinFold', 'FamilyHistory', 'ActivityLevel'\n",
    "# \n",
    "# X = diabetes_df[[]]\n",
    "# y = diabetes_df[]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nData split for modeling:\")\n",
    "print(f\"Total dataset size: {len(diabetes_df)} patients\")\n",
    "print(f\"Training set: {len(X_train)} patients ({len(X_train)/len(diabetes_df)*100:.1f}%)\")\n",
    "print(f\"Testing set: {len(X_test)} patients ({len(X_test)/len(diabetes_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Simple Classification Example: Predicting Diabetes\n",
    "\n",
    "Classification models predict categorical outcomes. In this example, we'll build a simple model to predict whether a patient has diabetes based on their health indicators.\n",
    "\n",
    "We'll use logistic regression, a fundamental classification algorithm that's both interpretable and commonly used in medical research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression for Diabetes Classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare the data\n",
    "\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# Select features that are clinically relevant for diabetes prediction\n",
    "features = ['...']\n",
    "target = ''\n",
    "X = diabetes_df[features]\n",
    "y = diabetes_df[target]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "# We use a higher C value (inverse of regularization strength) for medical data\n",
    "# to prioritize sensitivity over simplicity\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# Play with the hyperparameters and observe the results. \n",
    "# Following values are good values to start with...\n",
    "C = 10\n",
    "max_iter = 1000 \n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(C=C, max_iter=max_iter, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of diabetes\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the results\n",
    "print(\"Logistic Regression Model for Diabetes Prediction\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nTrue Negatives (correctly predicted non-diabetic):\", conf_matrix[0, 0])\n",
    "print(\"False Positives (incorrectly predicted diabetic):\", conf_matrix[0, 1])\n",
    "print(\"False Negatives (incorrectly predicted non-diabetic):\", conf_matrix[1, 0])\n",
    "print(\"True Positives (correctly predicted diabetic):\", conf_matrix[1, 1])\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Diabetic', 'Diabetic'],\n",
    "            yticklabels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for Diabetes Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the results and try to make clear to you what the model is doing and what it is not doing.\n",
    "\n",
    "Next, let us have a look at the feature importance. This means we want to see which features are most important for the model to make predictions.  \n",
    "\n",
    "We can do this by looking at the coefficients of the logistic regression model. The coefficients represent the strength of the relationship between each feature and the target variable. A positive coefficient indicates a positive relationship, while a negative coefficient indicates a negative relationship. The larger the absolute value of the coefficient, the stronger the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "coefficients = log_reg.coef_[0]\n",
    "feature_importance = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})\n",
    "feature_importance = feature_importance.sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Coefficients):\")\n",
    "display(feature_importance)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance for Diabetes Prediction')\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical interpretation of the model\n",
    "print(\"\\nMedical Interpretation:\")\n",
    "print(\"1. The model achieved an accuracy of {:.1f}%, meaning it correctly classified {:.1f}% of patients.\".format(\n",
    "    accuracy*100, accuracy*100))\n",
    "print(\"2. What else? Think about what you just found out so far.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate prediction for a new patient\n",
    "print(\"\\nPrediction Example for a New Patient:\")\n",
    "new_patient = pd.DataFrame({\n",
    "    'PatientAge': [55],\n",
    "    'BodyMassIndex': [32],  # Obese\n",
    "    'BloodGlucose': [140],  # Elevated\n",
    "    'SystolicBP': [150],    # Hypertension\n",
    "    'InsulinLevel': [150],\n",
    "    'FamilyHistory': [0.8]  # Strong family history\n",
    "})\n",
    "\n",
    "print(\"New Patient Data:\")\n",
    "display(new_patient)\n",
    "\n",
    "# Scale the new patient data\n",
    "new_patient_scaled = scaler.transform(new_patient)\n",
    "\n",
    "# Make prediction\n",
    "prediction = log_reg.predict(new_patient_scaled)[0]\n",
    "probability = log_reg.predict_proba(new_patient_scaled)[0, 1]\n",
    "\n",
    "print(f\"Prediction: {'Diabetic' if prediction == 1 else 'Non-Diabetic'}\")\n",
    "print(f\"Probability of Diabetes: {probability:.2f} ({probability*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to do a clinical interpretation and set some meaningful thresholds in this hypothetical scenario\n",
    "\n",
    "high_risk_probability = 1.0  # Choose a better value based on your analysis so far\n",
    "moderate_risk_probability = 0.1  #  Choose a better value based on your analysis so far\n",
    "\n",
    "print(\"\\nClinical Interpretation:\")\n",
    "if probability > high_risk_probability:\n",
    "    print(\"High risk of diabetes. Recommend comprehensive screening and lifestyle intervention.\")\n",
    "elif probability > moderate_risk_probability:\n",
    "    print(\"Moderate risk of diabetes. Recommend follow-up testing and preventive measures.\")\n",
    "else:\n",
    "    print(\"Low risk of diabetes. Recommend routine screening as per age-appropriate guidelines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Regression Example: Predicting Blood Glucose Levels\n",
    "\n",
    "Regression models predict continuous values. In this example, we'll build a model to predict a patient's blood glucose level based on other health indicators.\n",
    "\n",
    "This type of model could help identify factors that contribute to elevated blood glucose and potentially predict future glucose levels based on current health metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression for Blood Glucose Prediction\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data\n",
    "# Select features that might influence blood glucose levels\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# How do you select the features and target now?\n",
    "\n",
    "features = []  # Update this list\n",
    "target = ''  # Update this string\n",
    "X = diabetes_df[features]\n",
    "y = diabetes_df[target]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Linear Regression Model for Blood Glucose Prediction\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f} mg/dL\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Visualize actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Blood Glucose (mg/dL)')\n",
    "plt.ylabel('Predicted Blood Glucose (mg/dL)')\n",
    "plt.title('Actual vs Predicted Blood Glucose Levels')\n",
    "plt.show()\n",
    "\n",
    "# Analyze coefficients\n",
    "coefficients = pd.DataFrame({'Feature': features, 'Coefficient': lin_reg.coef_})\n",
    "coefficients = coefficients.sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "display(coefficients)\n",
    "\n",
    "# Visualize coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=coefficients)\n",
    "plt.title('Feature Importance for Blood Glucose Prediction')\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Medical interpretation\n",
    "print(\"\\nMedical Interpretation:\")\n",
    "print(f\"1. The model explains {r2*100:.1f}% of the variation in blood glucose levels.\")\n",
    "print(f\"2. The average prediction error is ±{rmse:.1f} mg/dL.\")\n",
    "print(\"3. Insulin level likely has a strong positive relationship with blood glucose.\")\n",
    "print(\"4. BMI appears to be positively associated with blood glucose levels.\")\n",
    "print(\"5. Physical activity level may have a negative relationship (more activity, lower glucose).\")\n",
    "\n",
    "# Demonstrate prediction for a new patient\n",
    "print(\"\\nBlood Glucose Prediction Example:\")\n",
    "new_patient = pd.DataFrame({\n",
    "    'PatientAge': [45],\n",
    "    'BodyMassIndex': [28],  # Overweight\n",
    "    'SystolicBP': [130],    # Elevated\n",
    "    'InsulinLevel': [120],\n",
    "    'ActivityLevel': [2],   # Moderate activity\n",
    "    'FamilyHistory': [0.5]  # Moderate family history\n",
    "})\n",
    "\n",
    "print(\"New Patient Data:\")\n",
    "display(new_patient)\n",
    "\n",
    "# Make prediction\n",
    "predicted_glucose = lin_reg.predict(new_patient)[0]\n",
    "\n",
    "print(f\"Predicted Blood Glucose: {predicted_glucose:.1f} mg/dL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Model Evaluation Guide for Beginners\n",
    "\n",
    "Evaluating predictive models is crucial in healthcare applications where decisions can impact patient care. This section introduces key metrics and approaches for assessing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Metrics and Techniques\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Let's use our diabetes classification model for demonstration\n",
    "# Prepare the data (same as before)\n",
    "features = ['PatientAge', 'BodyMassIndex', 'BloodGlucose', 'SystolicBP', 'InsulinLevel', 'FamilyHistory']\n",
    "X = diabetes_df[features]\n",
    "y = diabetes_df['DiabetesStatus']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a logistic regression model\n",
    "log_reg = LogisticRegression(C=10, max_iter=1000, random_state=42)\n",
    "\n",
    "# 1. Cross-Validation (more robust than a single train-test split)\n",
    "print(\"1. Cross-Validation\")\n",
    "cv_scores = cross_val_score(log_reg, X_scaled, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.2f}\")\n",
    "print(f\"Standard deviation: {cv_scores.std():.2f}\")\n",
    "\n",
    "# 2. ROC Curve and AUC\n",
    "print(\"\\n2. ROC Curve and AUC\")\n",
    "# Train the model on the full dataset for demonstration\n",
    "log_reg.fit(X_scaled, y)\n",
    "y_prob = log_reg.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Area Under the ROC Curve (AUC): {roc_auc:.2f}\")\n",
    "print(\"- AUC of 0.5 suggests no discrimination (equivalent to random guessing)\")\n",
    "print(\"- AUC of 1.0 suggests perfect discrimination\")\n",
    "print(\"- In medical contexts, AUC > 0.8 is often considered good\")\n",
    "\n",
    "# 3. Precision-Recall Curve (especially useful for imbalanced datasets)\n",
    "print(\"\\n3. Precision-Recall Curve\")\n",
    "precision, recall, _ = precision_recall_curve(y, y_prob)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2)\n",
    "plt.xlabel('Recall (Sensitivity)')\n",
    "plt.ylabel('Precision (Positive Predictive Value)')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n",
    "\n",
    "print(\"Precision: Proportion of positive identifications that were actually correct\")\n",
    "print(\"Recall: Proportion of actual positives that were identified correctly\")\n",
    "print(\"- In screening tests, high recall (sensitivity) is often prioritized\")\n",
    "print(\"- In confirmatory tests, high precision is often prioritized\")\n",
    "\n",
    "# 4. Learning Curves (to diagnose overfitting/underfitting)\n",
    "print(\"\\n4. Learning Curves\")\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    log_reg, X_scaled, y, cv=5, scoring='accuracy', \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "# Calculate mean and standard deviation for training and test scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curves for Diabetes Classification Model\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning curves help diagnose model performance issues:\")\n",
    "print(\"- If training score is much higher than validation score: Model is overfitting\")\n",
    "print(\"- If both scores are low: Model is underfitting\")\n",
    "print(\"- If scores converge at a high value: Model is well-fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model Evaluation in Medical Contexts\n",
    "\n",
    "#### Choosing the Right Evaluation Metrics for Medical Contexts\n",
    "\n",
    "Different medical applications require different evaluation priorities:\n",
    "\n",
    "##### a) Screening Tests (e.g., initial diabetes risk assessment):\n",
    "- **Prioritize Sensitivity/Recall**: Minimize false negatives\n",
    "- **Key metrics**: Recall, NPV (Negative Predictive Value)\n",
    "- **Goal**: Identify as many potential cases as possible\n",
    "- *Clinical rationale*: Missing a patient with disease (false negative) is often more harmful than incorrectly flagging a healthy patient for follow-up\n",
    "\n",
    "##### b) Diagnostic Tests (e.g., confirming diabetes diagnosis):\n",
    "- **Prioritize Specificity**: Minimize false positives\n",
    "- **Key metrics**: Specificity, PPV (Positive Predictive Value/Precision)\n",
    "- **Goal**: Ensure those diagnosed truly have the condition\n",
    "- *Clinical rationale*: Incorrect diagnosis (false positive) can lead to unnecessary treatment, anxiety, and resource utilization\n",
    "\n",
    "##### c) Prognostic Models (e.g., predicting diabetes complications):\n",
    "- **Prioritize Calibration**: Accurate probability estimates\n",
    "- **Key metrics**: Calibration plots, Brier score\n",
    "- **Goal**: Reliable risk stratification\n",
    "- *Clinical rationale*: Clinicians need accurate risk estimates to make appropriate treatment decisions and counsel patients\n",
    "\n",
    "##### d) Resource Allocation Models (e.g., targeting interventions):\n",
    "- **Prioritize overall accuracy and cost-effectiveness**\n",
    "- **Key metrics**: Accuracy, AUC, cost-benefit analysis\n",
    "- **Goal**: Optimize resource utilization\n",
    "- *Clinical rationale*: Limited healthcare resources must be directed where they will have the greatest impact\n",
    "\n",
    "### Clinical Validation Considerations\n",
    "\n",
    "Statistical performance is necessary but not sufficient for clinical utility:\n",
    "\n",
    "- **External validation**: Test on different patient populations\n",
    "  - Models should perform consistently across diverse healthcare settings and patient demographics\n",
    "\n",
    "- **Temporal validation**: Test on newer data\n",
    "  - Healthcare patterns change over time; models must remain accurate with recent data\n",
    "\n",
    "- **Impact analysis**: Assess whether the model improves clinical outcomes\n",
    "  - The ultimate test is whether the model leads to better patient outcomes when implemented\n",
    "\n",
    "- **Fairness**: Ensure the model performs equitably across different patient groups\n",
    "  - Models should not perpetuate or amplify existing healthcare disparities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Interpreting Results in Clinical Context\n",
    "\n",
    "Translating model predictions into clinically meaningful insights is essential for healthcare applications. This section explores how to interpret predictive model results in ways that can inform clinical decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting Predictive Models in Clinical Context\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Let's create a simple decision tree model for interpretability\n",
    "# Prepare the data (similar to before)\n",
    "features = ['PatientAge', 'BodyMassIndex', 'BloodGlucose', 'SystolicBP', 'InsulinLevel', 'FamilyHistory']\n",
    "X = diabetes_df[features]\n",
    "y = diabetes_df['DiabetesStatus']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Decision Trees for Interpretable Predictions\n",
    "print(\"1. Decision Trees for Interpretable Predictions\")\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Create a simple decision tree (limited depth for interpretability)\n",
    "# Use the DecisionTreeClassifier from sklearn.tree\n",
    "# dt_model = ...\n",
    "\n",
    "\n",
    "# After the model is set, we can predict and evaluate\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_model, feature_names=features, class_names=['Non-Diabetic', 'Diabetic'], \n",
    "          filled=True, rounded=True, fontsize=10)\n",
    "plt.title(\"Decision Tree for Diabetes Classification\")\n",
    "plt.show()\n",
    "\n",
    "# What does the decision tree tell us?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Clinical Relevance\n",
    "print(\"\\nFeature Importance for Clinical Relevance\")\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# Use Random Forest for more stable feature importance\n",
    "# rf_model = ...\n",
    "# \n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance for Diabetes Prediction')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClinical Relevance of Features:\")\n",
    "for _, row in feature_importance.iterrows():\n",
    "    feature = row['Feature']\n",
    "    importance = row['Importance']\n",
    "    \n",
    "    print(f\"{feature} (Importance: {importance:.3f}):\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Stratification and Clinical Thresholds\n",
    "print(\"\\nRisk Stratification and Clinical Thresholds\")\n",
    "\n",
    "# YOUR CODE HERE: \n",
    "# Use our logistic regression model for probability estimates\n",
    "# log_reg = ...\n",
    "\n",
    "# Get probabilities for test set\n",
    "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create risk categories\n",
    "risk_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Probability': y_prob\n",
    "})\n",
    "\n",
    "# YOUR CODE HERE\n",
    "risk_categories = [0, 0.1, 0.2, 0.3, 1.0]  # try different values...\n",
    "\n",
    "# Create risk categories based on your input\n",
    "risk_df['Risk Category'] = pd.cut(\n",
    "    risk_df['Probability'], \n",
    "    bins=risk_categories,\n",
    "    labels=['Low Risk', 'Moderate Risk', 'High Risk', 'Very High Risk']\n",
    ")\n",
    "\n",
    "# Count patients in each risk category\n",
    "risk_counts = risk_df['Risk Category'].value_counts().sort_index()\n",
    "print(\"Patients by Risk Category:\")\n",
    "for category, count in risk_counts.items():\n",
    "    print(f\"{category}: {count} patients ({count/len(risk_df)*100:.1f}%)\")\n",
    "\n",
    "# Calculate actual diabetes rates within each risk category\n",
    "risk_accuracy = risk_df.groupby('Risk Category')['Actual'].mean()\n",
    "print(\"\\nActual Diabetes Prevalence by Predicted Risk Category:\")\n",
    "for category, rate in risk_accuracy.items():\n",
    "    print(f\"{category}: {rate*100:.1f}% have diabetes\")\n",
    "\n",
    "# Visualize risk stratification\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Risk Category', hue='Actual', data=risk_df, palette=['green', 'red'])\n",
    "plt.title('Predicted Risk Categories vs. Actual Diabetes Status')\n",
    "plt.xlabel('Predicted Risk Category')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.legend(['Non-Diabetic', 'Diabetic'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Ethical Considerations\n",
    "## 7.1 Overview of Privacy Concerns in Medical Data Science\n",
    "\n",
    "Medical data is among the most sensitive personal information. Privacy concerns in medical data science include:\n",
    "\n",
    "- Protection of patient identities\n",
    "- Secure handling of health information\n",
    "- Appropriate data sharing practices\n",
    "- Balancing research benefits with privacy risks\n",
    "\n",
    "Let's explore these concerns and best practices for addressing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to identify potentially sensitive information\n",
    "print(\"Sample of medical dataset:\")\n",
    "display(diabetes_df.head())\n",
    "\n",
    "# Demonstrate privacy risks with quasi-identifiers\n",
    "print(\"\\nDemonstration of Re-identification Risk:\")\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# Count unique combinations of quasi-identifiers. Select columns where you think they might identify individuals.\n",
    "# quasi_identifiers = [...] YOUR CODE HERE\n",
    "\n",
    "# Then count the unique combinations. Hint pandas \"groupby\" could be a good help here\n",
    "# unique_combinations = ... YOUR CODE HERE\n",
    "\n",
    "# Show combinations that might uniquely identify individuals\n",
    "# rare_combinations = ... YOUR CODE HERE\n",
    "\n",
    "# Lets see the results:\n",
    "print(f\"Number of rare demographic combinations (≤5 patients): {len(rare_combinations)}\")\n",
    "print(f\"These rare combinations could potentially be used for re-identification.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy and Ethics in Healthcare Data Science\n",
    "\n",
    "Working with medical data comes with significant ethical and legal responsibilities. Healthcare data scientists must navigate a complex landscape of regulations designed to protect patient privacy while enabling valuable research and innovation.\n",
    "\n",
    "### Key Privacy Frameworks and Regulations in Healthcare\n",
    "\n",
    "The regulatory environment for medical data varies globally, with several major frameworks guiding how we collect, process, and protect sensitive health information:\n",
    "\n",
    "**HIPAA (Health Insurance Portability and Accountability Act)** serves as the cornerstone of health data protection in the United States. It safeguards individually identifiable health information through a comprehensive set of standards, including the definition of 18 specific protected health identifiers that must be carefully managed. HIPAA establishes clear boundaries for the use and disclosure of patient data, creating a framework that balances privacy protection with legitimate healthcare operations and research needs.\n",
    "\n",
    "In the European context, the **General Data Protection Regulation (GDPR)** provides even broader protections for health data. Unlike HIPAA's healthcare-specific approach, GDPR treats health information as a special category of personal data deserving heightened protection. The regulation emphasizes individual autonomy through requirements for explicit consent before processing health data and grants individuals powerful rights to access, correct, and even erase their personal information from databases.\n",
    "\n",
    "### Best Practices for Privacy Protection in Medical Data Science\n",
    "\n",
    "Beyond regulatory compliance, responsible data scientists should implement robust privacy protection strategies:\n",
    "\n",
    "**Data Minimization** represents a fundamental principle in privacy-preserving data science. By collecting and retaining only the data elements absolutely necessary for the specific research or clinical question, we reduce privacy risks at their source. This approach includes implementing strict access controls that limit exposure of sensitive information only to those with a legitimate need.\n",
    "\n",
    "**De-identification Techniques** transform identifiable patient data into more anonymous forms while preserving analytical utility. This process involves removing direct identifiers (like names and medical record numbers), generalizing quasi-identifiers (such as zip codes or birth dates), and applying statistical disclosure controls to minimize re-identification risks. Well-executed de-identification allows researchers to work with realistic data while protecting individual privacy.\n",
    "\n",
    "**Secure Data Handling** practices form the technical foundation of privacy protection. Implementing strong encryption for data both at rest and in transit ensures that even if unauthorized access occurs, the information remains protected. Conducting analysis within secure computing environments with appropriate access controls and authentication mechanisms adds additional layers of protection against data breaches.\n",
    "\n",
    "**Ethical Review and Governance** processes provide oversight and accountability. Obtaining IRB approval for research projects, establishing clear data use agreements between institutions, and maintaining transparent data practices all contribute to an ethical framework that respects patient autonomy while enabling scientific progress.\n",
    "\n",
    "By integrating these regulatory frameworks and best practices into data science workflows, healthcare researchers can advance medical knowledge while honoring their ethical obligation to protect patient privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Data De-identification Techniques\n",
    "\n",
    "De-identification is the process of removing or modifying personal information to reduce the risk of identifying individuals. In medical data science, effective de-identification is essential for protecting patient privacy while enabling valuable research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data De-identification Techniques\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from faker import Faker\n",
    "import hashlib\n",
    "\n",
    "# Set up a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "\n",
    "# For demonstration, let's add some fictional identifiers to our dataset\n",
    "# Note: In real medical datasets, these would already exist\n",
    "patient_ids = [f\"P{1000+i}\" for i in range(len(diabetes_df))]\n",
    "names = [fake.name() for _ in range(len(diabetes_df))]\n",
    "birth_dates = [fake.date_of_birth(minimum_age=18, maximum_age=90) for _ in range(len(diabetes_df))]\n",
    "zip_codes = [fake.zipcode() for _ in range(len(diabetes_df))]\n",
    "\n",
    "# Add these to our dataset\n",
    "diabetes_identified = diabetes_df.copy()\n",
    "diabetes_identified['PatientID'] = patient_ids\n",
    "diabetes_identified['Name'] = names\n",
    "diabetes_identified['BirthDate'] = birth_dates\n",
    "diabetes_identified['ZipCode'] = zip_codes\n",
    "\n",
    "# Display the dataset with identifiers\n",
    "print(\"Medical Dataset with Identifiers:\")\n",
    "display(diabetes_identified.head())\n",
    "\n",
    "# 1. Removal of Direct Identifiers\n",
    "print(\"\\n1. Removal of Direct Identifiers\")\n",
    "print(\"The simplest de-identification approach is removing direct identifiers.\")\n",
    "\n",
    "# Remove direct identifiers\n",
    "diabetes_deident1 = diabetes_identified.copy()\n",
    "diabetes_deident1 = diabetes_deident1.drop(columns=['PatientID', 'Name'])\n",
    "\n",
    "print(\"Dataset after removing direct identifiers:\")\n",
    "display(diabetes_deident1.head())\n",
    "print(\"Note: This still contains quasi-identifiers that could re-identify individuals.\")\n",
    "\n",
    "# 2. Pseudonymization\n",
    "print(\"\\n2. Pseudonymization\")\n",
    "print(\"Pseudonymization replaces identifiers with artificial values that can't be attributed to individuals without additional information.\")\n",
    "\n",
    "# Create pseudonyms using hashing\n",
    "diabetes_deident2 = diabetes_identified.copy()\n",
    "# Hash the patient IDs (in practice, you would use a secure key)\n",
    "diabetes_deident2['PatientID'] = diabetes_deident2['PatientID'].apply(\n",
    "    lambda x: hashlib.sha256(x.encode()).hexdigest()[:8]\n",
    ")\n",
    "# Remove names\n",
    "diabetes_deident2 = diabetes_deident2.drop(columns=['Name'])\n",
    "\n",
    "print(\"Dataset after pseudonymization:\")\n",
    "display(diabetes_deident2.head())\n",
    "print(\"The hashed IDs allow linking records without revealing identities.\")\n",
    "\n",
    "# 3. Generalization\n",
    "print(\"\\n3. Generalization\")\n",
    "print(\"Generalization reduces precision of data to protect privacy while preserving utility.\")\n",
    "\n",
    "# Generalize quasi-identifiers\n",
    "diabetes_deident3 = diabetes_identified.copy()\n",
    "# Age ranges instead of exact ages\n",
    "diabetes_deident3['AgeGroup'] = pd.cut(\n",
    "    diabetes_deident3['PatientAge'], \n",
    "    bins=[0, 30, 45, 60, 100],\n",
    "    labels=['<30', '30-45', '46-60', '>60']\n",
    ")\n",
    "# First 3 digits of ZIP code\n",
    "diabetes_deident3['ZipRegion'] = diabetes_deident3['ZipCode'].apply(lambda x: x[:3])\n",
    "# Year of birth instead of full date\n",
    "diabetes_deident3['BirthYear'] = pd.to_datetime(diabetes_deident3['BirthDate']).dt.year\n",
    "# Remove original identifiers\n",
    "diabetes_deident3 = diabetes_deident3.drop(\n",
    "    columns=['PatientID', 'Name', 'PatientAge', 'ZipCode', 'BirthDate']\n",
    ")\n",
    "\n",
    "print(\"Dataset after generalization:\")\n",
    "display(diabetes_deident3.head())\n",
    "print(\"Note how specific values are replaced with ranges or less precise information.\")\n",
    "\n",
    "# 4. Perturbation (Adding Noise)\n",
    "print(\"\\n4. Perturbation (Adding Noise)\")\n",
    "print(\"Perturbation adds random noise to numerical values to protect privacy.\")\n",
    "\n",
    "# Add noise to numerical values\n",
    "diabetes_deident4 = diabetes_identified.copy()\n",
    "# Add small random noise to continuous variables\n",
    "for col in ['BodyMassIndex', 'BloodGlucose', 'SystolicBP', 'InsulinLevel']:\n",
    "    # Calculate standard deviation\n",
    "    std = diabetes_deident4[col].std()\n",
    "    # Add noise (2% of standard deviation)\n",
    "    diabetes_deident4[col] = diabetes_deident4[col] + np.random.normal(0, 0.02 * std, len(diabetes_deident4))\n",
    "    # Round to reasonable precision\n",
    "    diabetes_deident4[col] = diabetes_deident4[col].round(1)\n",
    "# Remove direct identifiers\n",
    "diabetes_deident4 = diabetes_deident4.drop(columns=['PatientID', 'Name', 'BirthDate', 'ZipCode'])\n",
    "\n",
    "print(\"Dataset after perturbation:\")\n",
    "display(diabetes_deident4.head())\n",
    "print(\"Small random noise has been added to numerical values.\")\n",
    "\n",
    "# 5. Synthetic Data Generation\n",
    "print(\"\\n5. Synthetic Data Generation\")\n",
    "print(\"Synthetic data preserves statistical properties without containing real individuals.\")\n",
    "\n",
    "# For demonstration, we'll create a very simple synthetic dataset\n",
    "# In practice, more sophisticated methods would be used\n",
    "# Calculate statistics from original data\n",
    "means = diabetes_df[numeric_cols].mean()\n",
    "stds = diabetes_df[numeric_cols].std()\n",
    "correlations = diabetes_df[numeric_cols].corr()\n",
    "\n",
    "# Generate synthetic data with similar properties\n",
    "n_synthetic = 100\n",
    "synthetic_data = pd.DataFrame()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    synthetic_data[col] = np.random.normal(means[col], stds[col], n_synthetic)\n",
    "\n",
    "# Add a synthetic diabetes status\n",
    "synthetic_data['DiabetesStatus'] = np.random.binomial(1, diabetes_df['DiabetesStatus'].mean(), n_synthetic)\n",
    "\n",
    "print(\"Synthetic dataset:\")\n",
    "display(synthetic_data.head())\n",
    "print(\"This data maintains statistical properties without containing real patient information.\")\n",
    "\n",
    "# 6. K-Anonymity\n",
    "print(\"\\n6. K-Anonymity\")\n",
    "print(\"K-anonymity ensures each combination of quasi-identifiers appears at least k times.\")\n",
    "\n",
    "# Demonstrate k-anonymity concept\n",
    "diabetes_deident6 = diabetes_identified.copy()\n",
    "# Generalize age into decades\n",
    "diabetes_deident6['AgeDecade'] = (diabetes_deident6['PatientAge'] // 10) * 10\n",
    "# Generalize BMI into categories\n",
    "diabetes_deident6['BMICategory'] = pd.cut(\n",
    "    diabetes_deident6['BodyMassIndex'],\n",
    "    bins=[0, 18.5, 25, 30, 100],\n",
    "    labels=['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    ")\n",
    "# Remove direct identifiers and original values\n",
    "diabetes_deident6 = diabetes_deident6.drop(\n",
    "    columns=['PatientID', 'Name', 'BirthDate', 'ZipCode', 'PatientAge', 'BodyMassIndex']\n",
    ")\n",
    "\n",
    "# Check k-anonymity for quasi-identifiers\n",
    "quasi_ids = ['AgeDecade', 'BMICategory', 'ActivityLevel']\n",
    "group_counts = diabetes_deident6.groupby(quasi_ids).size().reset_index(name='count')\n",
    "k_value = group_counts['count'].min()\n",
    "\n",
    "print(f\"Dataset achieves {k_value}-anonymity for the selected quasi-identifiers.\")\n",
    "print(\"This means each unique combination of age decade, BMI category, and activity level\")\n",
    "print(f\"appears at least {k_value} times in the dataset.\")\n",
    "\n",
    "# Display the k-anonymized dataset\n",
    "print(\"\\nK-anonymized dataset:\")\n",
    "display(diabetes_deident6.head())\n",
    "\n",
    "# 7. Comparing De-identification Methods\n",
    "print(\"\\n7. Comparing De-identification Methods\")\n",
    "\n",
    "print(\"\\nPrivacy-Utility Tradeoff:\")\n",
    "methods = [\n",
    "    \"Removal of Direct Identifiers\",\n",
    "    \"Pseudonymization\",\n",
    "    \"Generalization\",\n",
    "    \"Perturbation\",\n",
    "    \"Synthetic Data\",\n",
    "    \"K-Anonymity\"\n",
    "]\n",
    "\n",
    "privacy_levels = [1, 2, 3, 4, 5, 4]  # 1=low, 5=high\n",
    "utility_levels = [5, 4, 3, 3, 2, 3]  # 1=low, 5=high\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': methods,\n",
    "    'Privacy Protection': privacy_levels,\n",
    "    'Data Utility': utility_levels\n",
    "})\n",
    "\n",
    "print(\"Comparison of de-identification methods:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize the privacy-utility tradeoff\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(comparison_df['Privacy Protection'], comparison_df['Data Utility'], s=100)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.annotate(method, \n",
    "                 (comparison_df['Privacy Protection'][i], comparison_df['Data Utility'][i]),\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "plt.xlabel('Privacy Protection (higher is better)')\n",
    "plt.ylabel('Data Utility (higher is better)')\n",
    "plt.title('Privacy-Utility Tradeoff for De-identification Methods')\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Best practices for de-identification\n",
    "print(\"\\nBest Practices for De-identification in Medical Research:\")\n",
    "print(\"1. Use multiple techniques in combination for stronger protection\")\n",
    "print(\"2. Consider the specific sensitivity of the data and research context\")\n",
    "print(\"3. Perform risk assessments to evaluate re-identification risk\")\n",
    "print(\"4. Document de-identification methods for transparency\")\n",
    "print(\"5. Implement additional safeguards (e.g., secure access, data use agreements)\")\n",
    "print(\"6. Regularly review de-identification as new methods and risks emerge\")\n",
    "print(\"7. Consult with privacy experts and ethics committees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Bias in Medical Data and Algorithms\n",
    "\n",
    "Bias in medical data science can lead to unfair treatment, inaccurate predictions, and perpetuation of health disparities. Understanding and mitigating bias is essential for developing equitable healthcare algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias in Medical Data and Algorithms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load our diabetes dataset\n",
    "import os\n",
    "data_dir = os.path.join(os.path.dirname(os.path.abspath('')), 'data')\n",
    "file_path = os.path.join(data_dir, 'synthetic_diabetes.csv')\n",
    "# diabetes_df = pd.read_csv(file_path)\n",
    "\n",
    "# For demonstration, let's add synthetic demographic variables\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add gender (binary for simplicity, though real applications should be more inclusive)\n",
    "diabetes_df['Gender'] = np.random.choice(['Male', 'Female'], size=len(diabetes_df))\n",
    "\n",
    "# Add ethnicity groups\n",
    "ethnicities = ['Group A', 'Group B', 'Group C', 'Group D']\n",
    "# Create an imbalanced distribution\n",
    "ethnicity_probs = [0.6, 0.2, 0.15, 0.05]  # Intentionally imbalanced\n",
    "diabetes_df['Ethnicity'] = np.random.choice(ethnicities, size=len(diabetes_df), p=ethnicity_probs)\n",
    "\n",
    "# Add socioeconomic status (SES)\n",
    "diabetes_df['SES'] = np.random.choice(['Low', 'Medium', 'High'], size=len(diabetes_df), p=[0.3, 0.5, 0.2])\n",
    "\n",
    "# Introduce synthetic bias: make diabetes more prevalent in certain groups\n",
    "# This is for educational purposes to demonstrate bias detection and mitigation\n",
    "for i, row in diabetes_df.iterrows():\n",
    "    # Higher diabetes rates for Group C and D (for demonstration)\n",
    "    if row['Ethnicity'] in ['Group C', 'Group D'] and np.random.random() < 0.3:\n",
    "        diabetes_df.at[i, 'DiabetesStatus'] = 1\n",
    "    # Higher diabetes rates for low SES (for demonstration)\n",
    "    if row['SES'] == 'Low' and np.random.random() < 0.2:\n",
    "        diabetes_df.at[i, 'DiabetesStatus'] = 1\n",
    "\n",
    "# Display the dataset with demographic variables\n",
    "print(\"Medical Dataset with Demographic Variables:\")\n",
    "display(diabetes_df.head())\n",
    "\n",
    "# 1. Types of Bias in Medical Data Science\n",
    "print(\"\\n1. Types of Bias in Medical Data Science\")\n",
    "\n",
    "print(\"\\na) Selection Bias\")\n",
    "print(\"   - Occurs when the data doesn't represent the population it's meant to serve\")\n",
    "print(\"   - Example: Clinical trials historically underrepresented women and minorities\")\n",
    "print(\"   - Consequence: Models may not work well for underrepresented groups\")\n",
    "\n",
    "print(\"\\nb) Measurement Bias\")\n",
    "print(\"   - Occurs when data collection methods vary across groups\")\n",
    "print(\"   - Example: Different diagnostic criteria or testing rates across populations\")\n",
    "print(\"   - Consequence: False differences in disease prevalence or severity\")\n",
    "\n",
    "print(\"\\nc) Confounding Bias\")\n",
    "print(\"   - Occurs when unmeasured variables affect both predictors and outcomes\")\n",
    "print(\"   - Example: Socioeconomic status affecting both healthcare access and health outcomes\")\n",
    "print(\"   - Consequence: Spurious associations that don't reflect causal relationships\")\n",
    "\n",
    "print(\"\\nd) Historical/Prejudice Bias\")\n",
    "print(\"   - Occurs when historical inequities are encoded in the data\")\n",
    "print(\"   - Example: Historical underdiagnosis of certain conditions in minority populations\")\n",
    "print(\"   - Consequence: Algorithms may perpetuate or amplify existing disparities\")\n",
    "\n",
    "print(\"\\ne) Algorithmic Bias\")\n",
    "print(\"   - Occurs when models perform differently across demographic groups\")\n",
    "print(\"   - Example: Higher error rates for certain populations\")\n",
    "print(\"   - Consequence: Unfair allocation of resources or clinical recommendations\")\n",
    "\n",
    "# 2. Detecting Bias in Medical Datasets\n",
    "print(\"\\n2. Detecting Bias in Medical Datasets\")\n",
    "\n",
    "# Analyze diabetes prevalence across demographic groups\n",
    "print(\"\\nDiabetes Prevalence by Demographic Groups:\")\n",
    "\n",
    "# By gender\n",
    "gender_diabetes = diabetes_df.groupby('Gender')['DiabetesStatus'].mean()\n",
    "print(\"\\nDiabetes Prevalence by Gender:\")\n",
    "for gender, rate in gender_diabetes.items():\n",
    "    print(f\"{gender}: {rate*100:.1f}%\")\n",
    "\n",
    "# By ethnicity\n",
    "ethnicity_diabetes = diabetes_df.groupby('Ethnicity')['DiabetesStatus'].mean()\n",
    "print(\"\\nDiabetes Prevalence by Ethnicity:\")\n",
    "for ethnicity, rate in ethnicity_diabetes.items():\n",
    "    print(f\"{ethnicity}: {rate*100:.1f}%\")\n",
    "    \n",
    "# By socioeconomic status\n",
    "ses_diabetes = diabetes_df.groupby('SES')['DiabetesStatus'].mean()\n",
    "print(\"\\nDiabetes Prevalence by Socioeconomic Status:\")\n",
    "for ses, rate in ses_diabetes.items():\n",
    "    print(f\"{ses}: {rate*100:.1f}%\")\n",
    "\n",
    "# Visualize the disparities\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Gender plot\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='Gender', y='DiabetesStatus', data=diabetes_df)\n",
    "plt.title('Diabetes Prevalence by Gender')\n",
    "plt.ylabel('Prevalence')\n",
    "\n",
    "# Ethnicity plot\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x='Ethnicity', y='DiabetesStatus', data=diabetes_df)\n",
    "plt.title('Diabetes Prevalence by Ethnicity')\n",
    "plt.ylabel('')\n",
    "\n",
    "# SES plot\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='SES', y='DiabetesStatus', data=diabetes_df)\n",
    "plt.title('Diabetes Prevalence by Socioeconomic Status')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- There appears to be higher diabetes prevalence in certain ethnic groups\")\n",
    "print(\"- Socioeconomic status shows a gradient in diabetes prevalence\")\n",
    "print(\"- These disparities could reflect real-world health inequities or bias in the data\")\n",
    "\n",
    "# 3. Bias in Predictive Models\n",
    "print(\"\\n3. Bias in Predictive Models\")\n",
    "\n",
    "# Prepare data for modeling\n",
    "features = ['PatientAge', 'BodyMassIndex', 'BloodGlucose', 'SystolicBP', 'Gender', 'Ethnicity', 'SES']\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "diabetes_model_df = pd.get_dummies(diabetes_df, columns=['Gender', 'Ethnicity', 'SES'], drop_first=True)\n",
    "\n",
    "# Select features for modeling (after dummy encoding)\n",
    "model_features = [col for col in diabetes_model_df.columns \n",
    "                 if col.startswith(('PatientAge', 'BodyMassIndex', 'BloodGlucose', 'SystolicBP', 'Gender', 'Ethnicity', 'SES'))]\n",
    "\n",
    "X = diabetes_model_df[model_features]\n",
    "y = diabetes_model_df['DiabetesStatus']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Overall model performance\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate model performance across demographic groups\n",
    "print(\"\\nModel Performance by Demographic Groups:\")\n",
    "\n",
    "# Before evaluating group performance, print the actual column names to help debug\n",
    "print(\"\\nAvailable columns in X_test:\")\n",
    "for col in X_test.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Function to evaluate model performance for a specific group\n",
    "def evaluate_group_performance(group_name, group_value, X, y_true):\n",
    "    # Create mask for the group - more robust approach\n",
    "    column_name = f\"{group_name}_{group_value}\"\n",
    "    \n",
    "    # Check if the exact column exists\n",
    "    if column_name in X.columns:\n",
    "        mask = X[column_name] == 1\n",
    "    else:\n",
    "        # Try to find a matching column (case insensitive)\n",
    "        matching_cols = [col for col in X.columns if col.lower() == column_name.lower()]\n",
    "        if matching_cols:\n",
    "            mask = X[matching_cols[0]] == 1\n",
    "        else:\n",
    "            # If no exact match, print available columns and return None\n",
    "            print(f\"Column '{column_name}' not found. Available columns related to {group_name}:\")\n",
    "            related_cols = [col for col in X.columns if group_name.lower() in col.lower()]\n",
    "            for col in related_cols:\n",
    "                print(f\"  - {col}\")\n",
    "            return None\n",
    "    \n",
    "    # Filter data for this group\n",
    "    X_group = X[mask]\n",
    "    y_true_group = y_true[mask]\n",
    "    \n",
    "    # Skip if no samples\n",
    "    if len(y_true_group) == 0:\n",
    "        print(f\"No samples found for {group_name}_{group_value}\")\n",
    "        return None\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_group = model.predict(X_group)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        # Try to get a 2x2 confusion matrix\n",
    "        cm = confusion_matrix(y_true_group, y_pred_group)\n",
    "        \n",
    "        # Check if we have a proper 2x2 matrix\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        else:\n",
    "            # Handle the case where all predictions are the same class\n",
    "            print(f\"Warning: All predictions for {group_name}_{group_value} are the same class.\")\n",
    "            print(f\"Confusion matrix shape: {cm.shape}\")\n",
    "            print(f\"Confusion matrix:\\n{cm}\")\n",
    "            \n",
    "            # Set default values\n",
    "            sensitivity = np.nan\n",
    "            specificity = np.nan\n",
    "            ppv = np.nan\n",
    "            npv = np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics for {group_name}_{group_value}: {e}\")\n",
    "        # Set default values\n",
    "        sensitivity = np.nan\n",
    "        specificity = np.nan\n",
    "        ppv = np.nan\n",
    "        npv = np.nan\n",
    "    \n",
    "    return {\n",
    "        'Group': f\"{group_name}_{group_value}\",\n",
    "        'Samples': len(y_true_group),\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv\n",
    "    }\n",
    "\n",
    "# Evaluate performance for each gender - dynamically find gender columns\n",
    "gender_results = []\n",
    "# Find what gender values are available as dummy variables\n",
    "gender_columns = [col for col in X_test.columns if col.startswith('Gender_')]\n",
    "print(f\"\\nAvailable gender columns: {gender_columns}\")\n",
    "\n",
    "if gender_columns:\n",
    "    for gender_col in gender_columns:\n",
    "        # Extract the gender value from the column name\n",
    "        gender_value = gender_col.replace('Gender_', '')\n",
    "        result = evaluate_group_performance('Gender', gender_value, X_test, y_test)\n",
    "        if result:\n",
    "            gender_results.append(result)\n",
    "else:\n",
    "    print(\"No gender columns found in the dataset.\")\n",
    "\n",
    "# Evaluate performance for each ethnicity - dynamically find ethnicity columns\n",
    "ethnicity_results = []\n",
    "# Find what ethnicity values are available as dummy variables\n",
    "ethnicity_columns = [col for col in X_test.columns if col.startswith('Ethnicity_')]\n",
    "print(f\"\\nAvailable ethnicity columns: {ethnicity_columns}\")\n",
    "\n",
    "if ethnicity_columns:\n",
    "    for ethnicity_col in ethnicity_columns:\n",
    "        # Extract the ethnicity value from the column name\n",
    "        ethnicity_value = ethnicity_col.replace('Ethnicity_', '')\n",
    "        result = evaluate_group_performance('Ethnicity', ethnicity_value, X_test, y_test)\n",
    "        if result:\n",
    "            ethnicity_results.append(result)\n",
    "else:\n",
    "    print(\"No ethnicity columns found in the dataset.\")\n",
    "\n",
    "# Evaluate performance for each SES - dynamically find SES columns\n",
    "ses_results = []\n",
    "# Find what SES values are available as dummy variables\n",
    "ses_columns = [col for col in X_test.columns if col.startswith('SES_')]\n",
    "print(f\"\\nAvailable SES columns: {ses_columns}\")\n",
    "\n",
    "if ses_columns:\n",
    "    for ses_col in ses_columns:\n",
    "        # Extract the SES value from the column name\n",
    "        ses_value = ses_col.replace('SES_', '')\n",
    "        result = evaluate_group_performance('SES', ses_value, X_test, y_test)\n",
    "        if result:\n",
    "            ses_results.append(result)\n",
    "else:\n",
    "    print(\"No SES columns found in the dataset.\")\n",
    "\n",
    "# Combine results\n",
    "all_results = gender_results + ethnicity_results + ses_results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\nPerformance Metrics Across Groups:\")\n",
    "display(results_df)\n",
    "\n",
    "# Visualize performance disparities\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Group', y='Sensitivity', data=results_df)\n",
    "plt.title('Model Sensitivity Across Demographic Groups')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
